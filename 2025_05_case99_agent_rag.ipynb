{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC3Be17wLLUn"
      },
      "source": [
        "# **RAGì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ ë§¤ìš° íš¨ê³¼ì ì´ê³  í˜„ëŒ€ì ì¸ ì ‘ê·¼ ë°©ì‹ì¸ ì¤‘ í•˜ë‚˜ê°€ ReAct(Reason + Act) í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ì˜ ì—ì´ì „íŠ¸(Agent)ì…ë‹ˆë‹¤. **\n",
        "\n",
        "Function Calling (ë˜ëŠ” LangChainì˜ Tool ì‚¬ìš©)ì€ ì´ ì—ì´ì „íŠ¸ê°€ \"í–‰ë™(Act)\"í•˜ëŠ” í•µì‹¬ ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "**ì´ ë°©ë²•ì´ ì™œ ì¢‹ì€ê°€?**\n",
        "\n",
        "- ì¡°ê±´ë¶€ ì‹¤í–‰:Â ê¸°ì¡´ RAGëŠ” ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ë¬´ì¡°ê±´ ê²€ìƒ‰(Retrieve)í•˜ê³  ìƒì„±(Generate)í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ReAct ì—ì´ì „íŠ¸ëŠ” LLMì´ ë¨¼ì € **ìƒê°(Reason)**í•˜ê³ , \"ì´ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ë¬¸ì„œ ê²€ìƒ‰(RAG)ì´ í•„ìš”í•œê°€?\" ë˜ëŠ” \"ê·¸ëƒ¥ ë‚´ ì§€ì‹ìœ¼ë¡œ ë‹µí•´ë„ ë˜ëŠ”ê°€?\" ë“±ì„ íŒë‹¨í•©ë‹ˆë‹¤. í•„ìš”í•  ë•Œë§Œ RAG í•¨ìˆ˜(Tool)ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "- ìƒí™© íŒë‹¨:Â RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ê±°ë‚˜ ë‹µë³€ì— ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨ë˜ë©´, ì—ì´ì „íŠ¸ëŠ” \"ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•˜ê±°ë‚˜, ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•˜ëŠ” ë“± ë‹¤ë¥¸ í–‰ë™ì„ ì·¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "- ë„êµ¬ í™œìš©:Â RAG íŒŒì´í”„ë¼ì¸ ìì²´ë¥¼ í•˜ë‚˜ì˜ \"ë„êµ¬(Tool)\"ë¡œ ê°„ì£¼í•˜ì—¬, ì—ì´ì „íŠ¸ê°€ ê³„ì‚°ê¸°, ì›¹ ê²€ìƒ‰, ë‹¤ë¥¸ API í˜¸ì¶œ ë“± ë‹¤ì–‘í•œ ë„êµ¬ì™€ í•¨ê»˜ ìƒí™©ì— ë§ê²Œ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfV42Er6tDWn"
      },
      "source": [
        "    base_folder_path = \"/content/drive/MyDrive/nomu_dataset3\" # <<< ì›ë³¸ ë°ì´í„° í´ë” ê²½ë¡œ í™•ì¸!\n",
        "    result_dir = \"/content/drive/MyDrive/nomu_rag_result\"    # <<< ê²°ê³¼ ì €ì¥ í´ë” ê²½ë¡œ í™•ì¸!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIL128MhoqWr"
      },
      "source": [
        "# 0: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (OpenAI ì¶”ê°€)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLKVnkOgotFA",
        "outputId": "22cf1f74-0d8a-4603-a7d7-eabad0272142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ë‹¨ê³„ 0: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì‹œì‘ (OpenAI ì¶”ê°€) ---\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m149.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m746.6/746.6 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m144.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.1/192.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "[ì•Œë¦¼] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜/ì—…ë°ì´íŠ¸ ì™„ë£Œ. langchain-openai, openai ì¶”ê°€ë¨.\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 0: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì˜ì¡´ì„± ì¶©ëŒ í•´ê²° ë²„ì „ + OpenAI ì¶”ê°€) ===\n",
        "print(\"--- ë‹¨ê³„ 0: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì‹œì‘ (OpenAI ì¶”ê°€) ---\")\n",
        "!pip install -qU \\\n",
        "    langchain langchain-core langchain-community langchainhub langchain-openai openai \\\n",
        "    pypdf openpyxl xlrd unstructured faiss-cpu sentence-transformers \\\n",
        "    pdf2image pillow pdfminer.six rank_bm25 pillow-heif jq \\\n",
        "    google-api-python-client google-auth-httplib2 google-auth-oauthlib gspread \\\n",
        "    ragas datasets \\\n",
        "    pandas==2.2.2 \\\n",
        "    PyPDF2 \\\n",
        "    fsspec==2025.3.2 # <<< fsspec ë²„ì „ì€ í™˜ê²½ì— ë”°ë¼ ì¡°ì • í•„ìš”, ì›ë˜ ë²„ì „ ì‚¬ìš©\n",
        "\n",
        "# google-ai-generativelanguageëŠ” OpenAI ì‚¬ìš© ì‹œ í•„ìˆ˜ëŠ” ì•„ë‹˜ (í•„ìš”ì‹œ ìœ ì§€)\n",
        "# !pip install -qU google-ai-generativelanguage==0.6.15\n",
        "\n",
        "print(\"\\n[ì•Œë¦¼] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜/ì—…ë°ì´íŠ¸ ì™„ë£Œ. langchain-openai, openai ì¶”ê°€ë¨.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw9hai20pM7K"
      },
      "source": [
        "# 1: ê¸°ë³¸ ë° í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (OpenAI LLM ì„í¬íŠ¸ ì¶”ê°€)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezNk5hE9pQv_",
        "outputId": "d6f3005e-7941-41ae-e23b-6554adca3eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ë‹¨ê³„ 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ (ChatOpenAI ì„í¬íŠ¸ë¨) ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 1: ê¸°ë³¸ ë° í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ===\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import sys\n",
        "import warnings\n",
        "import time\n",
        "import pandas as pd\n",
        "from google.colab import drive, auth, userdata\n",
        "import PyPDF2 # ëª…ì‹œì  ì„í¬íŠ¸\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import gspread\n",
        "from google.auth import default as google_auth_default\n",
        "from datasets import Dataset\n",
        "import re\n",
        "import traceback # ì˜¤ë¥˜ ìƒì„¸ ì¶œë ¥ì„ ìœ„í•´ ì¶”ê°€\n",
        "\n",
        "# LangChain ê´€ë ¨ ì„í¬íŠ¸\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, UnstructuredExcelLoader, CSVLoader,\n",
        "    UnstructuredFileLoader, DirectoryLoader, GoogleDriveLoader\n",
        ")\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain_google_genai import GoogleGenerativeAIEmbeddings # Google ì„ë² ë”© (í•„ìš” ì‹œ ìœ ì§€)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings # HuggingFace ì„ë² ë”© (ìœ ì§€)\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "# === LLM ì„í¬íŠ¸ ë³€ê²½ ===\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI # <<< ì‚­ì œ ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬\n",
        "from langchain_openai import ChatOpenAI # <<< OpenAI LLM í´ë˜ìŠ¤ ì„í¬íŠ¸\n",
        "# =======================\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# RAGAS ê´€ë ¨ ì„í¬íŠ¸ (ê¸°ì¡´ ìœ ì§€)\n",
        "try:\n",
        "    from ragas import evaluate\n",
        "    from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
        "    from ragas.llms import LangchainLLMWrapper\n",
        "    from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "except ImportError:\n",
        "    print(\"!! Ragas ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í‰ê°€ ë‹¨ê³„ ì „ì— ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "    LangchainLLMWrapper = None; LangchainEmbeddingsWrapper = None; context_precision = None; context_recall = None; faithfulness = None; answer_relevancy = None\n",
        "\n",
        "# ê¸°íƒ€ í‰ê°€ ê´€ë ¨ ì„í¬íŠ¸ (ê¸°ì¡´ ìœ ì§€)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# import google.generativeai as genai # OpenAI ì‚¬ìš© ì‹œ í•„ìˆ˜ëŠ” ì•„ë‹˜\n",
        "\n",
        "warnings.filterwarnings(\"ignore\") # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°\n",
        "\n",
        "print(\"--- ë‹¨ê³„ 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ (ChatOpenAI ì„í¬íŠ¸ë¨) ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNDH4-J6prCU"
      },
      "source": [
        "# 2: í™˜ê²½ ì„¤ì • (OpenAI API í‚¤ ì¶”ê°€)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLV3INSgrLZi"
      },
      "source": [
        "    base_folder_path = \"/content/drive/MyDrive/nomu_dataset3\" # <<< ì›ë³¸ ë°ì´í„° í´ë” ê²½ë¡œ í™•ì¸!\n",
        "    result_dir = \"/content/drive/MyDrive/nomu_rag_result\"    # <<< ê²°ê³¼ ì €ì¥ í´ë” ê²½ë¡œ í™•ì¸!\n",
        "    # <<< nomu_dataset3 í´ë”ì˜ ì‹¤ì œ IDë¡œ ë³€ê²½í•˜ê±°ë‚˜ í™•ì¸! >>>\n",
        "ì˜ˆ)\n",
        "    https://drive.google.com/drive/u/0/folders/1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U\n",
        "\n",
        "target_folder_id = \"1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3MRw_Yupuun",
        "outputId": "d94d6c02-4a02-4a56-86ff-f065c6d3dac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 2: í™˜ê²½ ì„¤ì • ì‹œì‘ ---\n",
            "Mounted at /content/drive\n",
            "[ì„±ê³µ] Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ.\n",
            "[ì„±ê³µ] Google Colab ì‚¬ìš©ì ì¸ì¦ ì™„ë£Œ.\n",
            "[ì„±ê³µ] OpenAI API í‚¤ ë¡œë“œ ë° ì„¤ì • ì™„ë£Œ.\n",
            "ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰ ê²½ë¡œ: /content/drive/MyDrive/nomu_dataset3\n",
            "ê²°ê³¼ ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/nomu_rag_result\n",
            "Google Sheets ê²€ìƒ‰ ëŒ€ìƒ í´ë” ID: 1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U\n",
            "--- ë‹¨ê³„ 2: í™˜ê²½ ì„¤ì • ì™„ë£Œ (OpenAI API í‚¤ ì„¤ì •ë¨) ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 2: í™˜ê²½ ì„¤ì • (Drive ë§ˆìš´íŠ¸, API í‚¤, ê²½ë¡œ) ===\n",
        "print(\"\\n--- ë‹¨ê³„ 2: í™˜ê²½ ì„¤ì • ì‹œì‘ ---\")\n",
        "\n",
        "# --- Google Drive ë§ˆìš´íŠ¸ (ê¸°ì¡´ ìœ ì§€) ---\n",
        "DRIVE_MOUNTED = False\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "    print(\"[ì„±ê³µ] Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ.\")\n",
        "except Exception as e:\n",
        "    print(f\"[ì‹¤íŒ¨] Google Drive ë§ˆìš´íŠ¸ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# --- Google ì¸ì¦ (Colab ì‚¬ìš©ì ì¸ì¦ - í•„ìš”ì‹œ ìœ ì§€) ---\n",
        "# Google Drive Loader ë“±ì„ ì‚¬ìš©í•œë‹¤ë©´ ì¸ì¦ ìœ ì§€ í•„ìš”\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    print(\"[ì„±ê³µ] Google Colab ì‚¬ìš©ì ì¸ì¦ ì™„ë£Œ.\")\n",
        "except Exception as e:\n",
        "    print(f\"[ì‹¤íŒ¨] Google Colab ì¸ì¦ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "# === API í‚¤ ì„¤ì • ë³€ê²½ ===\n",
        "# --- OpenAI API í‚¤ ì„¤ì • ---\n",
        "OPENAI_API_KEY = None\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY') # Colab Secrets ìš°ì„  í™•ì¸\n",
        "    if not OPENAI_API_KEY: OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') # í™˜ê²½ ë³€ìˆ˜ í™•ì¸\n",
        "    if not OPENAI_API_KEY: raise ValueError(\"OpenAI API í‚¤ë¥¼ Colab Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "    print(\"[ì„±ê³µ] OpenAI API í‚¤ ë¡œë“œ ë° ì„¤ì • ì™„ë£Œ.\")\n",
        "except Exception as e:\n",
        "    print(f\"[ì‹¤íŒ¨] OpenAI API í‚¤ ë¡œë“œ/ì„¤ì • ì˜¤ë¥˜: {e}\")\n",
        "    print(\"   !! OpenAI API í‚¤ ì—†ì´ëŠ” ì´í›„ LLM, RAGAS í‰ê°€ ë“± ì‚¬ìš© ë¶ˆê°€ !!\")\n",
        "\n",
        "# --- Google AI API í‚¤ ì„¤ì • (ì„ íƒ ì‚¬í•­) ---\n",
        "# ë§Œì•½ Google Embedding ë“±ì„ ê³„ì† ì‚¬ìš©í•œë‹¤ë©´ ìœ ì§€, ì•„ë‹ˆë©´ ì œê±° ê°€ëŠ¥\n",
        "# GOOGLE_API_KEY = None\n",
        "# try:\n",
        "#     GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "#     if not GOOGLE_API_KEY: GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
        "#     if GOOGLE_API_KEY:\n",
        "#         os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "#         import google.generativeai as genai\n",
        "#         genai.configure(api_key=GOOGLE_API_KEY)\n",
        "#         print(\"[ì •ë³´] Google API í‚¤ ë¡œë“œ ë° ì„¤ì • ì™„ë£Œ (ì„ íƒ ì‚¬í•­).\")\n",
        "#     # else: print(\"[ì •ë³´] Google API í‚¤ ë¡œë“œ ì•ˆ ë¨ (ì„ íƒ ì‚¬í•­).\") # í‚¤ ì—†ì–´ë„ ì˜¤ë¥˜ ì•„ë‹˜\n",
        "# except Exception as e: print(f\"[ê²½ê³ ] Google API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì„ íƒ ì‚¬í•­): {e}\")\n",
        "# =========================\n",
        "\n",
        "# --- ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ìœ ì§€) ---\n",
        "if DRIVE_MOUNTED:\n",
        "    base_folder_path = \"/content/drive/MyDrive/nomu_dataset3\"\n",
        "    result_dir = \"/content/drive/MyDrive/nomu_rag_result\"\n",
        "else:\n",
        "    base_folder_path = \"./nomu_data_local\"\n",
        "    result_dir = \"./nomu_rag_result_local\"\n",
        "\n",
        "print(f\"ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰ ê²½ë¡œ: {base_folder_path}\")\n",
        "print(f\"ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {result_dir}\")\n",
        "os.makedirs(base_folder_path, exist_ok=True)\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "# Google Sheets í´ë” ID (í•„ìš”ì‹œ ìœ ì§€)\n",
        "target_folder_id = \"1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U\"\n",
        "print(f\"Google Sheets ê²€ìƒ‰ ëŒ€ìƒ í´ë” ID: {target_folder_id}\")\n",
        "\n",
        "# --- ê²°ê³¼ íŒŒì¼ ê²½ë¡œ (ê¸°ì¡´ ìœ ì§€) ---\n",
        "vs_status_file = os.path.join(result_dir, \"vectorstore_build_status.json\")\n",
        "vs_checkpoint_path = os.path.join(result_dir, \"faiss_index_nomu_checkpoint\")\n",
        "vs_final_save_path = os.path.join(result_dir, \"faiss_index_nomu_final\")\n",
        "bm25_data_save_path = os.path.join(result_dir, \"split_texts_for_bm25.pkl\")\n",
        "\n",
        "print(\"--- ë‹¨ê³„ 2: í™˜ê²½ ì„¤ì • ì™„ë£Œ (OpenAI API í‚¤ ì„¤ì •ë¨) ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AC7tRibqyM4"
      },
      "source": [
        "# 3: ë°ì´í„° ë¡œë”©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK4AoDtiq8s2",
        "outputId": "a891305f-7967-4e5b-e422-80665ffd2a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 3: ë°ì´í„° ë¡œë”© ì‹œì‘ ---\n",
            "\n",
            "--- 3.1 Google Sheets íŒŒì¼ ë¡œë”© ---\n",
            "  GoogleDriveLoaderë¡œ í´ë” '1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U' ë¡œë”© ì¤‘...\n",
            "  [ì„±ê³µ] Google Sheets ë¡œë”© (89ê°œ ì¡°ê°).\n",
            "\n",
            "--- 3.2 ë‹¤ë¥¸ íŒŒì¼ í˜•ì‹ ë¡œë”© (DirectoryLoader) ---\n",
            "\n",
            "  '.pdf' í™•ì¥ì ë¡œë”© (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125/125 [00:20<00:00,  6.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [ì„±ê³µ] '.pdf' ë¡œë”© (1748ê°œ ì¡°ê°).\n",
            "\n",
            "  '.xlsx' í™•ì¥ì ë¡œë”© (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [ì •ë³´] '.xlsx' íŒŒì¼ ì—†ìŒ.\n",
            "\n",
            "  '.xls' í™•ì¥ì ë¡œë”© (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [ì •ë³´] '.xls' íŒŒì¼ ì—†ìŒ.\n",
            "\n",
            "  '.csv' í™•ì¥ì ë¡œë”© (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [ì •ë³´] '.csv' íŒŒì¼ ì—†ìŒ.\n",
            "\n",
            "  '.txt' í™•ì¥ì ë¡œë”© (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [ì •ë³´] '.txt' íŒŒì¼ ì—†ìŒ.\n",
            "\n",
            "--- ìµœì¢… ë¡œë“œëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: 1837 ---\n",
            "\n",
            "--- ë¡œë“œëœ ë¬¸ì„œ íƒ€ì…ë³„ ê°œìˆ˜ ---\n",
            "  - google_sheet: 89ê°œ ì¡°ê°\n",
            "  - pdf: 1748ê°œ ì¡°ê°\n",
            "\n",
            "--- ì²« ë¡œë“œ ë¬¸ì„œ ìƒ˜í”Œ ---\n",
            "  íƒ€ì…: google_sheet\n",
            "  ë©”íƒ€ë°ì´í„°: {'source': 'https://docs.google.com/spreadsheets/d/1QeMvmrcYe6QQ8L1n6o1NQ7ASTPafmzdbuDAAKXFCu3k/edit?gid=0', 'title': 'filtered_qa_dataset - Sheet1', 'row': 1, 'file_type': 'google_sheet'}\n",
            "  ë‚´ìš©(200ì): No.: 0\n",
            "question: ê·¼ë¡œê³„ì•½ì´ ë¯¸ì„±ë…„ìì—ê²Œ ë¶ˆë¦¬í•˜ë‹¤ê³  ì¸ì •ë˜ëŠ” ê²½ìš° ë¯¸ì„±ë…„í›„ê²¬ì¸ì€ ê·¸ ê³„ì•½ì„ í•´ì§€í•  ìˆ˜ ìˆë‚˜ìš”?\n",
            "answer: ë„¤. ê·¼ë¡œê³„ì•½ì„ í•´ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "ground_truths: [\"ã€Œê·¼ë¡œê¸°ì¤€ë²•ã€ ì œ67ì¡° ì œ2í•­ì€ ì¹œê¶Œì,í›„ê²¬ì¸ ë˜ëŠ” ê³ ìš©ë…¸ë™ë¶€ì¥ê´€ì€ ê·¼ë¡œê³„ì•½ì´ ë¯¸ì„±ë…„ìì—ê²Œ ë¶ˆë¦¬í•˜ë‹¤ê³  ì¸ì •í•˜ëŠ” ê²½ìš°ì—ëŠ” ì´ë¥¼ í•´ì§€í•  ìˆ˜ ìˆë‹¤.\"ë¼ê³  ê·œì •...\n",
            "\n",
            "--- ë‹¨ê³„ 3: ë°ì´í„° ë¡œë”© ì™„ë£Œ ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 3: ë°ì´í„° ë¡œë”©  ===\n",
        "\n",
        "print(\"\\n--- ë‹¨ê³„ 3: ë°ì´í„° ë¡œë”© ì‹œì‘ ---\")\n",
        "\n",
        "loaded_documents = []\n",
        "loading_errors = {}\n",
        "\n",
        "# --- 3.1. Google Sheets ë¡œë”© ---\n",
        "print(\"\\n--- 3.1 Google Sheets íŒŒì¼ ë¡œë”© ---\")\n",
        "if not target_folder_id or \"YOUR_\" in target_folder_id:\n",
        "    print(\"  âš ï¸ ê²½ê³ : Google Drive í´ë” IDê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ Google Sheet ë¡œë”© ê±´ë„ˆ<0xEB><0x9A><A9>ë‹ˆë‹¤.\")\n",
        "elif not DRIVE_MOUNTED:\n",
        "     print(\"  âš ï¸ ê²½ê³ : Google Driveê°€ ë§ˆìš´íŠ¸ë˜ì§€ ì•Šì•„ Google Sheet ë¡œë”© ê±´ë„ˆ<0xEB><0x9A><A9>ë‹ˆë‹¤.\")\n",
        "else:\n",
        "    try:\n",
        "        gsheet_loader = GoogleDriveLoader(folder_id=target_folder_id, file_types=[\"sheet\"], recursive=True)\n",
        "        print(f\"  GoogleDriveLoaderë¡œ í´ë” '{target_folder_id}' ë¡œë”© ì¤‘...\")\n",
        "        gsheet_docs = gsheet_loader.load()\n",
        "        if gsheet_docs:\n",
        "            print(f\"  [ì„±ê³µ] Google Sheets ë¡œë”© ({len(gsheet_docs)}ê°œ ì¡°ê°).\")\n",
        "            for doc in gsheet_docs:\n",
        "                doc.metadata['file_type'] = 'google_sheet'\n",
        "                if 'source' not in doc.metadata and 'id' in doc.metadata:\n",
        "                    doc.metadata['source'] = f\"https://docs.google.com/spreadsheets/d/{doc.metadata['id']}\"\n",
        "            loaded_documents.extend(gsheet_docs)\n",
        "        else: print(\"  [ì •ë³´] í•´ë‹¹ í´ë”ì— Google Sheets íŒŒì¼ ì—†ìŒ.\")\n",
        "    except ImportError as ie: print(f\"  âŒ ì„í¬íŠ¸ ì˜¤ë¥˜: {ie}. ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”.\"); loading_errors['google_sheets'] = str(ie)\n",
        "    except Exception as e: error_msg = f\"{type(e).__name__}: {e}\"; print(f\"  âŒ ë¡œë”© ì˜¤ë¥˜: {error_msg}\"); loading_errors['google_sheets'] = error_msg\n",
        "\n",
        "# --- 3.2. ë‹¤ë¥¸ íŒŒì¼ í˜•ì‹ ë¡œë”© (PDF, Excel, CSV, TXT ë“±) ---\n",
        "print(\"\\n--- 3.2 ë‹¤ë¥¸ íŒŒì¼ í˜•ì‹ ë¡œë”© (DirectoryLoader) ---\")\n",
        "if not DRIVE_MOUNTED and not os.path.exists(base_folder_path):\n",
        "    print(f\"  âš ï¸ ê²½ê³ : Drive ë¯¸ë§ˆìš´íŠ¸ ë° ë¡œì»¬ ê²½ë¡œ({base_folder_path}) ì—†ìŒ. íŒŒì¼ ë¡œë”© ê±´ë„ˆ<0xEB><0x9A><A9>ë‹ˆë‹¤.\")\n",
        "else:\n",
        "    LOADER_MAPPING = { \".pdf\": (PyPDFLoader, {}), \".xlsx\": (UnstructuredExcelLoader, {\"mode\": \"single\"}), \".xls\": (UnstructuredExcelLoader, {\"mode\": \"single\"}), \".csv\": (CSVLoader, {\"encoding\": \"utf-8\"}), \".txt\": (UnstructuredFileLoader, {}) }\n",
        "    supported_extensions = list(LOADER_MAPPING.keys())\n",
        "    for ext in supported_extensions:\n",
        "        print(f\"\\n  '{ext}' í™•ì¥ì ë¡œë”© ({base_folder_path})...\")\n",
        "        loader_cls, loader_args = LOADER_MAPPING[ext]\n",
        "        try:\n",
        "            loader = DirectoryLoader( base_folder_path, glob=f\"**/*{ext}\", loader_cls=loader_cls, loader_kwargs=loader_args, recursive=True, show_progress=True, use_multithreading=True, silent_errors=False )\n",
        "            docs = loader.load()\n",
        "            if docs:\n",
        "                print(f\"    [ì„±ê³µ] '{ext}' ë¡œë”© ({len(docs)}ê°œ ì¡°ê°).\")\n",
        "                for doc in docs: doc.metadata['file_type'] = ext.lstrip('.')\n",
        "                loaded_documents.extend(docs)\n",
        "            else: print(f\"    [ì •ë³´] '{ext}' íŒŒì¼ ì—†ìŒ.\")\n",
        "        except ImportError as ie: error_msg = f\"{ie}\"; print(f\"    âŒ ì„í¬íŠ¸ ì˜¤ë¥˜: {error_msg}\"); loading_errors[f'loader_{ext}'] = f\"ImportError: {error_msg}\"\n",
        "        except Exception as e: error_msg = f\"{type(e).__name__}: {e}\"; print(f\"    âŒ ë¡œë”© ì˜¤ë¥˜: {error_msg}\"); loading_errors[f'loader_{ext}'] = error_msg\n",
        "\n",
        "# --- 3.3. ë¡œë”© ê²°ê³¼ ìš”ì•½ ---\n",
        "print(f\"\\n--- ìµœì¢… ë¡œë“œëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(loaded_documents)} ---\")\n",
        "doc_type_counts = {}\n",
        "for doc in loaded_documents: file_type = doc.metadata.get('file_type', 'unknown'); doc_type_counts[file_type] = doc_type_counts.get(file_type, 0) + 1\n",
        "print(\"\\n--- ë¡œë“œëœ ë¬¸ì„œ íƒ€ì…ë³„ ê°œìˆ˜ ---\");\n",
        "if doc_type_counts: [print(f\"  - {f_type}: {count}ê°œ ì¡°ê°\") for f_type, count in sorted(doc_type_counts.items())]\n",
        "else: print(\"  ë¡œë“œëœ ë¬¸ì„œ ì—†ìŒ.\")\n",
        "if loading_errors: print(\"\\n--- ë¡œë”© ì˜¤ë¥˜ ìš”ì•½ ---\"); [print(f\"  - {src}: {err}\") for src, err in loading_errors.items()]\n",
        "if loaded_documents:\n",
        "    print(\"\\n--- ì²« ë¡œë“œ ë¬¸ì„œ ìƒ˜í”Œ ---\")\n",
        "    try: first_doc = loaded_documents[0]; print(f\"  íƒ€ì…: {first_doc.metadata.get('file_type')}\\n  ë©”íƒ€ë°ì´í„°: {first_doc.metadata}\\n  ë‚´ìš©(200ì): {first_doc.page_content[:200]}...\")\n",
        "    except Exception as e: print(f\"  !! ìƒ˜í”Œ ì¶œë ¥ ì˜¤ë¥˜: {e}\")\n",
        "else: print(\"\\në¡œë“œëœ ë¬¸ì„œ ì—†ìŒ. ê²½ë¡œ, íŒŒì¼ í˜•ì‹, ê¶Œí•œ í™•ì¸ í•„ìš”.\")\n",
        "print(\"\\n--- ë‹¨ê³„ 3: ë°ì´í„° ë¡œë”© ì™„ë£Œ ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L27LiqhHrCtN"
      },
      "source": [
        "# 4: í…ìŠ¤íŠ¸ ë¶„í•  (Chunking, íŒ¨í„´ ê¸°ë°˜ + ê¸¸ì´ ì œí•œ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "6d6419e0e426482d800a6d81917cd486",
            "dce05db05e5b4356bd212545971abd77",
            "207961f0a05248469ff92cb2cc90a6a1",
            "9dda8dc862654ed6adcd8fb009810080",
            "72e4aba7c9a046b298b9c1e655cc1e74",
            "be3fec1295264a4b84d044f23a608788",
            "a2b329d078164799b200eab8d49aee53",
            "344d06e18ea349419424771782a5ed28",
            "781c755903624a0684c2bf57f45a3334",
            "24baabd7703d49afbf3b7394ecb1a1ae",
            "2b00383503d648f8a56730358062d625"
          ]
        },
        "id": "uIt8ueGb6tMk",
        "outputId": "4c12240e-1de6-47ce-f2f8-33a8d2b18926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 4: í…ìŠ¤íŠ¸ ë¶„í•  ì‹œì‘ (íŒ¨í„´ ê¸°ë°˜ ì ìš©) ---\n",
            "[ì •ë³´] íŒ¨í„´ ê¸°ë°˜ ì²­í‚¹ ì‹œë„ (ë²•ë¥  PDF ëŒ€ìƒ). ê¸°ì¤€ í¬ê¸°=500, ì¤‘ì²©=100\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d6419e0e426482d800a6d81917cd486",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "ë¬¸ì„œ ì²­í‚¹ ì¤‘:   0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[ì„±ê³µ] ì´ 1837ê°œ ì›ë³¸ ì¡°ê° ì²˜ë¦¬ -> 9429ê°œ ì²­í¬ ë¶„í•  ì™„ë£Œ.\n",
            "\n",
            "ì²« ë¶„í•  ì²­í¬ ìƒ˜í”Œ:\n",
            "  ë‚´ìš©(200ì): No.: 0\n",
            "question: ê·¼ë¡œê³„ì•½ì´ ë¯¸ì„±ë…„ìì—ê²Œ ë¶ˆë¦¬í•˜ë‹¤ê³  ì¸ì •ë˜ëŠ” ê²½ìš° ë¯¸ì„±ë…„í›„ê²¬ì¸ì€ ê·¸ ê³„ì•½ì„ í•´ì§€í•  ìˆ˜ ìˆë‚˜ìš”?\n",
            "answer: ë„¤. ê·¼ë¡œê³„ì•½ì„ í•´ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "ground_truths: [\"ã€Œê·¼ë¡œê¸°ì¤€ë²•ã€ ì œ67ì¡° ì œ2í•­ì€ ì¹œê¶Œì,í›„ê²¬ì¸ ë˜ëŠ” ê³ ìš©ë…¸ë™ë¶€ì¥ê´€ì€ ê·¼ë¡œê³„ì•½ì´ ë¯¸ì„±ë…„ìì—ê²Œ ë¶ˆë¦¬í•˜ë‹¤ê³  ì¸ì •í•˜ëŠ” ê²½ìš°ì—ëŠ” ì´ë¥¼ í•´ì§€í•  ìˆ˜ ìˆë‹¤.\"ë¼ê³  ê·œì •...\n",
            "  ë©”íƒ€ë°ì´í„°: {'source': 'https://docs.google.com/spreadsheets/d/1QeMvmrcYe6QQ8L1n6o1NQ7ASTPafmzdbuDAAKXFCu3k/edit?gid=0', 'title': 'filtered_qa_dataset - Sheet1', 'row': 1, 'file_type': 'google_sheet'}\n",
            "\n",
            "[ì •ë³´] ëª¨ë“  ì²­í¬ í¬ê¸° ë¹„êµì  ì •ìƒ.\n",
            "--- ë‹¨ê³„ 4: í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 4: í…ìŠ¤íŠ¸ ë¶„í•  (íŒ¨í„´ ê¸°ë°˜ + ê¸¸ì´ ì œí•œ) ===\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document # Document í´ë˜ìŠ¤ ì„í¬íŠ¸ í™•ì¸\n",
        "\n",
        "print(\"\\n--- ë‹¨ê³„ 4: í…ìŠ¤íŠ¸ ë¶„í•  ì‹œì‘ (íŒ¨í„´ ê¸°ë°˜ ì ìš©) ---\")\n",
        "\n",
        "split_texts = []\n",
        "chunk_size_setting = 500\n",
        "chunk_overlap_setting = 100\n",
        "# ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ì‚¬ìš©í•  fallback ìŠ¤í”Œë¦¬í„°\n",
        "fallback_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size_setting,\n",
        "    chunk_overlap=chunk_overlap_setting\n",
        ")\n",
        "\n",
        "if 'loaded_documents' in locals() and loaded_documents:\n",
        "    print(f\"[ì •ë³´] íŒ¨í„´ ê¸°ë°˜ ì²­í‚¹ ì‹œë„ (ë²•ë¥  PDF ëŒ€ìƒ). ê¸°ì¤€ í¬ê¸°={chunk_size_setting}, ì¤‘ì²©={chunk_overlap_setting}\")\n",
        "    processed_docs_count = 0\n",
        "    skipped_docs_count = 0\n",
        "\n",
        "    for doc in tqdm(loaded_documents, desc=\"ë¬¸ì„œ ì²­í‚¹ ì¤‘\"):\n",
        "        doc_content = doc.page_content\n",
        "        doc_metadata = doc.metadata\n",
        "        file_type = doc_metadata.get('file_type', 'unknown')\n",
        "\n",
        "        # --- PDF ë¬¸ì„œ(ë²•ë¥  ë¬¸ì„œë¡œ ê°„ì£¼)ì—ë§Œ íŒ¨í„´ ê¸°ë°˜ ì ìš© ---\n",
        "        # íŒŒì¼ íƒ€ì…ì´ 'pdf'ê°€ ì•„ë‹ˆê±°ë‚˜ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ìŠ¤í”Œë¦¬í„° ì‚¬ìš©\n",
        "        if file_type != 'pdf' or not doc_content.strip():\n",
        "            if doc_content.strip(): # ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ë¶„í• \n",
        "                 try:\n",
        "                     sub_chunks = fallback_splitter.split_text(doc_content)\n",
        "                     for chunk_text in sub_chunks:\n",
        "                         split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))\n",
        "                     processed_docs_count += 1\n",
        "                 except Exception as e:\n",
        "                      print(f\"\\n!! ê¸°ë³¸ ë¶„í•  ì˜¤ë¥˜ (íƒ€ì…: {file_type}, ì†ŒìŠ¤: {doc_metadata.get('source', 'N/A')}): {e}\")\n",
        "                      skipped_docs_count += 1\n",
        "            else:\n",
        "                skipped_docs_count += 1\n",
        "            continue # ë‹¤ìŒ ë¬¸ì„œë¡œ ì´ë™\n",
        "\n",
        "        # --- íŒ¨í„´ ê¸°ë°˜ ì²­í‚¹ ë¡œì§ (PDF ëŒ€ìƒ) ---\n",
        "        try:\n",
        "            # 1. \"ì œXì¡°\" íŒ¨í„´ìœ¼ë¡œ ë¨¼ì € í¬ê²Œ ë‚˜ëˆ„ê¸°\n",
        "            # ì •ê·œí‘œí˜„ì‹: 'ì œ' + (ì„ íƒì  ê³µë°±) + ìˆ«ì + (ì„ íƒì  ê³µë°±) + 'ì¡°'\n",
        "            # re.splitì€ êµ¬ë¶„ìë„ ê²°ê³¼ì— í¬í•¨ì‹œí‚¤ë¯€ë¡œ, ì´ë¥¼ í™œìš©í•˜ì—¬ ì¡° ì œëª©ì„ ìœ ì§€\n",
        "            preliminary_chunks_with_title = re.split(r'(ì œ\\s?\\d+\\s?ì¡°)', doc_content)\n",
        "\n",
        "            current_chunk_content = \"\"\n",
        "            # ì²« ë¶€ë¶„ ì²˜ë¦¬ (ì²« 'ì œXì¡°' ì´ì „ ë‚´ìš©)\n",
        "            if preliminary_chunks_with_title[0].strip():\n",
        "                 current_chunk_content = preliminary_chunks_with_title[0].strip()\n",
        "\n",
        "            # 'ì œXì¡°' ì œëª©ê³¼ ê·¸ ë‚´ìš©ì„ ë¬¶ì–´ì„œ ì²˜ë¦¬\n",
        "            for i in range(1, len(preliminary_chunks_with_title), 2):\n",
        "                title = preliminary_chunks_with_title[i] # ì˜ˆ: \"ì œ1ì¡°\"\n",
        "                content = preliminary_chunks_with_title[i+1] if (i+1) < len(preliminary_chunks_with_title) else \"\"\n",
        "\n",
        "                article_block = title + content # \"ì œ1ì¡° ë‚´ìš©...\"\n",
        "\n",
        "                # ì´ì „ ì²­í¬ê°€ ìˆê³  + í˜„ì¬ ì¡°í•­ì„ í•©ì³ë„ í¬ê¸°ë¥¼ ë„˜ì§€ ì•Šìœ¼ë©´ í•©ì¹¨\n",
        "                # (ì§§ì€ ì¡°í•­ë“¤ì´ í•©ì³ì§€ëŠ” íš¨ê³¼)\n",
        "                if current_chunk_content and (len(current_chunk_content) + len(article_block) <= chunk_size_setting):\n",
        "                    current_chunk_content += \"\\n\\n\" + article_block # ë¬¸ë‹¨ êµ¬ë¶„ ì¶”ê°€\n",
        "                else:\n",
        "                    # ì´ì „ ì²­í¬ê°€ ë„ˆë¬´ ê¸¸ì—ˆê±°ë‚˜, í•©ì¹˜ë©´ ê¸¸ì–´ì§€ëŠ” ê²½ìš°\n",
        "                    # ì´ì „ ì²­í¬ë¥¼ ìµœì¢… ì²˜ë¦¬í•˜ê³  í˜„ì¬ ì¡°í•­ìœ¼ë¡œ ìƒˆ ì²­í¬ ì‹œì‘\n",
        "                    if current_chunk_content: # ì´ì „ ì²­í¬ ë‚´ìš©ì´ ìˆìœ¼ë©´\n",
        "                        # ì´ì „ ì²­í¬ê°€ ìµœëŒ€ í¬ê¸°ë¥¼ ë„˜ëŠ”ì§€ í™•ì¸\n",
        "                        if len(current_chunk_content) > chunk_size_setting:\n",
        "                            # ë„˜ìœ¼ë©´ fallback ìŠ¤í”Œë¦¬í„°ë¡œ ì¬ë¶„í• \n",
        "                            sub_chunks = fallback_splitter.split_text(current_chunk_content)\n",
        "                            for chunk_text in sub_chunks:\n",
        "                                split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))\n",
        "                        else:\n",
        "                            # ì•ˆ ë„˜ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€\n",
        "                            split_texts.append(Document(page_content=current_chunk_content, metadata=doc_metadata.copy()))\n",
        "\n",
        "                    # í˜„ì¬ ì¡°í•­ìœ¼ë¡œ ìƒˆ ì²­í¬ ì‹œì‘\n",
        "                    current_chunk_content = article_block.strip() # ìƒˆ ì²­í¬ ì‹œì‘\n",
        "\n",
        "            # ë§ˆì§€ë§‰ ë‚¨ì€ ì²­í¬ ì²˜ë¦¬\n",
        "            if current_chunk_content:\n",
        "                 if len(current_chunk_content) > chunk_size_setting:\n",
        "                     sub_chunks = fallback_splitter.split_text(current_chunk_content)\n",
        "                     for chunk_text in sub_chunks:\n",
        "                         split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))\n",
        "                 else:\n",
        "                     split_texts.append(Document(page_content=current_chunk_content, metadata=doc_metadata.copy()))\n",
        "\n",
        "            processed_docs_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n!! íŒ¨í„´ ê¸°ë°˜ ë¶„í•  ì˜¤ë¥˜ (ì†ŒìŠ¤: {doc_metadata.get('source', 'N/A')}): {e}\")\n",
        "            # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë¬¸ì„œëŠ” ê¸°ë³¸ ìŠ¤í”Œë¦¬í„°ë¡œ ì²˜ë¦¬ ì‹œë„ (ì„ íƒ ì‚¬í•­)\n",
        "            try:\n",
        "                sub_chunks = fallback_splitter.split_text(doc_content)\n",
        "                for chunk_text in sub_chunks:\n",
        "                     split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))\n",
        "                print(f\"  -> ì˜¤ë¥˜ ë°œìƒí•˜ì—¬ ê¸°ë³¸ ë¶„í•  ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•¨.\")\n",
        "                processed_docs_count += 1\n",
        "            except Exception as fallback_e:\n",
        "                 print(f\"  -> ê¸°ë³¸ ë¶„í•  ë°©ì‹ë„ ì‹¤íŒ¨: {fallback_e}\")\n",
        "                 skipped_docs_count += 1\n",
        "\n",
        "    print(f\"\\n[ì„±ê³µ] ì´ {processed_docs_count}ê°œ ì›ë³¸ ì¡°ê° ì²˜ë¦¬ -> {len(split_texts)}ê°œ ì²­í¬ ë¶„í•  ì™„ë£Œ.\")\n",
        "    if skipped_docs_count > 0: print(f\"[ê²½ê³ ] {skipped_docs_count}ê°œ ì›ë³¸ ì¡°ê° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒí•˜ì—¬ ê±´ë„ˆ<0xEB><0x9B><0x81>.\")\n",
        "\n",
        "    if split_texts:\n",
        "         print(\"\\nì²« ë¶„í•  ì²­í¬ ìƒ˜í”Œ:\")\n",
        "         print(f\"  ë‚´ìš©(200ì): {split_texts[0].page_content[:200]}...\")\n",
        "         print(f\"  ë©”íƒ€ë°ì´í„°: {split_texts[0].metadata}\")\n",
        "         # í¬ê¸° í™•ì¸ (Fallback ì ìš© í›„ì—ë„ í´ ìˆ˜ ìˆìŒ)\n",
        "         oversized = [len(c.page_content) for c in split_texts if len(c.page_content) > chunk_size_setting + chunk_overlap_setting] # ì˜¤ë²„ë© ê³ ë ¤\n",
        "         if oversized: print(f\"\\n[ê²½ê³ ] {len(oversized)}ê°œ ì²­í¬ê°€ ìµœëŒ€ í¬ê¸°({chunk_size_setting}ì)ë¥¼ ì•½ê°„ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ (ìµœëŒ€ {max(oversized)}ì).\")\n",
        "         else: print(f\"\\n[ì •ë³´] ëª¨ë“  ì²­í¬ í¬ê¸° ë¹„êµì  ì •ìƒ.\")\n",
        "    else: print(\"[ì •ë³´] ìƒì„±ëœ ì²­í¬ ì—†ìŒ.\")\n",
        "\n",
        "else:\n",
        "    print(\"!! ë¶„í• í•  ë¡œë“œëœ ë¬¸ì„œ ì—†ìŒ.\")\n",
        "\n",
        "print(\"--- ë‹¨ê³„ 4: í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbxJAJcwr6Uk"
      },
      "source": [
        "# 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • (text-embedding-3-large)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O55krQoYcdmf",
        "outputId": "bb19aca3-b682-469b-81bb-f03c3b43263e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì‹œì‘ (OpenAI: text-embedding-3-large) ---\n",
            "[ì •ë³´] langchain_openai.OpenAIEmbeddings ì„í¬íŠ¸ í™•ì¸.\n",
            "[ì •ë³´] OpenAI ì„ë² ë”© ëª¨ë¸ (text-embedding-3-large) ì„¤ì • ì‹œë„...\n",
            "[ì •ë³´] ë‹¨ê³„ 2ì—ì„œ ì„¤ì •ëœ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
            "[ì •ë³´] ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ (OpenAI API í˜¸ì¶œ)...\n",
            "[ì„±ê³µ] OpenAI ì„ë² ë”© ëª¨ë¸ (text-embedding-3-large) ì„¤ì • ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ.\n",
            "--- ë‹¨ê³„ 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì™„ë£Œ ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • (OpenAI: text-embedding-3-large) ===\n",
        "print(\"\\n--- ë‹¨ê³„ 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì‹œì‘ (OpenAI: text-embedding-3-large) ---\")\n",
        "\n",
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "try:\n",
        "    # OpenAI ì„ë² ë”©ì„ ìœ„í•œ í´ë˜ìŠ¤ ì„í¬íŠ¸\n",
        "    # pip install -U langchain-openai  <-- ë¨¼ì € ì´ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    print(\"[ì •ë³´] langchain_openai.OpenAIEmbeddings ì„í¬íŠ¸ í™•ì¸.\")\n",
        "except ImportError as e:\n",
        "    print(f\"!! [ì˜¤ë¥˜] í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}\")\n",
        "    print(\"   langchain-openai, openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "    # í•„ìš”í•œ í´ë˜ìŠ¤ê°€ ì—†ìœ¼ë©´ ì´í›„ ì§„í–‰ ë¶ˆê°€\n",
        "    OpenAIEmbeddings = None\n",
        "\n",
        "embeddings = None # ìµœì¢… ì„ë² ë”© ê°ì²´ ë³€ìˆ˜ ì´ˆê¸°í™”\n",
        "\n",
        "if OpenAIEmbeddings: # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì„±ê³µ ì‹œ ì§„í–‰\n",
        "    try:\n",
        "        # ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì§€ì •\n",
        "        openai_model_name = \"text-embedding-3-large\"\n",
        "        print(f\"[ì •ë³´] OpenAI ì„ë² ë”© ëª¨ë¸ ({openai_model_name}) ì„¤ì • ì‹œë„...\")\n",
        "        print(\"[ì •ë³´] ë‹¨ê³„ 2ì—ì„œ ì„¤ì •ëœ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "\n",
        "        # <<< OpenAIEmbeddings í´ë˜ìŠ¤ ì‚¬ìš© (API í‚¤ ìë™ ê°ì§€) >>>\n",
        "        # ë‹¨ê³„ 2ì—ì„œ os.environ[\"OPENAI_API_KEY\"] ê°€ ì„¤ì •ë˜ì—ˆìœ¼ë¯€ë¡œ,\n",
        "        # API í‚¤ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•  í•„ìš” ì—†ì´ ìë™ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "        embeddings = OpenAIEmbeddings(\n",
        "            model=openai_model_name # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ë§Œ ì§€ì •\n",
        "            # openai_api_key íŒŒë¼ë¯¸í„° ìƒëµ\n",
        "        )\n",
        "        # ------------------------------------------------------\n",
        "\n",
        "        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (API í‚¤ ìœ íš¨ì„± ë° í†µì‹  í™•ì¸)\n",
        "        print(\"[ì •ë³´] ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ (OpenAI API í˜¸ì¶œ)...\")\n",
        "        _ = embeddings.embed_query(\"í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.\")\n",
        "        print(f\"[ì„±ê³µ] OpenAI ì„ë² ë”© ëª¨ë¸ ({openai_model_name}) ì„¤ì • ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!! [ì˜¤ë¥˜] OpenAI ì„ë² ë”© ëª¨ë¸ ({openai_model_name}) ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
        "        print(\"   - ë‹¨ê³„ 2ì—ì„œ OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\") # <<< ì˜¤ë¥˜ ë©”ì‹œì§€ ìˆ˜ì •\n",
        "        print(\"   - OpenAI API í‚¤ ìì²´ì˜ ìœ íš¨ì„± ë° í• ë‹¹ëŸ‰(quota)ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "        print(\"   - ì¸í„°ë„· ì—°ê²° ìƒíƒœ ë° OpenAI ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "        print(\"   - ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬(langchain-openai, openai) ì„¤ì¹˜ ë° í˜¸í™˜ì„±ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "        embeddings = None # ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì •\n",
        "else:\n",
        "    print(\"!! í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(OpenAIEmbeddings) ì„í¬íŠ¸ ì‹¤íŒ¨. ì„ë² ë”© ëª¨ë¸ ì„¤ì • ë¶ˆê°€.\")\n",
        "\n",
        "print(\"--- ë‹¨ê³„ 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì™„ë£Œ ---\")\n",
        "\n",
        "# OpenAI ëª¨ë¸ì€ ì™¸ë¶€ APIë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë¡œì»¬ GPU/CPU í™•ì¸ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dWM7T_R4ZIF"
      },
      "source": [
        "# ë‹¨ê³„ 6: Vector Store êµ¬ì¶• (FAISS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvIY_g-74WB4"
      },
      "source": [
        "--- 6.2: ê²½ë¡œ ì„¤ì • ---\n",
        "ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/nomu_rag_result\n",
        "  - ìƒíƒœ íŒŒì¼: /content/drive/MyDrive/nomu_rag_result/vectorstore_build_status.json\n",
        "  - ì²´í¬í¬ì¸íŠ¸ í´ë” (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_checkpoint\n",
        "  - ìµœì¢… ì¸ë±ìŠ¤ í´ë” (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_final\n",
        "  - BM25ìš© ë°ì´í„° íŒŒì¼: /content/drive/MyDrive/nomu_rag_result/split_texts_for_bm25.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1d9c58383937428cad5ee18e87311544",
            "7818ce83534c4635942d9a151432374e",
            "636c9b9a640947768574acd4ee9d76cb",
            "ffec0d2134a040cfa24e8887cce55b66",
            "ba139e0a87104ec682b12d5e81bea071",
            "a6807fa524074dccb399fa0cd95ceb0f",
            "6fef3cfa6ade4c2da9d2b1908fe69d02",
            "e5cfe01f8b92442e83c9db32622cf0e2",
            "c3bf5302a364449e8c90595b1cb060b5",
            "792144951d63405a9e7e5ce21179d713",
            "0058beb07765417c984f68278c6458e2"
          ]
        },
        "id": "jo8-0rLR3ncZ",
        "outputId": "6f4af03b-6d95-42ba-f416-203b56810326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 6: Vector Store êµ¬ì¶• ë° BM25 ë°ì´í„° ì €ì¥ ì‹œì‘ ---\n",
            "\n",
            "--- 6.1: ì„¤ì • ë° ì…ë ¥ ë³€ìˆ˜ í™•ì¸ ---\n",
            "âœ… ì…ë ¥ í™•ì¸: ì´ 9429ê°œ ì²­í¬ ë° ì„ë² ë”© ëª¨ë¸ í™•ì¸ë¨.\n",
            "\n",
            "--- 6.2: ê²½ë¡œ ì„¤ì • ---\n",
            "ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/nomu_rag_result\n",
            "  - ìƒíƒœ íŒŒì¼: /content/drive/MyDrive/nomu_rag_result/vectorstore_build_status.json\n",
            "  - ì²´í¬í¬ì¸íŠ¸ í´ë” (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_checkpoint\n",
            "  - ìµœì¢… ì¸ë±ìŠ¤ í´ë” (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_final\n",
            "  - BM25ìš© ë°ì´í„° íŒŒì¼: /content/drive/MyDrive/nomu_rag_result/split_texts_for_bm25.pkl\n",
            "\n",
            "--- 6.3: ì¥ì¹˜ í™•ì¸ ---\n",
            "â„¹ï¸ ì‚¬ìš©í•  ì¥ì¹˜: cpu\n",
            "âš ï¸ ê²½ê³ : GPU ì‚¬ìš© ë¶ˆê°€. ì‹œê°„ ì†Œìš” ì˜ˆìƒ.\n",
            "\n",
            "--- 6.4: ì´ì „ ì‘ì—… ìƒíƒœ ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ---\n",
            "  - ì´ì „ ìƒíƒœ íŒŒì¼ ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ í´ë” ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘.\n",
            "\n",
            "--- 6.5: FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘ (ì´ 9429 ì¤‘ 0ë¶€í„°, ë°°ì¹˜ 500) ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d9c58383937428cad5ee18e87311544",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FAISS êµ¬ì¶• ì¤‘:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   - ì²« ë°°ì¹˜ FAISS ìƒì„± ì™„ë£Œ (ë²¡í„° 500ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 1000ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 1500ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 2000ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 2500ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 3000ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 3500ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 4000ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 4500ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 5000ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 5500ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 6000ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 6500ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 7000ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 7500ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 8000ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 8500ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 9000ê°œ)\n",
            "\n",
            "   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° 9429ê°œ)\n",
            "\n",
            "[ì„±ê³µ] ëª¨ë“  FAISS ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ.\n",
            "\n",
            "--- 6.6: ìµœì¢… ê²°ê³¼ ì €ì¥ ---\n",
            "ğŸ’¾ ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥: /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_final\n",
            "âœ… ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ.\n",
            "ğŸ’¾ BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° ì €ì¥: /content/drive/MyDrive/nomu_rag_result/split_texts_for_bm25.pkl\n",
            "âœ… BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° (9429ê°œ ì²­í¬) ì €ì¥ ì™„ë£Œ.\n",
            "\n",
            "--- ë‹¨ê³„ 6: Vector Store ì²˜ë¦¬ ë° BM25 ë°ì´í„° ì €ì¥ ì™„ë£Œ ---\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# === ë‹¨ê³„ 6: Vector Store êµ¬ì¶• (FAISS) ë° BM25ìš© ë°ì´í„° ì €ì¥ ===\n",
        "# ì²´í¬í¬ì¸íŒ… ë°©ì‹ì„ save_localë¡œ ë³€ê²½\n",
        "\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "# import pickle # pickleì€ BM25 ë°ì´í„° ì €ì¥ì—ë§Œ ì‚¬ìš©\n",
        "import torch\n",
        "import traceback\n",
        "from tqdm.auto import tqdm\n",
        "# LangChain ê´€ë ¨ ì„í¬íŠ¸ í™•ì¸\n",
        "if 'FAISS' not in locals() or 'Document' not in locals():\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "    from langchain_core.documents import Document\n",
        "    print(\"âš ï¸ FAISS ë˜ëŠ” Document í´ë˜ìŠ¤ ì¬ì„í¬íŠ¸ë¨.\")\n",
        "if 'HuggingFaceEmbeddings' not in locals() and 'GoogleGenerativeAIEmbeddings' not in locals():\n",
        "    # ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings # ì˜ˆì‹œ\n",
        "    # from langchain_google_genai import GoogleGenerativeAIEmbeddings # ì˜ˆì‹œ\n",
        "    print(\"âš ï¸ ì„ë² ë”© í´ë˜ìŠ¤ ì¬ì„í¬íŠ¸ë¨.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- ë‹¨ê³„ 6: Vector Store êµ¬ì¶• ë° BM25 ë°ì´í„° ì €ì¥ ì‹œì‘ ---\")\n",
        "\n",
        "# --- 6.1: ì„¤ì • ë° ì…ë ¥ ë³€ìˆ˜ í™•ì¸ ---\n",
        "print(\"\\n--- 6.1: ì„¤ì • ë° ì…ë ¥ ë³€ìˆ˜ í™•ì¸ ---\")\n",
        "vectorstore = None\n",
        "batch_size = 500 # ë°°ì¹˜ í¬ê¸° í™•ì¸\n",
        "sleep_time = 5\n",
        "max_retries = 3\n",
        "total_chunks = 0\n",
        "start_index = 0\n",
        "loaded_from_checkpoint = False\n",
        "\n",
        "if 'split_texts' not in locals() or not isinstance(split_texts, list) or not split_texts: print(\"!! ì˜¤ë¥˜: 'split_texts' ì—†ìŒ. ì¤‘ë‹¨.\"); exit()\n",
        "elif 'embeddings' not in locals() or embeddings is None: print(\"!! ì˜¤ë¥˜: 'embeddings' ì—†ìŒ. ì¤‘ë‹¨.\"); exit()\n",
        "else: total_chunks = len(split_texts); print(f\"âœ… ì…ë ¥ í™•ì¸: ì´ {total_chunks}ê°œ ì²­í¬ ë° ì„ë² ë”© ëª¨ë¸ í™•ì¸ë¨.\")\n",
        "\n",
        "# --- 6.2: ê²½ë¡œ ì„¤ì • ë° ë””ë ‰í† ë¦¬ ìƒì„± ---\n",
        "print(\"\\n--- 6.2: ê²½ë¡œ ì„¤ì • ---\")\n",
        "if 'result_dir' not in locals() or not result_dir: result_dir = \"/content/drive/MyDrive/nomu_rag_result\"; print(f\"âš ï¸ result_dir ë³€ìˆ˜ ì—†ì–´ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •: {result_dir}\"); os.makedirs(result_dir, exist_ok=True)\n",
        "else: print(f\"ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {result_dir}\"); os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "vs_status_file = os.path.join(result_dir, \"vectorstore_build_status.json\")\n",
        "# <<< ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ í´ë”ë¡œ ë³€ê²½ (save_local ì‚¬ìš©) >>>\n",
        "vs_checkpoint_path = os.path.join(result_dir, \"faiss_index_nomu_checkpoint\") # .pkl ëŒ€ì‹  í´ë”ëª…\n",
        "vs_final_save_path = os.path.join(result_dir, \"faiss_index_nomu_final\")\n",
        "bm25_data_save_path = os.path.join(result_dir, \"split_texts_for_bm25.pkl\")\n",
        "\n",
        "print(f\"  - ìƒíƒœ íŒŒì¼: {vs_status_file}\")\n",
        "print(f\"  - ì²´í¬í¬ì¸íŠ¸ í´ë” (FAISS): {vs_checkpoint_path}\") # <<< ì´ë¦„ ë³€ê²½\n",
        "print(f\"  - ìµœì¢… ì¸ë±ìŠ¤ í´ë” (FAISS): {vs_final_save_path}\")\n",
        "print(f\"  - BM25ìš© ë°ì´í„° íŒŒì¼: {bm25_data_save_path}\")\n",
        "\n",
        "# --- 6.3: GPU í™•ì¸ --- (ì´ì „ê³¼ ë™ì¼)\n",
        "print(\"\\n--- 6.3: ì¥ì¹˜ í™•ì¸ ---\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'; print(f\"â„¹ï¸ ì‚¬ìš©í•  ì¥ì¹˜: {device}\");\n",
        "if device == 'cpu': print(\"âš ï¸ ê²½ê³ : GPU ì‚¬ìš© ë¶ˆê°€. ì‹œê°„ ì†Œìš” ì˜ˆìƒ.\")\n",
        "\n",
        "# --- 6.4: ì´ì „ ì‘ì—… ìƒíƒœ ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ---\n",
        "if total_chunks > 0:\n",
        "    print(\"\\n--- 6.4: ì´ì „ ì‘ì—… ìƒíƒœ ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ---\")\n",
        "    try:\n",
        "        # <<< ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ëŒ€ì‹  í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ >>>\n",
        "        if os.path.exists(vs_status_file) and os.path.isdir(vs_checkpoint_path):\n",
        "            with open(vs_status_file, 'r') as f_status: status_data = json.load(f_status); last_processed_index = status_data.get('last_processed_index', -1); start_index = last_processed_index + 1; print(f\"  - ìƒíƒœ ë¡œë“œ ì™„ë£Œ. ë§ˆì§€ë§‰ ì¸ë±ìŠ¤: {last_processed_index}\")\n",
        "\n",
        "            # <<< pickle.load ëŒ€ì‹  FAISS.load_local ì‚¬ìš© >>>\n",
        "            print(f\"  - ì²´í¬í¬ì¸íŠ¸ ì¸ë±ìŠ¤ ë¡œë”©: {vs_checkpoint_path}\")\n",
        "            if 'embeddings' in locals() and embeddings:\n",
        "                vectorstore = FAISS.load_local(\n",
        "                    folder_path=vs_checkpoint_path,\n",
        "                    embeddings=embeddings,\n",
        "                    allow_dangerous_deserialization=True # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ì¼ ë•Œë§Œ\n",
        "                )\n",
        "                print(f\"  - FAISS ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ. {vectorstore.index.ntotal} ë²¡í„° í¬í•¨.\")\n",
        "                loaded_from_checkpoint = True\n",
        "            else:\n",
        "                print(\"  !! ì˜¤ë¥˜: ì„ë² ë”© í•¨ìˆ˜('embeddings')ê°€ ì—†ì–´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘.\")\n",
        "                vectorstore = None; start_index = 0; loaded_from_checkpoint = False\n",
        "\n",
        "            if start_index >= total_chunks: print(\"  âœ… ì´ì „ FAISS ì‘ì—… ì™„ë£Œë¨.\")\n",
        "            elif loaded_from_checkpoint: print(f\"  â–¶ï¸ {start_index}ë²ˆ ì¸ë±ìŠ¤ë¶€í„° FAISS ì‘ì—… ì¬ê°œ.\")\n",
        "            # else ë¸”ë¡ì€ ìœ„ì—ì„œ ì²˜ë¦¬ë¨\n",
        "\n",
        "        else: print(f\"  - ì´ì „ ìƒíƒœ íŒŒì¼ ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ í´ë” ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘.\"); vectorstore = None; start_index = 0\n",
        "    except Exception as e: print(f\"  âš ï¸ ìƒíƒœ/ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}. ì²˜ìŒë¶€í„° ì‹œì‘.\"); traceback.print_exc(); vectorstore = None; start_index = 0\n",
        "else: print(\"â„¹ï¸ ì²˜ë¦¬í•  ì²­í¬ ì—†ìŒ.\")\n",
        "\n",
        "# --- 6.5: FAISS Vector Store êµ¬ì¶• ---\n",
        "if total_chunks > 0 and start_index < total_chunks:\n",
        "    print(f\"\\n--- 6.5: FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘ (ì´ {total_chunks} ì¤‘ {start_index}ë¶€í„°, ë°°ì¹˜ {batch_size}) ---\")\n",
        "    all_processed_successfully = True\n",
        "    try:\n",
        "        progress_bar = tqdm(range(start_index, total_chunks, batch_size), initial=start_index // batch_size, total=-(total_chunks // -batch_size), desc=\"FAISS êµ¬ì¶• ì¤‘\")\n",
        "        for i in progress_bar:\n",
        "            batch_start_idx = i; batch_end_idx = min(i + batch_size, total_chunks); batch_docs = split_texts[batch_start_idx:batch_end_idx]\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    if i == start_index and not loaded_from_checkpoint:\n",
        "                        progress_bar.set_description(f\"ì²« ë°°ì¹˜({batch_start_idx}-{batch_end_idx-1}) ìƒì„± ì¤‘\")\n",
        "                        vectorstore = FAISS.from_documents(batch_docs, embeddings)\n",
        "                        print(f\"\\n   - ì²« ë°°ì¹˜ FAISS ìƒì„± ì™„ë£Œ (ë²¡í„° {vectorstore.index.ntotal}ê°œ)\")\n",
        "                    elif vectorstore is not None:\n",
        "                        progress_bar.set_description(f\"ë°°ì¹˜({batch_start_idx}-{batch_end_idx-1}) ì¶”ê°€ ì¤‘\")\n",
        "                        vectorstore.add_documents(batch_docs)\n",
        "                        print(f\"\\n   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° {vectorstore.index.ntotal}ê°œ)\")\n",
        "                    else: raise ValueError(\"Vectorstore ê°ì²´ None ìƒíƒœ.\")\n",
        "\n",
        "                    current_processed_index = batch_end_idx - 1\n",
        "                    try:\n",
        "                        # <<< ì²´í¬í¬ì¸íŒ…: ìƒíƒœ ì €ì¥ í›„ save_local í˜¸ì¶œ >>>\n",
        "                        with open(vs_status_file, 'w') as f_status: json.dump({'last_processed_index': current_processed_index}, f_status)\n",
        "                        # ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ë®ì–´ì“°ê¸° (ì´ì „ íŒŒì¼ ì‚­ì œë¨)\n",
        "                        vectorstore.save_local(vs_checkpoint_path)\n",
        "                        # print(f\"  ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ (í´ë”: {vs_checkpoint_path}, ì¸ë±ìŠ¤: {current_processed_index})\")\n",
        "                    except Exception as e_save: print(f\"  âš ï¸ ì§„í–‰ ìƒí™©(ì²´í¬í¬ì¸íŠ¸) ì €ì¥ ì‹¤íŒ¨: {e_save}\")\n",
        "                    break # ì„±ê³µ ì‹œ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n!! ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}): {type(e).__name__} - {e}\")\n",
        "                    traceback.print_exc()\n",
        "                    if attempt < max_retries - 1: wait_time = sleep_time * (attempt + 2); progress_bar.set_description(f\"ì˜¤ë¥˜, {wait_time}ì´ˆ í›„ ì¬ì‹œë„...\"); print(f\"  ... {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ...\"); time.sleep(wait_time)\n",
        "                    else: print(\"!! ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼. êµ¬ì¶• ì¤‘ë‹¨.\"); all_processed_successfully = False; raise RuntimeError(f\"ìµœëŒ€ ì¬ì‹œë„ ì‹¤íŒ¨: {e}\")\n",
        "            if not all_processed_successfully: break\n",
        "            if batch_end_idx < total_chunks: time.sleep(sleep_time) # API Rate Limit\n",
        "\n",
        "        # --- ë£¨í”„ ì™„ë£Œ í›„ ìµœì¢… ì²˜ë¦¬ ---\n",
        "        if all_processed_successfully:\n",
        "            print(\"\\n[ì„±ê³µ] ëª¨ë“  FAISS ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ.\")\n",
        "            # --- 6.6: ìµœì¢… ê²°ê³¼ ì €ì¥ ---\n",
        "            print(\"\\n--- 6.6: ìµœì¢… ê²°ê³¼ ì €ì¥ ---\")\n",
        "            # 1. ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥ (save_local)\n",
        "            print(f\"ğŸ’¾ ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥: {vs_final_save_path}\")\n",
        "            try:\n",
        "                if vectorstore: vectorstore.save_local(vs_final_save_path); print(f\"âœ… ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ.\")\n",
        "                else: print(\"âš ï¸ ìµœì¢… ì €ì¥ ì‹œ VectorStore ê°ì²´ ì—†ìŒ.\")\n",
        "            except Exception as e: print(f\"âŒ ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}\"); traceback.print_exc()\n",
        "            # 2. BM25ìš© ë°ì´í„° ì €ì¥ (pickle)\n",
        "            print(f\"ğŸ’¾ BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° ì €ì¥: {bm25_data_save_path}\")\n",
        "            try:\n",
        "                if 'split_texts' in locals() and split_texts:\n",
        "                    # <<< pickle ì„í¬íŠ¸ í™•ì¸ >>>\n",
        "                    import pickle\n",
        "                    with open(bm25_data_save_path, 'wb') as f_texts: pickle.dump(split_texts, f_texts); print(f\"âœ… BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° ({len(split_texts)}ê°œ ì²­í¬) ì €ì¥ ì™„ë£Œ.\")\n",
        "                else: print(\"âš ï¸ BM25ìš© ë°ì´í„°('split_texts') ì—†ì–´ ì €ì¥ ë¶ˆê°€.\")\n",
        "            except Exception as e: print(f\"âŒ split_texts ì €ì¥ ì‹¤íŒ¨: {e}\"); traceback.print_exc()\n",
        "            # (ì„ íƒ) ì¤‘ê°„ íŒŒì¼ ì‚­ì œ\n",
        "            # try: ... (ì‚­ì œ ë¡œì§) ... except ...\n",
        "\n",
        "    except Exception as e_main_loop: print(f\"\\n!! FAISS êµ¬ì¶• ì‹¤íŒ¨: {e_main_loop}\"); print(f\"[ì •ë³´] ë§ˆì§€ë§‰ ì„±ê³µ ì§€ì  ë°ì´í„°ëŠ” '{vs_checkpoint_path}' í´ë”ì— ìˆì„ ìˆ˜ ìˆìŒ.\")\n",
        "\n",
        "# --- ì´ì „ ì‹¤í–‰ì—ì„œ ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ì²˜ë¦¬ ---\n",
        "elif total_chunks > 0 and start_index >= total_chunks:\n",
        "     print(\"\\nâœ… FAISS Vector Store êµ¬ì¶• ì‘ì—…ì´ ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤.\")\n",
        "     # ì™„ë£Œëœ ìƒíƒœì—ì„œë„ ìµœì¢… íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì €ì¥ ì‹œë„\n",
        "     # 1. ìµœì¢… FAISS ì¸ë±ìŠ¤ í™•ì¸ ë° ì €ì¥\n",
        "     if not os.path.isdir(vs_final_save_path): # <<< í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ >>>\n",
        "         if vectorstore: # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œëœ vectorstoreê°€ ìˆë‹¤ë©´\n",
        "             print(f\"\\nğŸ’¾ ìµœì¢… FAISS ì¸ë±ìŠ¤ í´ë” ì—†ì–´ ì €ì¥ ì‹œë„: '{vs_final_save_path}'\") # <<< print ë¬¸ ë¶„ë¦¬\n",
        "             # <<< try...except ë¸”ë¡ì„ ìƒˆ ì¤„ì—ì„œ ì‹œì‘ >>>\n",
        "             try:\n",
        "                 vectorstore.save_local(vs_final_save_path)\n",
        "                 print(\"âœ… ì €ì¥ ì™„ë£Œ.\")\n",
        "             except Exception as e:\n",
        "                 print(f\"âŒ ì €ì¥ ì˜¤ë¥˜: {e}\")\n",
        "                 traceback.print_exc()\n",
        "         else:\n",
        "             print(f\"âš ï¸ ìµœì¢… FAISS ì¸ë±ìŠ¤ í´ë” ì—†ê³ , ë¡œë“œëœ vectorstoreë„ ì—†ì–´ ì €ì¥ ë¶ˆê°€.\")\n",
        "\n",
        "     # 2. BM25ìš© ë°ì´í„° í™•ì¸ ë° ì €ì¥\n",
        "     if not os.path.exists(bm25_data_save_path):\n",
        "         if 'split_texts' in locals() and split_texts:\n",
        "             print(f\"\\nğŸ’¾ BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼({bm25_data_save_path}) ì—†ì–´ ì €ì¥ ì‹œë„...\") # <<< print ë¬¸ ë¶„ë¦¬\n",
        "             # <<< try...except ë¸”ë¡ì„ ìƒˆ ì¤„ì—ì„œ ì‹œì‘ >>>\n",
        "             try:\n",
        "                 # <<< pickle ì„í¬íŠ¸ í™•ì¸ (í•„ìš”ì‹œ) >>>\n",
        "                 import pickle\n",
        "                 with open(bm25_data_save_path, 'wb') as f:\n",
        "                     pickle.dump(split_texts, f)\n",
        "                 print(\"âœ… ì €ì¥ ì™„ë£Œ.\")\n",
        "             except Exception as e:\n",
        "                 print(f\"âŒ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
        "                 traceback.print_exc()\n",
        "         else:\n",
        "             print(f\"âš ï¸ BM25ìš© ë°ì´í„° íŒŒì¼ ì—†ê³ , split_texts ë³€ìˆ˜ë„ ì—†ì–´ ì €ì¥ ë¶ˆê°€.\")\n",
        "\n",
        "print(\"\\n--- ë‹¨ê³„ 6: Vector Store ì²˜ë¦¬ ë° BM25 ë°ì´í„° ì €ì¥ ì™„ë£Œ ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_6r7-WP5Ynw"
      },
      "source": [
        "# ë‹¨ê³„ 8: Retriever ì„¤ì • (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc2RDKm14skp",
        "outputId": "01182549-280e-465c-c0a0-208ed07f343c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 8: Retriever ì„¤ì • ì‹œì‘ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰) ---\n",
            "[ì •ë³´] ìƒˆë¡œ ìƒì„±ëœ FAISS VectorStore ì‚¬ìš©.\n",
            "[ì •ë³´] í˜„ì¬ ì„¸ì…˜ì˜ split_texts ë°ì´í„° ì‚¬ìš© (BM25ìš©).\n",
            "- Dense Retriever (FAISS) ì„¤ì • ì™„ë£Œ (k=6).\n",
            "- Sparse Retriever (BM25) ì„¤ì • ì™„ë£Œ (k=6).\n",
            "- Ensemble Retriever ì„¤ì • ì™„ë£Œ (Weights: BM25=0.7, FAISS=0.3).\n",
            "--- ë‹¨ê³„ 8: Retriever ì„¤ì • ì™„ë£Œ ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 8: Retriever ì„¤ì • (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰) ===\n",
        "\n",
        "print(\"\\n--- ë‹¨ê³„ 8: Retriever ì„¤ì • ì‹œì‘ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰) ---\")\n",
        "\n",
        "# ì‚¬ìš©í•  VectorStoreì™€ split_texts ê²°ì •\n",
        "# ë¡œë“œ ì„±ê³µ ì‹œ ë¡œë“œëœ ê²ƒ ì‚¬ìš©, ì•„ë‹ˆë©´ êµ¬ì¶• ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ê²ƒ ì‚¬ìš©\n",
        "final_vectorstore = None\n",
        "final_split_texts = None # BM25ìš©\n",
        "\n",
        "if 'loaded_vectorstore' in locals() and loaded_vectorstore:\n",
        "    final_vectorstore = loaded_vectorstore\n",
        "    print(\"[ì •ë³´] ë¡œë“œëœ FAISS VectorStore ì‚¬ìš©.\")\n",
        "elif 'vectorstore' in locals() and vectorstore:\n",
        "    final_vectorstore = vectorstore\n",
        "    print(\"[ì •ë³´] ìƒˆë¡œ ìƒì„±ëœ FAISS VectorStore ì‚¬ìš©.\")\n",
        "else:\n",
        "    print(\"!! ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ FAISS VectorStore ê°ì²´ ì—†ìŒ.\")\n",
        "\n",
        "if 'loaded_split_texts' in locals() and loaded_split_texts:\n",
        "    final_split_texts = loaded_split_texts\n",
        "    print(\"[ì •ë³´] ë¡œë“œëœ split_texts ë°ì´í„° ì‚¬ìš© (BM25ìš©).\")\n",
        "elif 'split_texts' in locals() and split_texts:\n",
        "    final_split_texts = split_texts\n",
        "    print(\"[ì •ë³´] í˜„ì¬ ì„¸ì…˜ì˜ split_texts ë°ì´í„° ì‚¬ìš© (BM25ìš©).\")\n",
        "else:\n",
        "    print(\"!! ì˜¤ë¥˜: BM25ìš© split_texts ë°ì´í„° ì—†ìŒ.\")\n",
        "\n",
        "retriever = None # ìµœì¢… ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”\n",
        "\n",
        "if final_vectorstore:\n",
        "    # Dense Retriever (FAISS)\n",
        "    faiss_retriever = final_vectorstore.as_retriever(search_kwargs={'k': 6})\n",
        "    print(f\"- Dense Retriever (FAISS) ì„¤ì • ì™„ë£Œ (k={faiss_retriever.search_kwargs.get('k')}).\")\n",
        "\n",
        "    if final_split_texts: # BM25ìš© ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ í•˜ì´ë¸Œë¦¬ë“œ ì‹œë„\n",
        "        try:\n",
        "            # Sparse Retriever (BM25)\n",
        "            bm25_retriever = BM25Retriever.from_documents(final_split_texts)\n",
        "            bm25_retriever.k = 6\n",
        "            print(f\"- Sparse Retriever (BM25) ì„¤ì • ì™„ë£Œ (k={bm25_retriever.k}).\")\n",
        "            # Ensemble Retriever\n",
        "            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.4, 0.6])\n",
        "            retriever = ensemble_retriever\n",
        "            print(f\"- Ensemble Retriever ì„¤ì • ì™„ë£Œ (Weights: BM25=0.7, FAISS=0.3).\")\n",
        "        except Exception as e:\n",
        "            print(f\"!! BM25/Ensemble ì„¤ì • ì‹¤íŒ¨: {e}. Dense Retrieverë§Œ ì‚¬ìš©.\")\n",
        "            retriever = faiss_retriever # Fallback\n",
        "    else:\n",
        "        print(\"âš ï¸ BM25 ë°ì´í„° ì—†ì–´ Dense Retriever(FAISS)ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
        "        retriever = faiss_retriever # Fallback\n",
        "else:\n",
        "    print(\"!! Vector Store ì¤€ë¹„ ì•ˆ ë¨. Retriever ì„¤ì • ë¶ˆê°€.\")\n",
        "\n",
        "print(\"--- ë‹¨ê³„ 8: Retriever ì„¤ì • ì™„ë£Œ ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X_3hcxYsxfY"
      },
      "source": [
        "# 9: LLM ì„¤ì • (OpenAI: gpt-4.1-nano)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LviMIiKoNsln"
      },
      "source": [
        "í˜„ì¬ì˜ rag íŒŒì´í”„ë¼ì¸ì€ RAGAS ì„±ëŠ¥ í‰ê°€ìš© LLMê³¼ ì‘ë‹µ ìƒì„±ìš© LLMì„ ë¶„ë¦¬í•˜ì§€ ì•Šê³  ë™ì¼í•˜ê²Œ ì‚¬ìš©í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bBYfQwWsyx9",
        "outputId": "cbfbf630-af55-4c75-a49f-a4b19e3f50ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 9: LLM ì„¤ì • ì‹œì‘ (OpenAI) ---\n",
            "[ì„±ê³µ] OpenAI (gpt-4.1-nano) LLM ë¡œë”© ì™„ë£Œ.\n",
            "--- ë‹¨ê³„ 9: LLM ì„¤ì • ì™„ë£Œ ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 9: LLM ì„¤ì • (OpenAI - gpt-4.1-nano) ===\n",
        "\n",
        "print(\"\\n--- ë‹¨ê³„ 9: LLM ì„¤ì • ì‹œì‘ (OpenAI) ---\")\n",
        "llm = None\n",
        "# OpenAI API í‚¤ ì„¤ì • ì—¬ë¶€ í™•ì¸ (ë‹¨ê³„ 2ì—ì„œ ì„¤ì •ë¨)\n",
        "if 'OPENAI_API_KEY' in os.environ and os.environ[\"OPENAI_API_KEY\"]:\n",
        "    try:\n",
        "        # === ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì§€ì • ===\n",
        "        # llm_model_name = \"gpt-4o\" # ìµœì‹  ëª¨ë¸ (ì„±ëŠ¥ê³¼ ì†ë„ ê· í˜•)\n",
        "        llm_model_name = \"gpt-4.1-nano\" # ë§Œì•½ ë” ì‘ê³  ë¹ ë¥¸ ëª¨ë¸ì´ í•„ìš”í•˜ë‹¤ë©´ ê³ ë ¤ (í˜„ì¬ëŠ” gpt-4oê°€ ê°€ì¥ ìœ ì‚¬)\n",
        "        # llm_model_name = \"gpt-3.5-turbo\" # ì†ë„/ë¹„ìš© ìš°ì„  ì‹œ ê³ ë ¤\n",
        "        # ==============================\n",
        "\n",
        "        llm = ChatOpenAI(\n",
        "            model=llm_model_name,\n",
        "            temperature=0 # ë‹µë³€ì˜ ì°½ì˜ì„± ì¡°ì ˆ (ë‚®ì„ìˆ˜ë¡ ê²°ì •ì )\n",
        "            # max_tokens=1024 # í•„ìš”ì‹œ ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜ ì œí•œ\n",
        "        )\n",
        "        print(f\"[ì„±ê³µ] OpenAI ({llm.model_name}) LLM ë¡œë”© ì™„ë£Œ.\") # .model ëŒ€ì‹  .model_name ì‚¬ìš©\n",
        "\n",
        "    except ImportError:\n",
        "         print(\"!! [ì˜¤ë¥˜] langchain-openai ë˜ëŠ” openai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "         print(\"   ë‹¨ê³„ 0ì˜ ì„¤ì¹˜ ëª…ë ¹ì„ í™•ì¸í•˜ê³  ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.\")\n",
        "    except Exception as e:\n",
        "        print(f\"!! [ì˜¤ë¥˜] OpenAI LLM ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
        "        traceback.print_exc() # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥\n",
        "else:\n",
        "    print(\"!! [ì˜¤ë¥˜] OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ë¡œë“œ ë¶ˆê°€.\")\n",
        "\n",
        "print(\"--- ë‹¨ê³„ 9: LLM ì„¤ì • ì™„ë£Œ ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmu156Pos59b"
      },
      "source": [
        "# 10: RAG Chain/íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBJ8uY3ms4Zk",
        "outputId": "132d68a2-ce91-4fed-8237-d838c2c57a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 10: RAG Chain êµ¬ì¶• ì‹œì‘ ---\n",
            "[ì„±ê³µ] RetrievalQA Chain êµ¬ì¶• ì™„ë£Œ.\n",
            "--- ë‹¨ê³„ 10: RAG Chain êµ¬ì¶• ì™„ë£Œ ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 10: RAG Chain/íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ===\n",
        "# (ì½”ë“œ ë³€ê²½ ì—†ìŒ - ë‹¨ê³„ 9ì—ì„œ ë³€ê²½ëœ llm ê°ì²´ë¥¼ ì‚¬ìš©)\n",
        "print(\"\\n--- ë‹¨ê³„ 10: RAG Chain êµ¬ì¶• ì‹œì‘ ---\")\n",
        "qa_chain = None\n",
        "if llm and retriever: # llmì´ ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ë¡œ ì¤€ë¹„ë¨\n",
        "    template = \"\"\"ë‹¹ì‹ ì€ í•œêµ­ì˜ ë…¸ë¬´ ê·œì • ë° ê´€ë ¨ ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
        "    ì£¼ì–´ì§„ ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹­ì‹œì˜¤. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n",
        "    ë§Œì•½ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë§Œìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ì—†ë‹¤ë©´, \"\n",
        "    ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ëª…í™•íˆ ë‹µë³€í•˜ì‹­ì‹œì˜¤.\n",
        "    ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n",
        "\n",
        "    ì»¨í…ìŠ¤íŠ¸:\n",
        "    {context}\n",
        "\n",
        "    ì§ˆë¬¸: {question}\n",
        "\n",
        "    ë‹µë³€ (í•œêµ­ì–´):\"\"\"\n",
        "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "    try:\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm, # ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        "        )\n",
        "        print(\"[ì„±ê³µ] RetrievalQA Chain êµ¬ì¶• ì™„ë£Œ.\")\n",
        "    except Exception as e: print(f\"!! RAG Chain êµ¬ì¶• ì‹¤íŒ¨: {e}\")\n",
        "else: print(\"!! LLM ë˜ëŠ” Retriever ì¤€ë¹„ ì•ˆ ë¨. RAG Chain êµ¬ì¶• ë¶ˆê°€.\")\n",
        "print(\"--- ë‹¨ê³„ 10: RAG Chain êµ¬ì¶• ì™„ë£Œ ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y0QsYTnXbYJ"
      },
      "source": [
        "# 10.1: RAG íŒŒì´í”„ë¼ì¸ì„ Toolë¡œ ì •ì˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nlsd6obXcUB",
        "outputId": "61e18bad-aca7-434a-e8c7-d5b7a4477319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 10.1: RAG íŒŒì´í”„ë¼ì¸ì„ Toolë¡œ ì •ì˜ ì‹œì‘ ---\n",
            "[ì„±ê³µ] RAG íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ Tool ì •ì˜ ì™„ë£Œ.\n",
            "  Tool Name: KoreanLaborLawQASystem\n",
            "  Tool Description: í•œêµ­ì˜ ë…¸ë¬´ ë²•ë¥ , ê·œì •, ê´€ë ¨ íŒë¡€ ë° ë¬¸ì„œì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
            "                        ê·¼ë¡œê¸°ì¤€ë²•, í•´ê³  ì ˆì°¨, ì„ê¸ˆ, íœ´ê°€, ì‚°ì—… ì•ˆì „ ë“± ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œ ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
            "                        ì¼ë°˜ì ì¸ ìƒì‹ì´ë‚˜ ëŒ€í™”ì—ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì…ë ¥ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
            "--- ë‹¨ê³„ 10.1: Tool ì •ì˜ ì™„ë£Œ ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 10.1: RAG íŒŒì´í”„ë¼ì¸ì„ Toolë¡œ ì •ì˜ ===\n",
        "print(\"\\n--- ë‹¨ê³„ 10.1: RAG íŒŒì´í”„ë¼ì¸ì„ Toolë¡œ ì •ì˜ ì‹œì‘ ---\")\n",
        "\n",
        "from langchain.tools import Tool\n",
        "\n",
        "rag_tool = None\n",
        "\n",
        "# ë‹¨ê³„ 10ì—ì„œ qa_chainì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "if 'qa_chain' in locals() and qa_chain is not None:\n",
        "    try:\n",
        "        # Tool ì •ì˜\n",
        "        rag_tool = Tool.from_function(\n",
        "            func=lambda q: qa_chain.invoke({\"query\": q}), # qa_chainì„ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜\n",
        "            name=\"KoreanLaborLawQASystem\", # ë„êµ¬ ì´ë¦„ (ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ)\n",
        "            description=\"\"\"í•œêµ­ì˜ ë…¸ë¬´ ë²•ë¥ , ê·œì •, ê´€ë ¨ íŒë¡€ ë° ë¬¸ì„œì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "                        ê·¼ë¡œê¸°ì¤€ë²•, í•´ê³  ì ˆì°¨, ì„ê¸ˆ, íœ´ê°€, ì‚°ì—… ì•ˆì „ ë“± ë…¸ë¬´ ê´€ë ¨ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆì„ ë•Œ ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "                        ì¼ë°˜ì ì¸ ìƒì‹ì´ë‚˜ ëŒ€í™”ì—ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ì…ë ¥ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\"\"\", # <<< LLMì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìƒì„¸íˆ ì‘ì„±!\n",
        "            return_direct=False # Falseë¡œ ì„¤ì •í•´ì•¼ LLMì´ ë„êµ¬ ê²°ê³¼ë¥¼ ë³´ê³  ì¶”ê°€ ìƒê°ì„ í•  ìˆ˜ ìˆìŒ (ReAct ë°©ì‹)\n",
        "                               # Trueë¡œ í•˜ë©´ ë„êµ¬ ê²°ê³¼ê°€ ë°”ë¡œ ìµœì¢… ë‹µë³€ì´ ë¨\n",
        "        )\n",
        "        print(\"[ì„±ê³µ] RAG íŒŒì´í”„ë¼ì¸ì„ ìœ„í•œ Tool ì •ì˜ ì™„ë£Œ.\")\n",
        "        print(f\"  Tool Name: {rag_tool.name}\")\n",
        "        print(f\"  Tool Description: {rag_tool.description}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!! [ì˜¤ë¥˜] Tool ì •ì˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"!! [ì˜¤ë¥˜] 'qa_chain'ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Toolì„ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(\"--- ë‹¨ê³„ 10.1: Tool ì •ì˜ ì™„ë£Œ ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDcaPlSqXrzV"
      },
      "source": [
        "# 10.2: ReAct ì—ì´ì „íŠ¸ ì„¤ì • (OpenAI Tools Agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHiHU5WFXmOH",
        "outputId": "e4a9bc57-e752-4224-82f6-2ebbee40a152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 10.2: ReAct ì—ì´ì „íŠ¸ ì„¤ì • ì‹œì‘ ---\n",
            "[ì •ë³´] ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ë„êµ¬: ['KoreanLaborLawQASystem']\n",
            "[ì„±ê³µ] OpenAI Tools Agent ìƒì„± ì™„ë£Œ.\n",
            "[ì„±ê³µ] Agent Executor ìƒì„± ì™„ë£Œ.\n",
            "--- ë‹¨ê³„ 10.2: ì—ì´ì „íŠ¸ ì„¤ì • ì™„ë£Œ ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 10.2: ReAct ì—ì´ì „íŠ¸ ì„¤ì • (OpenAI Tools Agent) ===\n",
        "print(\"\\n--- ë‹¨ê³„ 10.2: ReAct ì—ì´ì „íŠ¸ ì„¤ì • ì‹œì‘ ---\")\n",
        "\n",
        "from langchain import hub # ReAct í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "\n",
        "agent_executor = None\n",
        "agent = None\n",
        "\n",
        "# ë‹¨ê³„ 9ì—ì„œ LLM(ChatOpenAI)ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "# ë‹¨ê³„ 10.1ì—ì„œ rag_toolì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
        "if 'llm' in locals() and llm is not None and 'rag_tool' in locals() and rag_tool is not None:\n",
        "    try:\n",
        "        # ì‚¬ìš©í•  ë„êµ¬ ë¦¬ìŠ¤íŠ¸ ì •ì˜ (í˜„ì¬ëŠ” RAG ë„êµ¬ í•˜ë‚˜)\n",
        "        tools = [rag_tool]\n",
        "        print(f\"[ì •ë³´] ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ë„êµ¬: {[tool.name for tool in tools]}\")\n",
        "\n",
        "        # ReAct ìŠ¤íƒ€ì¼ì˜ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° (OpenAI Tools Agentìš©)\n",
        "        # LangChain Hubì—ì„œ ê²€ì¦ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ\n",
        "        # prompt = hub.pull(\"hwchase17/openai-tools-agent\") # ì˜ˆì‹œ í”„ë¡¬í”„íŠ¸ ì£¼ì†Œ\n",
        "        # ë˜ëŠ” ì§ì ‘ ChatPromptTemplate êµ¬ì„±\n",
        "        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "        # ê°„ë‹¨í•œ ReAct ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ (í•„ìš”ì‹œ ìˆ˜ì •)\n",
        "        prompt_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
        "             í•„ìš”í•˜ë‹¤ë©´ ë‹¤ìŒ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³ , ë„êµ¬ ì‚¬ìš©ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨ë˜ë©´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n",
        "             ë§Œì•½ ë„êµ¬ ì—†ì´ ë‹µë³€í•  ìˆ˜ ìˆê±°ë‚˜, ì§ˆë¬¸ì´ ë„êµ¬ ì‚¬ìš©ì— ì í•©í•˜ì§€ ì•Šë‹¤ë©´ ì§ì ‘ ë‹µë³€í•˜ì„¸ìš”.\n",
        "             ë‹µë³€ì€ í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\"\"),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\", optional=True), # ëŒ€í™” ê¸°ë¡ (í•„ìš”ì‹œ)\n",
        "            (\"human\", \"{input}\"), # ì‚¬ìš©ì ì…ë ¥\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # ì—ì´ì „íŠ¸ì˜ ìƒê°/í–‰ë™ ê¸°ë¡\n",
        "        ])\n",
        "\n",
        "        # OpenAI Tools Agent ìƒì„±\n",
        "        # ì´ ì—ì´ì „íŠ¸ëŠ” LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í• ì§€, ì–´ë–¤ ë„êµ¬ë¥¼ ì‚¬ìš©í• ì§€, ì–´ë–¤ ì…ë ¥ì„ ì¤„ì§€ ê²°ì •í•˜ê²Œ í•¨\n",
        "        agent = create_openai_tools_agent(llm, tools, prompt_template)\n",
        "        print(\"[ì„±ê³µ] OpenAI Tools Agent ìƒì„± ì™„ë£Œ.\")\n",
        "\n",
        "        # Agent Executor ìƒì„± (ì‹¤ì œ ReAct ë£¨í”„ ì‹¤í–‰)\n",
        "        agent_executor = AgentExecutor(\n",
        "            agent=agent,\n",
        "            tools=tools,\n",
        "            verbose=True, # ì—ì´ì „íŠ¸ì˜ ìƒê° ê³¼ì •ì„ ë³´ë ¤ë©´ Trueë¡œ ì„¤ì •\n",
        "            handle_parsing_errors=True # LLM ì¶œë ¥ íŒŒì‹± ì˜¤ë¥˜ ì²˜ë¦¬\n",
        "        )\n",
        "        print(\"[ì„±ê³µ] Agent Executor ìƒì„± ì™„ë£Œ.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!! [ì˜¤ë¥˜] ì—ì´ì „íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        traceback.print_exc()\n",
        "        agent_executor = None\n",
        "\n",
        "elif not ('llm' in locals() and llm is not None):\n",
        "     print(\"!! [ì˜¤ë¥˜] LLMì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ì—ì´ì „íŠ¸ë¥¼ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "elif not ('rag_tool' in locals() and rag_tool is not None):\n",
        "     print(\"!! [ì˜¤ë¥˜] RAG Toolì´ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ì—ì´ì „íŠ¸ë¥¼ ì„¤ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(\"--- ë‹¨ê³„ 10.2: ì—ì´ì „íŠ¸ ì„¤ì • ì™„ë£Œ ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW9ruAEOXxUh"
      },
      "source": [
        "# ë‹¨ê³„ 11: ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TGaR7zNX0cp",
        "outputId": "300a85f7-ca7c-429c-c5b1-cddec02769e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ë‹¨ê³„ 11 (ìˆ˜ì •): ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘ ---\n",
            "\n",
            "\n",
            "=========================================\n",
            "ì§ˆë¬¸ ì…ë ¥: ê·¼ë¡œìë¥¼ í•´ê³ í•˜ë ¤ë©´ ë©°ì¹  ì „ì— ì˜ˆê³ ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?\n",
            "=========================================\n",
            "ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mê·¼ë¡œìë¥¼ í•´ê³ í•˜ë ¤ë©´ ê·¼ë¡œê¸°ì¤€ë²•ì— ë”°ë¼ ì¼ì • ê¸°ê°„ ì „ì— ì˜ˆê³ ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í•´ê³  ì˜ˆê³  ê¸°ê°„ì€ ìµœì†Œ 30ì¼ì…ë‹ˆë‹¤. ì¦‰, í•´ê³ ë¥¼ í†µë³´í•˜ê¸° ìµœì†Œ 30ì¼ ì „ì— ê·¼ë¡œìì—ê²Œ ì˜ˆê³ í•´ì•¼ í•˜ë©°, ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš° 30ì¼ë¶„ì˜ í‰ê· ì„ê¸ˆì„ í•´ê³  ì˜ˆê³ ìˆ˜ë‹¹ìœ¼ë¡œ ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
            "\n",
            "í˜¹ì‹œ êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ í•´ê³  ì‚¬ìœ ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ, ë” ìì„¸í•œ ë²•ë¥  ìƒë‹´ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ (1.34ì´ˆ)\n",
            "\n",
            "[ì—ì´ì „íŠ¸ ìµœì¢… ë‹µë³€]:\n",
            "ê·¼ë¡œìë¥¼ í•´ê³ í•˜ë ¤ë©´ ê·¼ë¡œê¸°ì¤€ë²•ì— ë”°ë¼ ì¼ì • ê¸°ê°„ ì „ì— ì˜ˆê³ ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í•´ê³  ì˜ˆê³  ê¸°ê°„ì€ ìµœì†Œ 30ì¼ì…ë‹ˆë‹¤. ì¦‰, í•´ê³ ë¥¼ í†µë³´í•˜ê¸° ìµœì†Œ 30ì¼ ì „ì— ê·¼ë¡œìì—ê²Œ ì˜ˆê³ í•´ì•¼ í•˜ë©°, ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš° 30ì¼ë¶„ì˜ í‰ê· ì„ê¸ˆì„ í•´ê³  ì˜ˆê³ ìˆ˜ë‹¹ìœ¼ë¡œ ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
            "\n",
            "í˜¹ì‹œ êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ í•´ê³  ì‚¬ìœ ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ, ë” ìì„¸í•œ ë²•ë¥  ìƒë‹´ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.\n",
            "\n",
            "\n",
            "=========================================\n",
            "ì§ˆë¬¸ ì…ë ¥: ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\n",
            "=========================================\n",
            "ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mì£„ì†¡í•˜ì§€ë§Œ, ì €ëŠ” í˜„ì¬ ì‹¤ì‹œê°„ ë‚ ì”¨ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìµœì‹  ë‚ ì”¨ ì •ë³´ë¥¼ í™•ì¸í•˜ì‹œë ¤ë©´ ë‚ ì”¨ ì•±ì´ë‚˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì´ìš©í•˜ì‹œê±°ë‚˜, ìŠ¤ë§ˆíŠ¸í°ì˜ ë‚ ì”¨ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ ì£¼ì„¸ìš”. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ (1.00ì´ˆ)\n",
            "\n",
            "[ì—ì´ì „íŠ¸ ìµœì¢… ë‹µë³€]:\n",
            "ì£„ì†¡í•˜ì§€ë§Œ, ì €ëŠ” í˜„ì¬ ì‹¤ì‹œê°„ ë‚ ì”¨ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìµœì‹  ë‚ ì”¨ ì •ë³´ë¥¼ í™•ì¸í•˜ì‹œë ¤ë©´ ë‚ ì”¨ ì•±ì´ë‚˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì´ìš©í•˜ì‹œê±°ë‚˜, ìŠ¤ë§ˆíŠ¸í°ì˜ ë‚ ì”¨ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ ì£¼ì„¸ìš”. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!\n",
            "\n",
            "--- ë‹¨ê³„ 11 (ìˆ˜ì •): ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ ---\n"
          ]
        }
      ],
      "source": [
        "# === ë‹¨ê³„ 11 (ìˆ˜ì •): ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ===\n",
        "\n",
        "print(\"\\n--- ë‹¨ê³„ 11 (ìˆ˜ì •): ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘ ---\")\n",
        "\n",
        "# ì´ì „ ë‹¨ê³„ì—ì„œ ì •ì˜ëœ query ë³€ìˆ˜ ì‚¬ìš© ë˜ëŠ” ìƒˆ ì§ˆë¬¸ ì •ì˜\n",
        "# query = \"ê·¼ë¡œìë¥¼ í•´ê³ í•˜ë ¤ë©´ ëª‡ì¼ ì „ì— ì˜ˆê³ ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?\" # ì˜ˆì‹œ ì§ˆë¬¸\n",
        "test_query_1 = \"ê·¼ë¡œìë¥¼ í•´ê³ í•˜ë ¤ë©´ ë©°ì¹  ì „ì— ì˜ˆê³ ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?\"\n",
        "test_query_2 = \"ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?\" # RAG ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ì•¼ í•˜ëŠ” ì§ˆë¬¸ ì˜ˆì‹œ\n",
        "\n",
        "queries_to_test = [test_query_1, test_query_2]\n",
        "agent_results = {}\n",
        "\n",
        "if agent_executor:\n",
        "    for query in queries_to_test:\n",
        "        print(f\"\\n\\n=========================================\")\n",
        "        print(f\"ì§ˆë¬¸ ì…ë ¥: {query}\")\n",
        "        print(f\"=========================================\")\n",
        "        agent_results[query] = None # ê²°ê³¼ ì´ˆê¸°í™”\n",
        "        try:\n",
        "            print(\"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...\")\n",
        "            start_agent_time = time.time()\n",
        "\n",
        "            # ì—ì´ì „íŠ¸ ì‹¤í–‰!\n",
        "            response = agent_executor.invoke({\"input\": query})\n",
        "\n",
        "            end_agent_time = time.time()\n",
        "            print(f\"\\nì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ ({end_agent_time - start_agent_time:.2f}ì´ˆ)\")\n",
        "\n",
        "            # ìµœì¢… ë‹µë³€ ì¶œë ¥\n",
        "            final_answer = response.get(\"output\", \"N/A\")\n",
        "            agent_results[query] = final_answer\n",
        "            print(\"\\n[ì—ì´ì „íŠ¸ ìµœì¢… ë‹µë³€]:\")\n",
        "            print(final_answer)\n",
        "\n",
        "            # (ì°¸ê³ ) verbose=True ì„¤ì • ì‹œ, ì‹¤í–‰ ê³¼ì •ì—ì„œ Thought, Action, Observationì´ ì¶œë ¥ë¨\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!! [ì˜¤ë¥˜] ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    print(\"!! Agent Executorê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤í–‰ ë¶ˆê°€.\")\n",
        "\n",
        "print(\"\\n--- ë‹¨ê³„ 11 (ìˆ˜ì •): ReAct ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbgtbWRGQlOG"
      },
      "source": [
        "RAG ê¸°ë°˜ ì—ì´ì „íŠ¸ í‰ê°€ì—ì„œëŠ” í•­ìƒ \"Agentê°€ RAG ë„êµ¬ë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í–ˆëŠ”ê°€?\" ë¥¼ ë¨¼ì € í™•ì¸\n",
        "\n",
        "âœ… 1. intermediate_stepsì— contextê°€ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸\n",
        "\n",
        "âœ… 2. í‰ê°€ ê°€ëŠ¥í•œ ì§ˆë¬¸ë§Œ ê³¨ë¼ì„œ í‰ê°€\n",
        "\n",
        "[] ë‚˜ì˜¤ëŠ” ê²½ìš°ëŠ” ragë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ contextê°€ ì—†ë‹¤ëŠ” ì˜ë¯¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmGpr-dniqsm",
        "outputId": "0e8c86f7-dc95-461c-f6ce-13398b258927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(response.get(\"intermediate_steps\", []), width=150)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ABXNe2uRJF5"
      },
      "source": [
        "# âœ… íšŒê³ \n",
        "\n",
        "ReAct(Reason + Act) í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ì˜ ì—ì´ì „íŠ¸(Agent)ë¥¼ êµ¬í˜„í•´ ë³´ì•˜ìŠµë‹ˆë‹¤.\n",
        "LLMì´ íŒë‹¨í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©ì—¬ë¶€, ë‹µë³€ ìƒì„±ì„ í•˜ëŠ” ê³¼ì •ì—ì„œ LLMì˜ í•œê³„ê°€ ì ìš©ë  ìˆ˜ ë°–ì— ì—†ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì¸ \"ê·¼ë¡œìë¥¼ í•´ê³ í•˜ë ¤ë©´ ë©°ì¹  ì „ì— ì˜ˆê³ ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?\"ì— ìƒì„±ëœ ì‘ë‹µì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "(ë‹µë³€)\n",
        "ê·¼ë¡œìë¥¼ í•´ê³ í•˜ë ¤ë©´ ê·¼ë¡œê¸°ì¤€ë²•ì— ë”°ë¼ ì¼ì • ê¸°ê°„ ì „ì— ì˜ˆê³ ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í•´ê³  ì˜ˆê³  ê¸°ê°„ì€ ìµœì†Œ 30ì¼ì…ë‹ˆë‹¤. ì¦‰, í•´ê³ ë¥¼ í†µë³´í•˜ê¸° ìµœì†Œ 30ì¼ ì „ì— ê·¼ë¡œìì—ê²Œ ì˜ˆê³ í•´ì•¼ í•˜ë©°, ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš° 30ì¼ë¶„ì˜ í‰ê· ì„ê¸ˆì„ í•´ê³  ì˜ˆê³ ìˆ˜ë‹¹ìœ¼ë¡œ ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "í˜¹ì‹œ êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ í•´ê³  ì‚¬ìœ ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ, ë” ìì„¸í•œ ë²•ë¥  ìƒë‹´ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.\n",
        "\n",
        "ìƒì„±ëœ ë‹µë³€ì—ì„œ 'í‰ê· ì„ê¸ˆì´ ì•„ë‹ˆë¼ í†µìƒì„ê¸ˆ'ìœ¼ë¡œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "í˜„ì¬ì˜ RAG agentëŠ” ì°¸ê³  ë¬¸ì„œ ìì²´ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆê³ , ì²« ë²ˆì§¸ í…ŒìŠ¤íŠ¸ë¶€í„° ì˜¤ë‹µì´ ë°œìƒí•˜ì—¬ ë²•ë¥  ë„ë©”ì¸ì— ì í•©í•˜ì§€ ì•Šë‹¤ê³  íŒë‹¨í•˜ì—¬ ë” ì´ìƒ ê°œì„ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í”„ë¡¬í”„íŠ¸ë¥¼ ì¢€ ë” ë©´ë°€í•˜ê²Œ ìˆ˜ì •í•œë‹¤ë©´, ìƒì„±ëœ ì‘ë‹µì˜ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}