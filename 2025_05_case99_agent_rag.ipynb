{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC3Be17wLLUn"
      },
      "source": [
        "# **RAG의 한계를 극복하기 위한 매우 효과적이고 현대적인 접근 방식인 중 하나가 ReAct(Reason + Act) 프레임워크 기반의 에이전트(Agent)입니다. **\n",
        "\n",
        "Function Calling (또는 LangChain의 Tool 사용)은 이 에이전트가 \"행동(Act)\"하는 핵심 방법 중 하나입니다.\n",
        "\n",
        "**이 방법이 왜 좋은가?**\n",
        "\n",
        "- 조건부 실행: 기존 RAG는 질문이 들어오면 무조건 검색(Retrieve)하고 생성(Generate)합니다. 하지만 ReAct 에이전트는 LLM이 먼저 **생각(Reason)**하고, \"이 질문에 답변하기 위해 문서 검색(RAG)이 필요한가?\" 또는 \"그냥 내 지식으로 답해도 되는가?\" 등을 판단합니다. 필요할 때만 RAG 함수(Tool)를 호출합니다.\n",
        "\n",
        "\n",
        "- 상황 판단: RAG 검색 결과가 질문과 관련 없거나 답변에 충분하지 않다고 판단되면, 에이전트는 \"문서에서 관련 정보를 찾을 수 없습니다\"라고 답변하거나, 추가 정보를 요청하는 등 다른 행동을 취할 수 있습니다.\n",
        "\n",
        "\n",
        "- 도구 활용: RAG 파이프라인 자체를 하나의 \"도구(Tool)\"로 간주하여, 에이전트가 계산기, 웹 검색, 다른 API 호출 등 다양한 도구와 함께 상황에 맞게 선택하여 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfV42Er6tDWn"
      },
      "source": [
        "    base_folder_path = \"/content/drive/MyDrive/nomu_dataset3\" # <<< 원본 데이터 폴더 경로 확인!\n",
        "    result_dir = \"/content/drive/MyDrive/nomu_rag_result\"    # <<< 결과 저장 폴더 경로 확인!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIL128MhoqWr"
      },
      "source": [
        "# 0: 필요한 라이브러리 설치 (OpenAI 추가)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLKVnkOgotFA",
        "outputId": "22cf1f74-0d8a-4603-a7d7-eabad0272142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 단계 0: 라이브러리 설치 시작 (OpenAI 추가) ---\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m128.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m149.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m746.6/746.6 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m144.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.1/192.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "[알림] 라이브러리 설치/업데이트 완료. langchain-openai, openai 추가됨.\n"
          ]
        }
      ],
      "source": [
        "# === 단계 0: 필요한 라이브러리 설치 (의존성 충돌 해결 버전 + OpenAI 추가) ===\n",
        "print(\"--- 단계 0: 라이브러리 설치 시작 (OpenAI 추가) ---\")\n",
        "!pip install -qU \\\n",
        "    langchain langchain-core langchain-community langchainhub langchain-openai openai \\\n",
        "    pypdf openpyxl xlrd unstructured faiss-cpu sentence-transformers \\\n",
        "    pdf2image pillow pdfminer.six rank_bm25 pillow-heif jq \\\n",
        "    google-api-python-client google-auth-httplib2 google-auth-oauthlib gspread \\\n",
        "    ragas datasets \\\n",
        "    pandas==2.2.2 \\\n",
        "    PyPDF2 \\\n",
        "    fsspec==2025.3.2 # <<< fsspec 버전은 환경에 따라 조정 필요, 원래 버전 사용\n",
        "\n",
        "# google-ai-generativelanguage는 OpenAI 사용 시 필수는 아님 (필요시 유지)\n",
        "# !pip install -qU google-ai-generativelanguage==0.6.15\n",
        "\n",
        "print(\"\\n[알림] 라이브러리 설치/업데이트 완료. langchain-openai, openai 추가됨.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw9hai20pM7K"
      },
      "source": [
        "# 1: 기본 및 필요 라이브러리 임포트 (OpenAI LLM 임포트 추가)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezNk5hE9pQv_",
        "outputId": "d6f3005e-7941-41ae-e23b-6554adca3eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 단계 1: 라이브러리 임포트 완료 (ChatOpenAI 임포트됨) ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 1: 기본 및 필요 라이브러리 임포트 ===\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "import sys\n",
        "import warnings\n",
        "import time\n",
        "import pandas as pd\n",
        "from google.colab import drive, auth, userdata\n",
        "import PyPDF2 # 명시적 임포트\n",
        "import json\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import gspread\n",
        "from google.auth import default as google_auth_default\n",
        "from datasets import Dataset\n",
        "import re\n",
        "import traceback # 오류 상세 출력을 위해 추가\n",
        "\n",
        "# LangChain 관련 임포트\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader, UnstructuredExcelLoader, CSVLoader,\n",
        "    UnstructuredFileLoader, DirectoryLoader, GoogleDriveLoader\n",
        ")\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain_google_genai import GoogleGenerativeAIEmbeddings # Google 임베딩 (필요 시 유지)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings # HuggingFace 임베딩 (유지)\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "# === LLM 임포트 변경 ===\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI # <<< 삭제 또는 주석 처리\n",
        "from langchain_openai import ChatOpenAI # <<< OpenAI LLM 클래스 임포트\n",
        "# =======================\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# RAGAS 관련 임포트 (기존 유지)\n",
        "try:\n",
        "    from ragas import evaluate\n",
        "    from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy\n",
        "    from ragas.llms import LangchainLLMWrapper\n",
        "    from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "except ImportError:\n",
        "    print(\"!! Ragas 관련 라이브러리가 설치되지 않았습니다. 평가 단계 전에 설치가 필요합니다.\")\n",
        "    LangchainLLMWrapper = None; LangchainEmbeddingsWrapper = None; context_precision = None; context_recall = None; faithfulness = None; answer_relevancy = None\n",
        "\n",
        "# 기타 평가 관련 임포트 (기존 유지)\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# import google.generativeai as genai # OpenAI 사용 시 필수는 아님\n",
        "\n",
        "warnings.filterwarnings(\"ignore\") # 경고 메시지 숨기기\n",
        "\n",
        "print(\"--- 단계 1: 라이브러리 임포트 완료 (ChatOpenAI 임포트됨) ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNDH4-J6prCU"
      },
      "source": [
        "# 2: 환경 설정 (OpenAI API 키 추가)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLV3INSgrLZi"
      },
      "source": [
        "    base_folder_path = \"/content/drive/MyDrive/nomu_dataset3\" # <<< 원본 데이터 폴더 경로 확인!\n",
        "    result_dir = \"/content/drive/MyDrive/nomu_rag_result\"    # <<< 결과 저장 폴더 경로 확인!\n",
        "    # <<< nomu_dataset3 폴더의 실제 ID로 변경하거나 확인! >>>\n",
        "예)\n",
        "    https://drive.google.com/drive/u/0/folders/1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U\n",
        "\n",
        "target_folder_id = \"1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3MRw_Yupuun",
        "outputId": "d94d6c02-4a02-4a56-86ff-f065c6d3dac8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 2: 환경 설정 시작 ---\n",
            "Mounted at /content/drive\n",
            "[성공] Google Drive 마운트 완료.\n",
            "[성공] Google Colab 사용자 인증 완료.\n",
            "[성공] OpenAI API 키 로드 및 설정 완료.\n",
            "데이터 소스 검색 경로: /content/drive/MyDrive/nomu_dataset3\n",
            "결과 저장 경로: /content/drive/MyDrive/nomu_rag_result\n",
            "Google Sheets 검색 대상 폴더 ID: 1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U\n",
            "--- 단계 2: 환경 설정 완료 (OpenAI API 키 설정됨) ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 2: 환경 설정 (Drive 마운트, API 키, 경로) ===\n",
        "print(\"\\n--- 단계 2: 환경 설정 시작 ---\")\n",
        "\n",
        "# --- Google Drive 마운트 (기존 유지) ---\n",
        "DRIVE_MOUNTED = False\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    DRIVE_MOUNTED = True\n",
        "    print(\"[성공] Google Drive 마운트 완료.\")\n",
        "except Exception as e:\n",
        "    print(f\"[실패] Google Drive 마운트 오류: {e}\")\n",
        "\n",
        "# --- Google 인증 (Colab 사용자 인증 - 필요시 유지) ---\n",
        "# Google Drive Loader 등을 사용한다면 인증 유지 필요\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    print(\"[성공] Google Colab 사용자 인증 완료.\")\n",
        "except Exception as e:\n",
        "    print(f\"[실패] Google Colab 인증 오류: {e}\")\n",
        "\n",
        "# === API 키 설정 변경 ===\n",
        "# --- OpenAI API 키 설정 ---\n",
        "OPENAI_API_KEY = None\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY') # Colab Secrets 우선 확인\n",
        "    if not OPENAI_API_KEY: OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') # 환경 변수 확인\n",
        "    if not OPENAI_API_KEY: raise ValueError(\"OpenAI API 키를 Colab Secrets 또는 환경 변수에서 찾을 수 없습니다.\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "    print(\"[성공] OpenAI API 키 로드 및 설정 완료.\")\n",
        "except Exception as e:\n",
        "    print(f\"[실패] OpenAI API 키 로드/설정 오류: {e}\")\n",
        "    print(\"   !! OpenAI API 키 없이는 이후 LLM, RAGAS 평가 등 사용 불가 !!\")\n",
        "\n",
        "# --- Google AI API 키 설정 (선택 사항) ---\n",
        "# 만약 Google Embedding 등을 계속 사용한다면 유지, 아니면 제거 가능\n",
        "# GOOGLE_API_KEY = None\n",
        "# try:\n",
        "#     GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "#     if not GOOGLE_API_KEY: GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')\n",
        "#     if GOOGLE_API_KEY:\n",
        "#         os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "#         import google.generativeai as genai\n",
        "#         genai.configure(api_key=GOOGLE_API_KEY)\n",
        "#         print(\"[정보] Google API 키 로드 및 설정 완료 (선택 사항).\")\n",
        "#     # else: print(\"[정보] Google API 키 로드 안 됨 (선택 사항).\") # 키 없어도 오류 아님\n",
        "# except Exception as e: print(f\"[경고] Google API 키 설정 중 오류 발생 (선택 사항): {e}\")\n",
        "# =========================\n",
        "\n",
        "# --- 경로 설정 (기존 유지) ---\n",
        "if DRIVE_MOUNTED:\n",
        "    base_folder_path = \"/content/drive/MyDrive/nomu_dataset3\"\n",
        "    result_dir = \"/content/drive/MyDrive/nomu_rag_result\"\n",
        "else:\n",
        "    base_folder_path = \"./nomu_data_local\"\n",
        "    result_dir = \"./nomu_rag_result_local\"\n",
        "\n",
        "print(f\"데이터 소스 검색 경로: {base_folder_path}\")\n",
        "print(f\"결과 저장 경로: {result_dir}\")\n",
        "os.makedirs(base_folder_path, exist_ok=True)\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "# Google Sheets 폴더 ID (필요시 유지)\n",
        "target_folder_id = \"1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U\"\n",
        "print(f\"Google Sheets 검색 대상 폴더 ID: {target_folder_id}\")\n",
        "\n",
        "# --- 결과 파일 경로 (기존 유지) ---\n",
        "vs_status_file = os.path.join(result_dir, \"vectorstore_build_status.json\")\n",
        "vs_checkpoint_path = os.path.join(result_dir, \"faiss_index_nomu_checkpoint\")\n",
        "vs_final_save_path = os.path.join(result_dir, \"faiss_index_nomu_final\")\n",
        "bm25_data_save_path = os.path.join(result_dir, \"split_texts_for_bm25.pkl\")\n",
        "\n",
        "print(\"--- 단계 2: 환경 설정 완료 (OpenAI API 키 설정됨) ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AC7tRibqyM4"
      },
      "source": [
        "# 3: 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK4AoDtiq8s2",
        "outputId": "a891305f-7967-4e5b-e422-80665ffd2a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 3: 데이터 로딩 시작 ---\n",
            "\n",
            "--- 3.1 Google Sheets 파일 로딩 ---\n",
            "  GoogleDriveLoader로 폴더 '1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U' 로딩 중...\n",
            "  [성공] Google Sheets 로딩 (89개 조각).\n",
            "\n",
            "--- 3.2 다른 파일 형식 로딩 (DirectoryLoader) ---\n",
            "\n",
            "  '.pdf' 확장자 로딩 (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 125/125 [00:20<00:00,  6.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [성공] '.pdf' 로딩 (1748개 조각).\n",
            "\n",
            "  '.xlsx' 확장자 로딩 (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [정보] '.xlsx' 파일 없음.\n",
            "\n",
            "  '.xls' 확장자 로딩 (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [정보] '.xls' 파일 없음.\n",
            "\n",
            "  '.csv' 확장자 로딩 (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [정보] '.csv' 파일 없음.\n",
            "\n",
            "  '.txt' 확장자 로딩 (/content/drive/MyDrive/nomu_dataset3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    [정보] '.txt' 파일 없음.\n",
            "\n",
            "--- 최종 로드된 문서 조각 수: 1837 ---\n",
            "\n",
            "--- 로드된 문서 타입별 개수 ---\n",
            "  - google_sheet: 89개 조각\n",
            "  - pdf: 1748개 조각\n",
            "\n",
            "--- 첫 로드 문서 샘플 ---\n",
            "  타입: google_sheet\n",
            "  메타데이터: {'source': 'https://docs.google.com/spreadsheets/d/1QeMvmrcYe6QQ8L1n6o1NQ7ASTPafmzdbuDAAKXFCu3k/edit?gid=0', 'title': 'filtered_qa_dataset - Sheet1', 'row': 1, 'file_type': 'google_sheet'}\n",
            "  내용(200자): No.: 0\n",
            "question: 근로계약이 미성년자에게 불리하다고 인정되는 경우 미성년후견인은 그 계약을 해지할 수 있나요?\n",
            "answer: 네. 근로계약을 해지할 수 있습니다.\n",
            "ground_truths: [\"「근로기준법」 제67조 제2항은 친권자,후견인 또는 고용노동부장관은 근로계약이 미성년자에게 불리하다고 인정하는 경우에는 이를 해지할 수 있다.\"라고 규정...\n",
            "\n",
            "--- 단계 3: 데이터 로딩 완료 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === 단계 3: 데이터 로딩  ===\n",
        "\n",
        "print(\"\\n--- 단계 3: 데이터 로딩 시작 ---\")\n",
        "\n",
        "loaded_documents = []\n",
        "loading_errors = {}\n",
        "\n",
        "# --- 3.1. Google Sheets 로딩 ---\n",
        "print(\"\\n--- 3.1 Google Sheets 파일 로딩 ---\")\n",
        "if not target_folder_id or \"YOUR_\" in target_folder_id:\n",
        "    print(\"  ⚠️ 경고: Google Drive 폴더 ID가 유효하지 않아 Google Sheet 로딩 건너<0xEB><0x9A><A9>니다.\")\n",
        "elif not DRIVE_MOUNTED:\n",
        "     print(\"  ⚠️ 경고: Google Drive가 마운트되지 않아 Google Sheet 로딩 건너<0xEB><0x9A><A9>니다.\")\n",
        "else:\n",
        "    try:\n",
        "        gsheet_loader = GoogleDriveLoader(folder_id=target_folder_id, file_types=[\"sheet\"], recursive=True)\n",
        "        print(f\"  GoogleDriveLoader로 폴더 '{target_folder_id}' 로딩 중...\")\n",
        "        gsheet_docs = gsheet_loader.load()\n",
        "        if gsheet_docs:\n",
        "            print(f\"  [성공] Google Sheets 로딩 ({len(gsheet_docs)}개 조각).\")\n",
        "            for doc in gsheet_docs:\n",
        "                doc.metadata['file_type'] = 'google_sheet'\n",
        "                if 'source' not in doc.metadata and 'id' in doc.metadata:\n",
        "                    doc.metadata['source'] = f\"https://docs.google.com/spreadsheets/d/{doc.metadata['id']}\"\n",
        "            loaded_documents.extend(gsheet_docs)\n",
        "        else: print(\"  [정보] 해당 폴더에 Google Sheets 파일 없음.\")\n",
        "    except ImportError as ie: print(f\"  ❌ 임포트 오류: {ie}. 관련 라이브러리 설치 필요.\"); loading_errors['google_sheets'] = str(ie)\n",
        "    except Exception as e: error_msg = f\"{type(e).__name__}: {e}\"; print(f\"  ❌ 로딩 오류: {error_msg}\"); loading_errors['google_sheets'] = error_msg\n",
        "\n",
        "# --- 3.2. 다른 파일 형식 로딩 (PDF, Excel, CSV, TXT 등) ---\n",
        "print(\"\\n--- 3.2 다른 파일 형식 로딩 (DirectoryLoader) ---\")\n",
        "if not DRIVE_MOUNTED and not os.path.exists(base_folder_path):\n",
        "    print(f\"  ⚠️ 경고: Drive 미마운트 및 로컬 경로({base_folder_path}) 없음. 파일 로딩 건너<0xEB><0x9A><A9>니다.\")\n",
        "else:\n",
        "    LOADER_MAPPING = { \".pdf\": (PyPDFLoader, {}), \".xlsx\": (UnstructuredExcelLoader, {\"mode\": \"single\"}), \".xls\": (UnstructuredExcelLoader, {\"mode\": \"single\"}), \".csv\": (CSVLoader, {\"encoding\": \"utf-8\"}), \".txt\": (UnstructuredFileLoader, {}) }\n",
        "    supported_extensions = list(LOADER_MAPPING.keys())\n",
        "    for ext in supported_extensions:\n",
        "        print(f\"\\n  '{ext}' 확장자 로딩 ({base_folder_path})...\")\n",
        "        loader_cls, loader_args = LOADER_MAPPING[ext]\n",
        "        try:\n",
        "            loader = DirectoryLoader( base_folder_path, glob=f\"**/*{ext}\", loader_cls=loader_cls, loader_kwargs=loader_args, recursive=True, show_progress=True, use_multithreading=True, silent_errors=False )\n",
        "            docs = loader.load()\n",
        "            if docs:\n",
        "                print(f\"    [성공] '{ext}' 로딩 ({len(docs)}개 조각).\")\n",
        "                for doc in docs: doc.metadata['file_type'] = ext.lstrip('.')\n",
        "                loaded_documents.extend(docs)\n",
        "            else: print(f\"    [정보] '{ext}' 파일 없음.\")\n",
        "        except ImportError as ie: error_msg = f\"{ie}\"; print(f\"    ❌ 임포트 오류: {error_msg}\"); loading_errors[f'loader_{ext}'] = f\"ImportError: {error_msg}\"\n",
        "        except Exception as e: error_msg = f\"{type(e).__name__}: {e}\"; print(f\"    ❌ 로딩 오류: {error_msg}\"); loading_errors[f'loader_{ext}'] = error_msg\n",
        "\n",
        "# --- 3.3. 로딩 결과 요약 ---\n",
        "print(f\"\\n--- 최종 로드된 문서 조각 수: {len(loaded_documents)} ---\")\n",
        "doc_type_counts = {}\n",
        "for doc in loaded_documents: file_type = doc.metadata.get('file_type', 'unknown'); doc_type_counts[file_type] = doc_type_counts.get(file_type, 0) + 1\n",
        "print(\"\\n--- 로드된 문서 타입별 개수 ---\");\n",
        "if doc_type_counts: [print(f\"  - {f_type}: {count}개 조각\") for f_type, count in sorted(doc_type_counts.items())]\n",
        "else: print(\"  로드된 문서 없음.\")\n",
        "if loading_errors: print(\"\\n--- 로딩 오류 요약 ---\"); [print(f\"  - {src}: {err}\") for src, err in loading_errors.items()]\n",
        "if loaded_documents:\n",
        "    print(\"\\n--- 첫 로드 문서 샘플 ---\")\n",
        "    try: first_doc = loaded_documents[0]; print(f\"  타입: {first_doc.metadata.get('file_type')}\\n  메타데이터: {first_doc.metadata}\\n  내용(200자): {first_doc.page_content[:200]}...\")\n",
        "    except Exception as e: print(f\"  !! 샘플 출력 오류: {e}\")\n",
        "else: print(\"\\n로드된 문서 없음. 경로, 파일 형식, 권한 확인 필요.\")\n",
        "print(\"\\n--- 단계 3: 데이터 로딩 완료 ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L27LiqhHrCtN"
      },
      "source": [
        "# 4: 텍스트 분할 (Chunking, 패턴 기반 + 길이 제한)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "6d6419e0e426482d800a6d81917cd486",
            "dce05db05e5b4356bd212545971abd77",
            "207961f0a05248469ff92cb2cc90a6a1",
            "9dda8dc862654ed6adcd8fb009810080",
            "72e4aba7c9a046b298b9c1e655cc1e74",
            "be3fec1295264a4b84d044f23a608788",
            "a2b329d078164799b200eab8d49aee53",
            "344d06e18ea349419424771782a5ed28",
            "781c755903624a0684c2bf57f45a3334",
            "24baabd7703d49afbf3b7394ecb1a1ae",
            "2b00383503d648f8a56730358062d625"
          ]
        },
        "id": "uIt8ueGb6tMk",
        "outputId": "4c12240e-1de6-47ce-f2f8-33a8d2b18926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 4: 텍스트 분할 시작 (패턴 기반 적용) ---\n",
            "[정보] 패턴 기반 청킹 시도 (법률 PDF 대상). 기준 크기=500, 중첩=100\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d6419e0e426482d800a6d81917cd486",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "문서 청킹 중:   0%|          | 0/1837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[성공] 총 1837개 원본 조각 처리 -> 9429개 청크 분할 완료.\n",
            "\n",
            "첫 분할 청크 샘플:\n",
            "  내용(200자): No.: 0\n",
            "question: 근로계약이 미성년자에게 불리하다고 인정되는 경우 미성년후견인은 그 계약을 해지할 수 있나요?\n",
            "answer: 네. 근로계약을 해지할 수 있습니다.\n",
            "ground_truths: [\"「근로기준법」 제67조 제2항은 친권자,후견인 또는 고용노동부장관은 근로계약이 미성년자에게 불리하다고 인정하는 경우에는 이를 해지할 수 있다.\"라고 규정...\n",
            "  메타데이터: {'source': 'https://docs.google.com/spreadsheets/d/1QeMvmrcYe6QQ8L1n6o1NQ7ASTPafmzdbuDAAKXFCu3k/edit?gid=0', 'title': 'filtered_qa_dataset - Sheet1', 'row': 1, 'file_type': 'google_sheet'}\n",
            "\n",
            "[정보] 모든 청크 크기 비교적 정상.\n",
            "--- 단계 4: 텍스트 분할 완료 ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 4: 텍스트 분할 (패턴 기반 + 길이 제한) ===\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document # Document 클래스 임포트 확인\n",
        "\n",
        "print(\"\\n--- 단계 4: 텍스트 분할 시작 (패턴 기반 적용) ---\")\n",
        "\n",
        "split_texts = []\n",
        "chunk_size_setting = 500\n",
        "chunk_overlap_setting = 100\n",
        "# 최대 길이 초과 시 사용할 fallback 스플리터\n",
        "fallback_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size_setting,\n",
        "    chunk_overlap=chunk_overlap_setting\n",
        ")\n",
        "\n",
        "if 'loaded_documents' in locals() and loaded_documents:\n",
        "    print(f\"[정보] 패턴 기반 청킹 시도 (법률 PDF 대상). 기준 크기={chunk_size_setting}, 중첩={chunk_overlap_setting}\")\n",
        "    processed_docs_count = 0\n",
        "    skipped_docs_count = 0\n",
        "\n",
        "    for doc in tqdm(loaded_documents, desc=\"문서 청킹 중\"):\n",
        "        doc_content = doc.page_content\n",
        "        doc_metadata = doc.metadata\n",
        "        file_type = doc_metadata.get('file_type', 'unknown')\n",
        "\n",
        "        # --- PDF 문서(법률 문서로 간주)에만 패턴 기반 적용 ---\n",
        "        # 파일 타입이 'pdf'가 아니거나 내용이 없으면 기본 스플리터 사용\n",
        "        if file_type != 'pdf' or not doc_content.strip():\n",
        "            if doc_content.strip(): # 내용이 있을 때만 분할\n",
        "                 try:\n",
        "                     sub_chunks = fallback_splitter.split_text(doc_content)\n",
        "                     for chunk_text in sub_chunks:\n",
        "                         split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))\n",
        "                     processed_docs_count += 1\n",
        "                 except Exception as e:\n",
        "                      print(f\"\\n!! 기본 분할 오류 (타입: {file_type}, 소스: {doc_metadata.get('source', 'N/A')}): {e}\")\n",
        "                      skipped_docs_count += 1\n",
        "            else:\n",
        "                skipped_docs_count += 1\n",
        "            continue # 다음 문서로 이동\n",
        "\n",
        "        # --- 패턴 기반 청킹 로직 (PDF 대상) ---\n",
        "        try:\n",
        "            # 1. \"제X조\" 패턴으로 먼저 크게 나누기\n",
        "            # 정규표현식: '제' + (선택적 공백) + 숫자 + (선택적 공백) + '조'\n",
        "            # re.split은 구분자도 결과에 포함시키므로, 이를 활용하여 조 제목을 유지\n",
        "            preliminary_chunks_with_title = re.split(r'(제\\s?\\d+\\s?조)', doc_content)\n",
        "\n",
        "            current_chunk_content = \"\"\n",
        "            # 첫 부분 처리 (첫 '제X조' 이전 내용)\n",
        "            if preliminary_chunks_with_title[0].strip():\n",
        "                 current_chunk_content = preliminary_chunks_with_title[0].strip()\n",
        "\n",
        "            # '제X조' 제목과 그 내용을 묶어서 처리\n",
        "            for i in range(1, len(preliminary_chunks_with_title), 2):\n",
        "                title = preliminary_chunks_with_title[i] # 예: \"제1조\"\n",
        "                content = preliminary_chunks_with_title[i+1] if (i+1) < len(preliminary_chunks_with_title) else \"\"\n",
        "\n",
        "                article_block = title + content # \"제1조 내용...\"\n",
        "\n",
        "                # 이전 청크가 있고 + 현재 조항을 합쳐도 크기를 넘지 않으면 합침\n",
        "                # (짧은 조항들이 합쳐지는 효과)\n",
        "                if current_chunk_content and (len(current_chunk_content) + len(article_block) <= chunk_size_setting):\n",
        "                    current_chunk_content += \"\\n\\n\" + article_block # 문단 구분 추가\n",
        "                else:\n",
        "                    # 이전 청크가 너무 길었거나, 합치면 길어지는 경우\n",
        "                    # 이전 청크를 최종 처리하고 현재 조항으로 새 청크 시작\n",
        "                    if current_chunk_content: # 이전 청크 내용이 있으면\n",
        "                        # 이전 청크가 최대 크기를 넘는지 확인\n",
        "                        if len(current_chunk_content) > chunk_size_setting:\n",
        "                            # 넘으면 fallback 스플리터로 재분할\n",
        "                            sub_chunks = fallback_splitter.split_text(current_chunk_content)\n",
        "                            for chunk_text in sub_chunks:\n",
        "                                split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))\n",
        "                        else:\n",
        "                            # 안 넘으면 그대로 추가\n",
        "                            split_texts.append(Document(page_content=current_chunk_content, metadata=doc_metadata.copy()))\n",
        "\n",
        "                    # 현재 조항으로 새 청크 시작\n",
        "                    current_chunk_content = article_block.strip() # 새 청크 시작\n",
        "\n",
        "            # 마지막 남은 청크 처리\n",
        "            if current_chunk_content:\n",
        "                 if len(current_chunk_content) > chunk_size_setting:\n",
        "                     sub_chunks = fallback_splitter.split_text(current_chunk_content)\n",
        "                     for chunk_text in sub_chunks:\n",
        "                         split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))\n",
        "                 else:\n",
        "                     split_texts.append(Document(page_content=current_chunk_content, metadata=doc_metadata.copy()))\n",
        "\n",
        "            processed_docs_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n!! 패턴 기반 분할 오류 (소스: {doc_metadata.get('source', 'N/A')}): {e}\")\n",
        "            # 오류 발생 시 해당 문서는 기본 스플리터로 처리 시도 (선택 사항)\n",
        "            try:\n",
        "                sub_chunks = fallback_splitter.split_text(doc_content)\n",
        "                for chunk_text in sub_chunks:\n",
        "                     split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))\n",
        "                print(f\"  -> 오류 발생하여 기본 분할 방식으로 처리함.\")\n",
        "                processed_docs_count += 1\n",
        "            except Exception as fallback_e:\n",
        "                 print(f\"  -> 기본 분할 방식도 실패: {fallback_e}\")\n",
        "                 skipped_docs_count += 1\n",
        "\n",
        "    print(f\"\\n[성공] 총 {processed_docs_count}개 원본 조각 처리 -> {len(split_texts)}개 청크 분할 완료.\")\n",
        "    if skipped_docs_count > 0: print(f\"[경고] {skipped_docs_count}개 원본 조각 처리 중 오류 발생하여 건너<0xEB><0x9B><0x81>.\")\n",
        "\n",
        "    if split_texts:\n",
        "         print(\"\\n첫 분할 청크 샘플:\")\n",
        "         print(f\"  내용(200자): {split_texts[0].page_content[:200]}...\")\n",
        "         print(f\"  메타데이터: {split_texts[0].metadata}\")\n",
        "         # 크기 확인 (Fallback 적용 후에도 클 수 있음)\n",
        "         oversized = [len(c.page_content) for c in split_texts if len(c.page_content) > chunk_size_setting + chunk_overlap_setting] # 오버랩 고려\n",
        "         if oversized: print(f\"\\n[경고] {len(oversized)}개 청크가 최대 크기({chunk_size_setting}자)를 약간 초과할 수 있음 (최대 {max(oversized)}자).\")\n",
        "         else: print(f\"\\n[정보] 모든 청크 크기 비교적 정상.\")\n",
        "    else: print(\"[정보] 생성된 청크 없음.\")\n",
        "\n",
        "else:\n",
        "    print(\"!! 분할할 로드된 문서 없음.\")\n",
        "\n",
        "print(\"--- 단계 4: 텍스트 분할 완료 ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbxJAJcwr6Uk"
      },
      "source": [
        "# 5: 임베딩 모델 설정 (text-embedding-3-large)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O55krQoYcdmf",
        "outputId": "bb19aca3-b682-469b-81bb-f03c3b43263e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 5: 임베딩 모델 설정 시작 (OpenAI: text-embedding-3-large) ---\n",
            "[정보] langchain_openai.OpenAIEmbeddings 임포트 확인.\n",
            "[정보] OpenAI 임베딩 모델 (text-embedding-3-large) 설정 시도...\n",
            "[정보] 단계 2에서 설정된 OPENAI_API_KEY 환경 변수를 사용합니다.\n",
            "[정보] 임베딩 모델 테스트 중 (OpenAI API 호출)...\n",
            "[성공] OpenAI 임베딩 모델 (text-embedding-3-large) 설정 및 테스트 완료.\n",
            "--- 단계 5: 임베딩 모델 설정 완료 ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 5: 임베딩 모델 설정 (OpenAI: text-embedding-3-large) ===\n",
        "print(\"\\n--- 단계 5: 임베딩 모델 설정 시작 (OpenAI: text-embedding-3-large) ---\")\n",
        "\n",
        "# 필요한 라이브러리 임포트\n",
        "try:\n",
        "    # OpenAI 임베딩을 위한 클래스 임포트\n",
        "    # pip install -U langchain-openai  <-- 먼저 이 패키지를 설치해야 합니다.\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    print(\"[정보] langchain_openai.OpenAIEmbeddings 임포트 확인.\")\n",
        "except ImportError as e:\n",
        "    print(f\"!! [오류] 필요한 라이브러리 임포트 실패: {e}\")\n",
        "    print(\"   langchain-openai, openai 패키지가 설치되었는지 확인하세요.\")\n",
        "    # 필요한 클래스가 없으면 이후 진행 불가\n",
        "    OpenAIEmbeddings = None\n",
        "\n",
        "embeddings = None # 최종 임베딩 객체 변수 초기화\n",
        "\n",
        "if OpenAIEmbeddings: # 라이브러리 임포트 성공 시 진행\n",
        "    try:\n",
        "        # 사용할 OpenAI 모델 지정\n",
        "        openai_model_name = \"text-embedding-3-large\"\n",
        "        print(f\"[정보] OpenAI 임베딩 모델 ({openai_model_name}) 설정 시도...\")\n",
        "        print(\"[정보] 단계 2에서 설정된 OPENAI_API_KEY 환경 변수를 사용합니다.\")\n",
        "\n",
        "        # <<< OpenAIEmbeddings 클래스 사용 (API 키 자동 감지) >>>\n",
        "        # 단계 2에서 os.environ[\"OPENAI_API_KEY\"] 가 설정되었으므로,\n",
        "        # API 키를 명시적으로 전달할 필요 없이 자동으로 사용됩니다.\n",
        "        embeddings = OpenAIEmbeddings(\n",
        "            model=openai_model_name # 사용할 모델 이름만 지정\n",
        "            # openai_api_key 파라미터 생략\n",
        "        )\n",
        "        # ------------------------------------------------------\n",
        "\n",
        "        # 간단한 테스트 (API 키 유효성 및 통신 확인)\n",
        "        print(\"[정보] 임베딩 모델 테스트 중 (OpenAI API 호출)...\")\n",
        "        _ = embeddings.embed_query(\"테스트 문장입니다.\")\n",
        "        print(f\"[성공] OpenAI 임베딩 모델 ({openai_model_name}) 설정 및 테스트 완료.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!! [오류] OpenAI 임베딩 모델 ({openai_model_name}) 설정 실패: {e}\")\n",
        "        print(\"   - 단계 2에서 OpenAI API 키가 올바르게 설정되었는지 확인하세요.\") # <<< 오류 메시지 수정\n",
        "        print(\"   - OpenAI API 키 자체의 유효성 및 할당량(quota)을 확인하세요.\")\n",
        "        print(\"   - 인터넷 연결 상태 및 OpenAI 서비스 상태를 확인하세요.\")\n",
        "        print(\"   - 관련 라이브러리(langchain-openai, openai) 설치 및 호환성을 확인하세요.\")\n",
        "        embeddings = None # 실패 시 None으로 설정\n",
        "else:\n",
        "    print(\"!! 필요한 라이브러리(OpenAIEmbeddings) 임포트 실패. 임베딩 모델 설정 불가.\")\n",
        "\n",
        "print(\"--- 단계 5: 임베딩 모델 설정 완료 ---\")\n",
        "\n",
        "# OpenAI 모델은 외부 API를 사용하므로 로컬 GPU/CPU 확인은 불필요합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dWM7T_R4ZIF"
      },
      "source": [
        "# 단계 6: Vector Store 구축 (FAISS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvIY_g-74WB4"
      },
      "source": [
        "--- 6.2: 경로 설정 ---\n",
        "📂 결과 저장 경로: /content/drive/MyDrive/nomu_rag_result\n",
        "  - 상태 파일: /content/drive/MyDrive/nomu_rag_result/vectorstore_build_status.json\n",
        "  - 체크포인트 폴더 (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_checkpoint\n",
        "  - 최종 인덱스 폴더 (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_final\n",
        "  - BM25용 데이터 파일: /content/drive/MyDrive/nomu_rag_result/split_texts_for_bm25.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1d9c58383937428cad5ee18e87311544",
            "7818ce83534c4635942d9a151432374e",
            "636c9b9a640947768574acd4ee9d76cb",
            "ffec0d2134a040cfa24e8887cce55b66",
            "ba139e0a87104ec682b12d5e81bea071",
            "a6807fa524074dccb399fa0cd95ceb0f",
            "6fef3cfa6ade4c2da9d2b1908fe69d02",
            "e5cfe01f8b92442e83c9db32622cf0e2",
            "c3bf5302a364449e8c90595b1cb060b5",
            "792144951d63405a9e7e5ce21179d713",
            "0058beb07765417c984f68278c6458e2"
          ]
        },
        "id": "jo8-0rLR3ncZ",
        "outputId": "6f4af03b-6d95-42ba-f416-203b56810326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 6: Vector Store 구축 및 BM25 데이터 저장 시작 ---\n",
            "\n",
            "--- 6.1: 설정 및 입력 변수 확인 ---\n",
            "✅ 입력 확인: 총 9429개 청크 및 임베딩 모델 확인됨.\n",
            "\n",
            "--- 6.2: 경로 설정 ---\n",
            "📂 결과 저장 경로: /content/drive/MyDrive/nomu_rag_result\n",
            "  - 상태 파일: /content/drive/MyDrive/nomu_rag_result/vectorstore_build_status.json\n",
            "  - 체크포인트 폴더 (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_checkpoint\n",
            "  - 최종 인덱스 폴더 (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_final\n",
            "  - BM25용 데이터 파일: /content/drive/MyDrive/nomu_rag_result/split_texts_for_bm25.pkl\n",
            "\n",
            "--- 6.3: 장치 확인 ---\n",
            "ℹ️ 사용할 장치: cpu\n",
            "⚠️ 경고: GPU 사용 불가. 시간 소요 예상.\n",
            "\n",
            "--- 6.4: 이전 작업 상태 및 체크포인트 로드 ---\n",
            "  - 이전 상태 파일 또는 체크포인트 폴더 없음. 처음부터 시작.\n",
            "\n",
            "--- 6.5: FAISS 인덱스 구축 시작 (총 9429 중 0부터, 배치 500) ---\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d9c58383937428cad5ee18e87311544",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "FAISS 구축 중:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   - 첫 배치 FAISS 생성 완료 (벡터 500개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 1000개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 1500개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 2000개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 2500개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 3000개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 3500개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 4000개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 4500개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 5000개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 5500개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 6000개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 6500개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 7000개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 7500개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 8000개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 8500개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 9000개)\n",
            "\n",
            "   - 배치 FAISS 추가 완료 (총 벡터 9429개)\n",
            "\n",
            "[성공] 모든 FAISS 배치 처리 완료.\n",
            "\n",
            "--- 6.6: 최종 결과 저장 ---\n",
            "💾 최종 FAISS 인덱스 저장: /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_final\n",
            "✅ 최종 FAISS 인덱스 저장 완료.\n",
            "💾 BM25용 텍스트 데이터 저장: /content/drive/MyDrive/nomu_rag_result/split_texts_for_bm25.pkl\n",
            "✅ BM25용 텍스트 데이터 (9429개 청크) 저장 완료.\n",
            "\n",
            "--- 단계 6: Vector Store 처리 및 BM25 데이터 저장 완료 ---\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# === 단계 6: Vector Store 구축 (FAISS) 및 BM25용 데이터 저장 ===\n",
        "# 체크포인팅 방식을 save_local로 변경\n",
        "\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "# import pickle # pickle은 BM25 데이터 저장에만 사용\n",
        "import torch\n",
        "import traceback\n",
        "from tqdm.auto import tqdm\n",
        "# LangChain 관련 임포트 확인\n",
        "if 'FAISS' not in locals() or 'Document' not in locals():\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "    from langchain_core.documents import Document\n",
        "    print(\"⚠️ FAISS 또는 Document 클래스 재임포트됨.\")\n",
        "if 'HuggingFaceEmbeddings' not in locals() and 'GoogleGenerativeAIEmbeddings' not in locals():\n",
        "    # 사용 중인 임베딩 클래스를 임포트해야 합니다.\n",
        "    from langchain_community.embeddings import HuggingFaceEmbeddings # 예시\n",
        "    # from langchain_google_genai import GoogleGenerativeAIEmbeddings # 예시\n",
        "    print(\"⚠️ 임베딩 클래스 재임포트됨.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- 단계 6: Vector Store 구축 및 BM25 데이터 저장 시작 ---\")\n",
        "\n",
        "# --- 6.1: 설정 및 입력 변수 확인 ---\n",
        "print(\"\\n--- 6.1: 설정 및 입력 변수 확인 ---\")\n",
        "vectorstore = None\n",
        "batch_size = 500 # 배치 크기 확인\n",
        "sleep_time = 5\n",
        "max_retries = 3\n",
        "total_chunks = 0\n",
        "start_index = 0\n",
        "loaded_from_checkpoint = False\n",
        "\n",
        "if 'split_texts' not in locals() or not isinstance(split_texts, list) or not split_texts: print(\"!! 오류: 'split_texts' 없음. 중단.\"); exit()\n",
        "elif 'embeddings' not in locals() or embeddings is None: print(\"!! 오류: 'embeddings' 없음. 중단.\"); exit()\n",
        "else: total_chunks = len(split_texts); print(f\"✅ 입력 확인: 총 {total_chunks}개 청크 및 임베딩 모델 확인됨.\")\n",
        "\n",
        "# --- 6.2: 경로 설정 및 디렉토리 생성 ---\n",
        "print(\"\\n--- 6.2: 경로 설정 ---\")\n",
        "if 'result_dir' not in locals() or not result_dir: result_dir = \"/content/drive/MyDrive/nomu_rag_result\"; print(f\"⚠️ result_dir 변수 없어 기본 경로 설정: {result_dir}\"); os.makedirs(result_dir, exist_ok=True)\n",
        "else: print(f\"📂 결과 저장 경로: {result_dir}\"); os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "vs_status_file = os.path.join(result_dir, \"vectorstore_build_status.json\")\n",
        "# <<< 체크포인트 경로를 폴더로 변경 (save_local 사용) >>>\n",
        "vs_checkpoint_path = os.path.join(result_dir, \"faiss_index_nomu_checkpoint\") # .pkl 대신 폴더명\n",
        "vs_final_save_path = os.path.join(result_dir, \"faiss_index_nomu_final\")\n",
        "bm25_data_save_path = os.path.join(result_dir, \"split_texts_for_bm25.pkl\")\n",
        "\n",
        "print(f\"  - 상태 파일: {vs_status_file}\")\n",
        "print(f\"  - 체크포인트 폴더 (FAISS): {vs_checkpoint_path}\") # <<< 이름 변경\n",
        "print(f\"  - 최종 인덱스 폴더 (FAISS): {vs_final_save_path}\")\n",
        "print(f\"  - BM25용 데이터 파일: {bm25_data_save_path}\")\n",
        "\n",
        "# --- 6.3: GPU 확인 --- (이전과 동일)\n",
        "print(\"\\n--- 6.3: 장치 확인 ---\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'; print(f\"ℹ️ 사용할 장치: {device}\");\n",
        "if device == 'cpu': print(\"⚠️ 경고: GPU 사용 불가. 시간 소요 예상.\")\n",
        "\n",
        "# --- 6.4: 이전 작업 상태 및 체크포인트 로드 ---\n",
        "if total_chunks > 0:\n",
        "    print(\"\\n--- 6.4: 이전 작업 상태 및 체크포인트 로드 ---\")\n",
        "    try:\n",
        "        # <<< 체크포인트 파일 대신 폴더 존재 여부 확인 >>>\n",
        "        if os.path.exists(vs_status_file) and os.path.isdir(vs_checkpoint_path):\n",
        "            with open(vs_status_file, 'r') as f_status: status_data = json.load(f_status); last_processed_index = status_data.get('last_processed_index', -1); start_index = last_processed_index + 1; print(f\"  - 상태 로드 완료. 마지막 인덱스: {last_processed_index}\")\n",
        "\n",
        "            # <<< pickle.load 대신 FAISS.load_local 사용 >>>\n",
        "            print(f\"  - 체크포인트 인덱스 로딩: {vs_checkpoint_path}\")\n",
        "            if 'embeddings' in locals() and embeddings:\n",
        "                vectorstore = FAISS.load_local(\n",
        "                    folder_path=vs_checkpoint_path,\n",
        "                    embeddings=embeddings,\n",
        "                    allow_dangerous_deserialization=True # 신뢰할 수 있는 소스일 때만\n",
        "                )\n",
        "                print(f\"  - FAISS 체크포인트 로드 완료. {vectorstore.index.ntotal} 벡터 포함.\")\n",
        "                loaded_from_checkpoint = True\n",
        "            else:\n",
        "                print(\"  !! 오류: 임베딩 함수('embeddings')가 없어 체크포인트를 로드할 수 없음. 처음부터 시작.\")\n",
        "                vectorstore = None; start_index = 0; loaded_from_checkpoint = False\n",
        "\n",
        "            if start_index >= total_chunks: print(\"  ✅ 이전 FAISS 작업 완료됨.\")\n",
        "            elif loaded_from_checkpoint: print(f\"  ▶️ {start_index}번 인덱스부터 FAISS 작업 재개.\")\n",
        "            # else 블록은 위에서 처리됨\n",
        "\n",
        "        else: print(f\"  - 이전 상태 파일 또는 체크포인트 폴더 없음. 처음부터 시작.\"); vectorstore = None; start_index = 0\n",
        "    except Exception as e: print(f\"  ⚠️ 상태/체크포인트 로드 오류: {e}. 처음부터 시작.\"); traceback.print_exc(); vectorstore = None; start_index = 0\n",
        "else: print(\"ℹ️ 처리할 청크 없음.\")\n",
        "\n",
        "# --- 6.5: FAISS Vector Store 구축 ---\n",
        "if total_chunks > 0 and start_index < total_chunks:\n",
        "    print(f\"\\n--- 6.5: FAISS 인덱스 구축 시작 (총 {total_chunks} 중 {start_index}부터, 배치 {batch_size}) ---\")\n",
        "    all_processed_successfully = True\n",
        "    try:\n",
        "        progress_bar = tqdm(range(start_index, total_chunks, batch_size), initial=start_index // batch_size, total=-(total_chunks // -batch_size), desc=\"FAISS 구축 중\")\n",
        "        for i in progress_bar:\n",
        "            batch_start_idx = i; batch_end_idx = min(i + batch_size, total_chunks); batch_docs = split_texts[batch_start_idx:batch_end_idx]\n",
        "            for attempt in range(max_retries):\n",
        "                try:\n",
        "                    if i == start_index and not loaded_from_checkpoint:\n",
        "                        progress_bar.set_description(f\"첫 배치({batch_start_idx}-{batch_end_idx-1}) 생성 중\")\n",
        "                        vectorstore = FAISS.from_documents(batch_docs, embeddings)\n",
        "                        print(f\"\\n   - 첫 배치 FAISS 생성 완료 (벡터 {vectorstore.index.ntotal}개)\")\n",
        "                    elif vectorstore is not None:\n",
        "                        progress_bar.set_description(f\"배치({batch_start_idx}-{batch_end_idx-1}) 추가 중\")\n",
        "                        vectorstore.add_documents(batch_docs)\n",
        "                        print(f\"\\n   - 배치 FAISS 추가 완료 (총 벡터 {vectorstore.index.ntotal}개)\")\n",
        "                    else: raise ValueError(\"Vectorstore 객체 None 상태.\")\n",
        "\n",
        "                    current_processed_index = batch_end_idx - 1\n",
        "                    try:\n",
        "                        # <<< 체크포인팅: 상태 저장 후 save_local 호출 >>>\n",
        "                        with open(vs_status_file, 'w') as f_status: json.dump({'last_processed_index': current_processed_index}, f_status)\n",
        "                        # 체크포인트 폴더에 덮어쓰기 (이전 파일 삭제됨)\n",
        "                        vectorstore.save_local(vs_checkpoint_path)\n",
        "                        # print(f\"  💾 체크포인트 저장 완료 (폴더: {vs_checkpoint_path}, 인덱스: {current_processed_index})\")\n",
        "                    except Exception as e_save: print(f\"  ⚠️ 진행 상황(체크포인트) 저장 실패: {e_save}\")\n",
        "                    break # 성공 시 재시도 루프 탈출\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n!! 배치 처리 오류 (시도 {attempt+1}/{max_retries}): {type(e).__name__} - {e}\")\n",
        "                    traceback.print_exc()\n",
        "                    if attempt < max_retries - 1: wait_time = sleep_time * (attempt + 2); progress_bar.set_description(f\"오류, {wait_time}초 후 재시도...\"); print(f\"  ... {wait_time}초 후 재시도 ...\"); time.sleep(wait_time)\n",
        "                    else: print(\"!! 최대 재시도 초과. 구축 중단.\"); all_processed_successfully = False; raise RuntimeError(f\"최대 재시도 실패: {e}\")\n",
        "            if not all_processed_successfully: break\n",
        "            if batch_end_idx < total_chunks: time.sleep(sleep_time) # API Rate Limit\n",
        "\n",
        "        # --- 루프 완료 후 최종 처리 ---\n",
        "        if all_processed_successfully:\n",
        "            print(\"\\n[성공] 모든 FAISS 배치 처리 완료.\")\n",
        "            # --- 6.6: 최종 결과 저장 ---\n",
        "            print(\"\\n--- 6.6: 최종 결과 저장 ---\")\n",
        "            # 1. 최종 FAISS 인덱스 저장 (save_local)\n",
        "            print(f\"💾 최종 FAISS 인덱스 저장: {vs_final_save_path}\")\n",
        "            try:\n",
        "                if vectorstore: vectorstore.save_local(vs_final_save_path); print(f\"✅ 최종 FAISS 인덱스 저장 완료.\")\n",
        "                else: print(\"⚠️ 최종 저장 시 VectorStore 객체 없음.\")\n",
        "            except Exception as e: print(f\"❌ 최종 FAISS 인덱스 저장 오류: {e}\"); traceback.print_exc()\n",
        "            # 2. BM25용 데이터 저장 (pickle)\n",
        "            print(f\"💾 BM25용 텍스트 데이터 저장: {bm25_data_save_path}\")\n",
        "            try:\n",
        "                if 'split_texts' in locals() and split_texts:\n",
        "                    # <<< pickle 임포트 확인 >>>\n",
        "                    import pickle\n",
        "                    with open(bm25_data_save_path, 'wb') as f_texts: pickle.dump(split_texts, f_texts); print(f\"✅ BM25용 텍스트 데이터 ({len(split_texts)}개 청크) 저장 완료.\")\n",
        "                else: print(\"⚠️ BM25용 데이터('split_texts') 없어 저장 불가.\")\n",
        "            except Exception as e: print(f\"❌ split_texts 저장 실패: {e}\"); traceback.print_exc()\n",
        "            # (선택) 중간 파일 삭제\n",
        "            # try: ... (삭제 로직) ... except ...\n",
        "\n",
        "    except Exception as e_main_loop: print(f\"\\n!! FAISS 구축 실패: {e_main_loop}\"); print(f\"[정보] 마지막 성공 지점 데이터는 '{vs_checkpoint_path}' 폴더에 있을 수 있음.\")\n",
        "\n",
        "# --- 이전 실행에서 이미 완료된 경우 처리 ---\n",
        "elif total_chunks > 0 and start_index >= total_chunks:\n",
        "     print(\"\\n✅ FAISS Vector Store 구축 작업이 이미 완료된 상태입니다.\")\n",
        "     # 완료된 상태에서도 최종 파일들이 존재하는지 확인하고 없으면 저장 시도\n",
        "     # 1. 최종 FAISS 인덱스 확인 및 저장\n",
        "     if not os.path.isdir(vs_final_save_path): # <<< 폴더 존재 여부 확인 >>>\n",
        "         if vectorstore: # 체크포인트에서 로드된 vectorstore가 있다면\n",
        "             print(f\"\\n💾 최종 FAISS 인덱스 폴더 없어 저장 시도: '{vs_final_save_path}'\") # <<< print 문 분리\n",
        "             # <<< try...except 블록을 새 줄에서 시작 >>>\n",
        "             try:\n",
        "                 vectorstore.save_local(vs_final_save_path)\n",
        "                 print(\"✅ 저장 완료.\")\n",
        "             except Exception as e:\n",
        "                 print(f\"❌ 저장 오류: {e}\")\n",
        "                 traceback.print_exc()\n",
        "         else:\n",
        "             print(f\"⚠️ 최종 FAISS 인덱스 폴더 없고, 로드된 vectorstore도 없어 저장 불가.\")\n",
        "\n",
        "     # 2. BM25용 데이터 확인 및 저장\n",
        "     if not os.path.exists(bm25_data_save_path):\n",
        "         if 'split_texts' in locals() and split_texts:\n",
        "             print(f\"\\n💾 BM25용 텍스트 데이터 파일({bm25_data_save_path}) 없어 저장 시도...\") # <<< print 문 분리\n",
        "             # <<< try...except 블록을 새 줄에서 시작 >>>\n",
        "             try:\n",
        "                 # <<< pickle 임포트 확인 (필요시) >>>\n",
        "                 import pickle\n",
        "                 with open(bm25_data_save_path, 'wb') as f:\n",
        "                     pickle.dump(split_texts, f)\n",
        "                 print(\"✅ 저장 완료.\")\n",
        "             except Exception as e:\n",
        "                 print(f\"❌ 저장 실패: {e}\")\n",
        "                 traceback.print_exc()\n",
        "         else:\n",
        "             print(f\"⚠️ BM25용 데이터 파일 없고, split_texts 변수도 없어 저장 불가.\")\n",
        "\n",
        "print(\"\\n--- 단계 6: Vector Store 처리 및 BM25 데이터 저장 완료 ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_6r7-WP5Ynw"
      },
      "source": [
        "# 단계 8: Retriever 설정 (하이브리드 검색)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc2RDKm14skp",
        "outputId": "01182549-280e-465c-c0a0-208ed07f343c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 8: Retriever 설정 시작 (하이브리드 검색) ---\n",
            "[정보] 새로 생성된 FAISS VectorStore 사용.\n",
            "[정보] 현재 세션의 split_texts 데이터 사용 (BM25용).\n",
            "- Dense Retriever (FAISS) 설정 완료 (k=6).\n",
            "- Sparse Retriever (BM25) 설정 완료 (k=6).\n",
            "- Ensemble Retriever 설정 완료 (Weights: BM25=0.7, FAISS=0.3).\n",
            "--- 단계 8: Retriever 설정 완료 ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 8: Retriever 설정 (하이브리드 검색) ===\n",
        "\n",
        "print(\"\\n--- 단계 8: Retriever 설정 시작 (하이브리드 검색) ---\")\n",
        "\n",
        "# 사용할 VectorStore와 split_texts 결정\n",
        "# 로드 성공 시 로드된 것 사용, 아니면 구축 단계에서 생성된 것 사용\n",
        "final_vectorstore = None\n",
        "final_split_texts = None # BM25용\n",
        "\n",
        "if 'loaded_vectorstore' in locals() and loaded_vectorstore:\n",
        "    final_vectorstore = loaded_vectorstore\n",
        "    print(\"[정보] 로드된 FAISS VectorStore 사용.\")\n",
        "elif 'vectorstore' in locals() and vectorstore:\n",
        "    final_vectorstore = vectorstore\n",
        "    print(\"[정보] 새로 생성된 FAISS VectorStore 사용.\")\n",
        "else:\n",
        "    print(\"!! 오류: 사용 가능한 FAISS VectorStore 객체 없음.\")\n",
        "\n",
        "if 'loaded_split_texts' in locals() and loaded_split_texts:\n",
        "    final_split_texts = loaded_split_texts\n",
        "    print(\"[정보] 로드된 split_texts 데이터 사용 (BM25용).\")\n",
        "elif 'split_texts' in locals() and split_texts:\n",
        "    final_split_texts = split_texts\n",
        "    print(\"[정보] 현재 세션의 split_texts 데이터 사용 (BM25용).\")\n",
        "else:\n",
        "    print(\"!! 오류: BM25용 split_texts 데이터 없음.\")\n",
        "\n",
        "retriever = None # 최종 리트리버 초기화\n",
        "\n",
        "if final_vectorstore:\n",
        "    # Dense Retriever (FAISS)\n",
        "    faiss_retriever = final_vectorstore.as_retriever(search_kwargs={'k': 6})\n",
        "    print(f\"- Dense Retriever (FAISS) 설정 완료 (k={faiss_retriever.search_kwargs.get('k')}).\")\n",
        "\n",
        "    if final_split_texts: # BM25용 데이터가 있을 때만 하이브리드 시도\n",
        "        try:\n",
        "            # Sparse Retriever (BM25)\n",
        "            bm25_retriever = BM25Retriever.from_documents(final_split_texts)\n",
        "            bm25_retriever.k = 6\n",
        "            print(f\"- Sparse Retriever (BM25) 설정 완료 (k={bm25_retriever.k}).\")\n",
        "            # Ensemble Retriever\n",
        "            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.4, 0.6])\n",
        "            retriever = ensemble_retriever\n",
        "            print(f\"- Ensemble Retriever 설정 완료 (Weights: BM25=0.7, FAISS=0.3).\")\n",
        "        except Exception as e:\n",
        "            print(f\"!! BM25/Ensemble 설정 실패: {e}. Dense Retriever만 사용.\")\n",
        "            retriever = faiss_retriever # Fallback\n",
        "    else:\n",
        "        print(\"⚠️ BM25 데이터 없어 Dense Retriever(FAISS)만 사용합니다.\")\n",
        "        retriever = faiss_retriever # Fallback\n",
        "else:\n",
        "    print(\"!! Vector Store 준비 안 됨. Retriever 설정 불가.\")\n",
        "\n",
        "print(\"--- 단계 8: Retriever 설정 완료 ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X_3hcxYsxfY"
      },
      "source": [
        "# 9: LLM 설정 (OpenAI: gpt-4.1-nano)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LviMIiKoNsln"
      },
      "source": [
        "현재의 rag 파이프라인은 RAGAS 성능 평가용 LLM과 응답 생성용 LLM을 분리하지 않고 동일하게 사용하는 코드입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bBYfQwWsyx9",
        "outputId": "cbfbf630-af55-4c75-a49f-a4b19e3f50ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 9: LLM 설정 시작 (OpenAI) ---\n",
            "[성공] OpenAI (gpt-4.1-nano) LLM 로딩 완료.\n",
            "--- 단계 9: LLM 설정 완료 ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 9: LLM 설정 (OpenAI - gpt-4.1-nano) ===\n",
        "\n",
        "print(\"\\n--- 단계 9: LLM 설정 시작 (OpenAI) ---\")\n",
        "llm = None\n",
        "# OpenAI API 키 설정 여부 확인 (단계 2에서 설정됨)\n",
        "if 'OPENAI_API_KEY' in os.environ and os.environ[\"OPENAI_API_KEY\"]:\n",
        "    try:\n",
        "        # === 사용할 OpenAI 모델 지정 ===\n",
        "        # llm_model_name = \"gpt-4o\" # 최신 모델 (성능과 속도 균형)\n",
        "        llm_model_name = \"gpt-4.1-nano\" # 만약 더 작고 빠른 모델이 필요하다면 고려 (현재는 gpt-4o가 가장 유사)\n",
        "        # llm_model_name = \"gpt-3.5-turbo\" # 속도/비용 우선 시 고려\n",
        "        # ==============================\n",
        "\n",
        "        llm = ChatOpenAI(\n",
        "            model=llm_model_name,\n",
        "            temperature=0 # 답변의 창의성 조절 (낮을수록 결정적)\n",
        "            # max_tokens=1024 # 필요시 최대 출력 토큰 수 제한\n",
        "        )\n",
        "        print(f\"[성공] OpenAI ({llm.model_name}) LLM 로딩 완료.\") # .model 대신 .model_name 사용\n",
        "\n",
        "    except ImportError:\n",
        "         print(\"!! [오류] langchain-openai 또는 openai 라이브러리가 설치되지 않았습니다.\")\n",
        "         print(\"   단계 0의 설치 명령을 확인하고 런타임을 재시작하세요.\")\n",
        "    except Exception as e:\n",
        "        print(f\"!! [오류] OpenAI LLM 로딩 실패: {e}\")\n",
        "        traceback.print_exc() # 상세 오류 출력\n",
        "else:\n",
        "    print(\"!! [오류] OpenAI API 키가 설정되지 않았습니다. LLM 로드 불가.\")\n",
        "\n",
        "print(\"--- 단계 9: LLM 설정 완료 ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmu156Pos59b"
      },
      "source": [
        "# 10: RAG Chain/파이프라인 구축"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBJ8uY3ms4Zk",
        "outputId": "132d68a2-ce91-4fed-8237-d838c2c57a65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 10: RAG Chain 구축 시작 ---\n",
            "[성공] RetrievalQA Chain 구축 완료.\n",
            "--- 단계 10: RAG Chain 구축 완료 ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 10: RAG Chain/파이프라인 구축 ===\n",
        "# (코드 변경 없음 - 단계 9에서 변경된 llm 객체를 사용)\n",
        "print(\"\\n--- 단계 10: RAG Chain 구축 시작 ---\")\n",
        "qa_chain = None\n",
        "if llm and retriever: # llm이 ChatOpenAI 인스턴스로 준비됨\n",
        "    template = \"\"\"당신은 한국의 노무 규정 및 관련 문서에 기반하여 질문에 답변하는 유용한 AI 어시스턴트입니다.\n",
        "    주어진 다음 컨텍스트 정보만을 사용하여 질문에 답변하십시오. 컨텍스트에 없는 내용은 답변에 포함하지 마십시오.\n",
        "    만약 컨텍스트 정보만으로 질문에 답할 수 없다면, \"\n",
        "    제공된 문서 내용만으로는 답변할 수 없습니다.\"라고 명확히 답변하십시오.\n",
        "    답변은 간결하고 명확하게 한국어로 작성해주세요.\n",
        "\n",
        "    컨텍스트:\n",
        "    {context}\n",
        "\n",
        "    질문: {question}\n",
        "\n",
        "    답변 (한국어):\"\"\"\n",
        "    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
        "    try:\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm, # ChatOpenAI 인스턴스 사용\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        "        )\n",
        "        print(\"[성공] RetrievalQA Chain 구축 완료.\")\n",
        "    except Exception as e: print(f\"!! RAG Chain 구축 실패: {e}\")\n",
        "else: print(\"!! LLM 또는 Retriever 준비 안 됨. RAG Chain 구축 불가.\")\n",
        "print(\"--- 단계 10: RAG Chain 구축 완료 ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y0QsYTnXbYJ"
      },
      "source": [
        "# 10.1: RAG 파이프라인을 Tool로 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Nlsd6obXcUB",
        "outputId": "61e18bad-aca7-434a-e8c7-d5b7a4477319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 10.1: RAG 파이프라인을 Tool로 정의 시작 ---\n",
            "[성공] RAG 파이프라인을 위한 Tool 정의 완료.\n",
            "  Tool Name: KoreanLaborLawQASystem\n",
            "  Tool Description: 한국의 노무 법률, 규정, 관련 판례 및 문서에 대한 구체적인 질문에 답변할 때 사용합니다.\n",
            "                        근로기준법, 해고 절차, 임금, 휴가, 산업 안전 등 노무 관련 질문이 입력되었을 때 이 도구를 사용해야 합니다.\n",
            "                        일반적인 상식이나 대화에는 사용하지 마세요. 입력은 사용자의 질문이어야 합니다.\n",
            "--- 단계 10.1: Tool 정의 완료 ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 10.1: RAG 파이프라인을 Tool로 정의 ===\n",
        "print(\"\\n--- 단계 10.1: RAG 파이프라인을 Tool로 정의 시작 ---\")\n",
        "\n",
        "from langchain.tools import Tool\n",
        "\n",
        "rag_tool = None\n",
        "\n",
        "# 단계 10에서 qa_chain이 성공적으로 생성되었는지 확인\n",
        "if 'qa_chain' in locals() and qa_chain is not None:\n",
        "    try:\n",
        "        # Tool 정의\n",
        "        rag_tool = Tool.from_function(\n",
        "            func=lambda q: qa_chain.invoke({\"query\": q}), # qa_chain을 호출하는 함수\n",
        "            name=\"KoreanLaborLawQASystem\", # 도구 이름 (간결하고 명확하게)\n",
        "            description=\"\"\"한국의 노무 법률, 규정, 관련 판례 및 문서에 대한 구체적인 질문에 답변할 때 사용합니다.\n",
        "                        근로기준법, 해고 절차, 임금, 휴가, 산업 안전 등 노무 관련 질문이 입력되었을 때 이 도구를 사용해야 합니다.\n",
        "                        일반적인 상식이나 대화에는 사용하지 마세요. 입력은 사용자의 질문이어야 합니다.\"\"\", # <<< LLM이 이해할 수 있도록 상세히 작성!\n",
        "            return_direct=False # False로 설정해야 LLM이 도구 결과를 보고 추가 생각을 할 수 있음 (ReAct 방식)\n",
        "                               # True로 하면 도구 결과가 바로 최종 답변이 됨\n",
        "        )\n",
        "        print(\"[성공] RAG 파이프라인을 위한 Tool 정의 완료.\")\n",
        "        print(f\"  Tool Name: {rag_tool.name}\")\n",
        "        print(f\"  Tool Description: {rag_tool.description}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!! [오류] Tool 정의 중 오류 발생: {e}\")\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"!! [오류] 'qa_chain'이 정의되지 않았습니다. Tool을 만들 수 없습니다.\")\n",
        "\n",
        "print(\"--- 단계 10.1: Tool 정의 완료 ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDcaPlSqXrzV"
      },
      "source": [
        "# 10.2: ReAct 에이전트 설정 (OpenAI Tools Agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHiHU5WFXmOH",
        "outputId": "e4a9bc57-e752-4224-82f6-2ebbee40a152"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 10.2: ReAct 에이전트 설정 시작 ---\n",
            "[정보] 에이전트가 사용할 도구: ['KoreanLaborLawQASystem']\n",
            "[성공] OpenAI Tools Agent 생성 완료.\n",
            "[성공] Agent Executor 생성 완료.\n",
            "--- 단계 10.2: 에이전트 설정 완료 ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 10.2: ReAct 에이전트 설정 (OpenAI Tools Agent) ===\n",
        "print(\"\\n--- 단계 10.2: ReAct 에이전트 설정 시작 ---\")\n",
        "\n",
        "from langchain import hub # ReAct 프롬프트 템플릿 가져오기 위함\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "\n",
        "agent_executor = None\n",
        "agent = None\n",
        "\n",
        "# 단계 9에서 LLM(ChatOpenAI)이 성공적으로 로드되었는지 확인\n",
        "# 단계 10.1에서 rag_tool이 성공적으로 생성되었는지 확인\n",
        "if 'llm' in locals() and llm is not None and 'rag_tool' in locals() and rag_tool is not None:\n",
        "    try:\n",
        "        # 사용할 도구 리스트 정의 (현재는 RAG 도구 하나)\n",
        "        tools = [rag_tool]\n",
        "        print(f\"[정보] 에이전트가 사용할 도구: {[tool.name for tool in tools]}\")\n",
        "\n",
        "        # ReAct 스타일의 프롬프트 가져오기 (OpenAI Tools Agent용)\n",
        "        # LangChain Hub에서 검증된 프롬프트를 사용하는 것이 좋음\n",
        "        # prompt = hub.pull(\"hwchase17/openai-tools-agent\") # 예시 프롬프트 주소\n",
        "        # 또는 직접 ChatPromptTemplate 구성\n",
        "        from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "        # 간단한 ReAct 스타일 프롬프트 예시 (필요시 수정)\n",
        "        prompt_template = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"당신은 질문에 답변하는 AI 어시스턴트입니다.\n",
        "             필요하다면 다음 도구를 사용할 수 있습니다. 질문을 분석하고, 도구 사용이 필요하다고 판단되면 도구를 사용하세요.\n",
        "             만약 도구 없이 답변할 수 있거나, 질문이 도구 사용에 적합하지 않다면 직접 답변하세요.\n",
        "             답변은 항상 한국어로 작성해주세요.\"\"\"),\n",
        "            MessagesPlaceholder(variable_name=\"chat_history\", optional=True), # 대화 기록 (필요시)\n",
        "            (\"human\", \"{input}\"), # 사용자 입력\n",
        "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # 에이전트의 생각/행동 기록\n",
        "        ])\n",
        "\n",
        "        # OpenAI Tools Agent 생성\n",
        "        # 이 에이전트는 LLM이 도구를 사용할지, 어떤 도구를 사용할지, 어떤 입력을 줄지 결정하게 함\n",
        "        agent = create_openai_tools_agent(llm, tools, prompt_template)\n",
        "        print(\"[성공] OpenAI Tools Agent 생성 완료.\")\n",
        "\n",
        "        # Agent Executor 생성 (실제 ReAct 루프 실행)\n",
        "        agent_executor = AgentExecutor(\n",
        "            agent=agent,\n",
        "            tools=tools,\n",
        "            verbose=True, # 에이전트의 생각 과정을 보려면 True로 설정\n",
        "            handle_parsing_errors=True # LLM 출력 파싱 오류 처리\n",
        "        )\n",
        "        print(\"[성공] Agent Executor 생성 완료.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!! [오류] 에이전트 설정 중 오류 발생: {e}\")\n",
        "        traceback.print_exc()\n",
        "        agent_executor = None\n",
        "\n",
        "elif not ('llm' in locals() and llm is not None):\n",
        "     print(\"!! [오류] LLM이 준비되지 않아 에이전트를 설정할 수 없습니다.\")\n",
        "elif not ('rag_tool' in locals() and rag_tool is not None):\n",
        "     print(\"!! [오류] RAG Tool이 준비되지 않아 에이전트를 설정할 수 없습니다.\")\n",
        "\n",
        "print(\"--- 단계 10.2: 에이전트 설정 완료 ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW9ruAEOXxUh"
      },
      "source": [
        "# 단계 11: ReAct 에이전트 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TGaR7zNX0cp",
        "outputId": "300a85f7-ca7c-429c-c5b1-cddec02769e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 단계 11 (수정): ReAct 에이전트 실행 시작 ---\n",
            "\n",
            "\n",
            "=========================================\n",
            "질문 입력: 근로자를 해고하려면 며칠 전에 예고를 해야 하나요?\n",
            "=========================================\n",
            "에이전트 실행 중...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m근로자를 해고하려면 근로기준법에 따라 일정 기간 전에 예고를 해야 합니다. 일반적으로 해고 예고 기간은 최소 30일입니다. 즉, 해고를 통보하기 최소 30일 전에 근로자에게 예고해야 하며, 그렇지 않을 경우 30일분의 평균임금을 해고 예고수당으로 지급해야 합니다.\n",
            "\n",
            "혹시 구체적인 상황이나 해고 사유에 따라 달라질 수 있으니, 더 자세한 법률 상담이 필요하시면 말씀해 주세요.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "에이전트 실행 완료 (1.34초)\n",
            "\n",
            "[에이전트 최종 답변]:\n",
            "근로자를 해고하려면 근로기준법에 따라 일정 기간 전에 예고를 해야 합니다. 일반적으로 해고 예고 기간은 최소 30일입니다. 즉, 해고를 통보하기 최소 30일 전에 근로자에게 예고해야 하며, 그렇지 않을 경우 30일분의 평균임금을 해고 예고수당으로 지급해야 합니다.\n",
            "\n",
            "혹시 구체적인 상황이나 해고 사유에 따라 달라질 수 있으니, 더 자세한 법률 상담이 필요하시면 말씀해 주세요.\n",
            "\n",
            "\n",
            "=========================================\n",
            "질문 입력: 오늘 날씨 어때?\n",
            "=========================================\n",
            "에이전트 실행 중...\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m죄송하지만, 저는 현재 실시간 날씨 정보를 제공할 수 없습니다. 최신 날씨 정보를 확인하시려면 날씨 앱이나 웹사이트를 이용하시거나, 스마트폰의 날씨 서비스를 이용해 주세요. 다른 도움이 필요하시면 말씀해 주세요!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "에이전트 실행 완료 (1.00초)\n",
            "\n",
            "[에이전트 최종 답변]:\n",
            "죄송하지만, 저는 현재 실시간 날씨 정보를 제공할 수 없습니다. 최신 날씨 정보를 확인하시려면 날씨 앱이나 웹사이트를 이용하시거나, 스마트폰의 날씨 서비스를 이용해 주세요. 다른 도움이 필요하시면 말씀해 주세요!\n",
            "\n",
            "--- 단계 11 (수정): ReAct 에이전트 실행 완료 ---\n"
          ]
        }
      ],
      "source": [
        "# === 단계 11 (수정): ReAct 에이전트 실행 ===\n",
        "\n",
        "print(\"\\n--- 단계 11 (수정): ReAct 에이전트 실행 시작 ---\")\n",
        "\n",
        "# 이전 단계에서 정의된 query 변수 사용 또는 새 질문 정의\n",
        "# query = \"근로자를 해고하려면 몇일 전에 예고를 해야 하나요?\" # 예시 질문\n",
        "test_query_1 = \"근로자를 해고하려면 며칠 전에 예고를 해야 하나요?\"\n",
        "test_query_2 = \"오늘 날씨 어때?\" # RAG 도구를 사용하지 않아야 하는 질문 예시\n",
        "\n",
        "queries_to_test = [test_query_1, test_query_2]\n",
        "agent_results = {}\n",
        "\n",
        "if agent_executor:\n",
        "    for query in queries_to_test:\n",
        "        print(f\"\\n\\n=========================================\")\n",
        "        print(f\"질문 입력: {query}\")\n",
        "        print(f\"=========================================\")\n",
        "        agent_results[query] = None # 결과 초기화\n",
        "        try:\n",
        "            print(\"에이전트 실행 중...\")\n",
        "            start_agent_time = time.time()\n",
        "\n",
        "            # 에이전트 실행!\n",
        "            response = agent_executor.invoke({\"input\": query})\n",
        "\n",
        "            end_agent_time = time.time()\n",
        "            print(f\"\\n에이전트 실행 완료 ({end_agent_time - start_agent_time:.2f}초)\")\n",
        "\n",
        "            # 최종 답변 출력\n",
        "            final_answer = response.get(\"output\", \"N/A\")\n",
        "            agent_results[query] = final_answer\n",
        "            print(\"\\n[에이전트 최종 답변]:\")\n",
        "            print(final_answer)\n",
        "\n",
        "            # (참고) verbose=True 설정 시, 실행 과정에서 Thought, Action, Observation이 출력됨\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!! [오류] 에이전트 실행 중 오류 발생: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "else:\n",
        "    print(\"!! Agent Executor가 준비되지 않았습니다. 실행 불가.\")\n",
        "\n",
        "print(\"\\n--- 단계 11 (수정): ReAct 에이전트 실행 완료 ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbgtbWRGQlOG"
      },
      "source": [
        "RAG 기반 에이전트 평가에서는 항상 \"Agent가 RAG 도구를 실제로 사용했는가?\" 를 먼저 확인\n",
        "\n",
        "✅ 1. intermediate_steps에 context가 있는지 먼저 확인\n",
        "\n",
        "✅ 2. 평가 가능한 질문만 골라서 평가\n",
        "\n",
        "[] 나오는 경우는 rag를 사용하지 않아 context가 없다는 의미"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmGpr-dniqsm",
        "outputId": "0e8c86f7-dc95-461c-f6ce-13398b258927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "pprint(response.get(\"intermediate_steps\", []), width=150)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ABXNe2uRJF5"
      },
      "source": [
        "# ✅ 회고\n",
        "\n",
        "ReAct(Reason + Act) 프레임워크 기반의 에이전트(Agent)를 구현해 보았습니다.\n",
        "LLM이 판단하여 컨텍스트 사용여부, 답변 생성을 하는 과정에서 LLM의 한계가 적용될 수 밖에 없습니다.\n",
        "\n",
        "첫 번째 테스트 질문인 \"근로자를 해고하려면 며칠 전에 예고를 해야 하나요?\"에 생성된 응답은 다음과 같습니다.\n",
        "\n",
        "(답변)\n",
        "근로자를 해고하려면 근로기준법에 따라 일정 기간 전에 예고를 해야 합니다. 일반적으로 해고 예고 기간은 최소 30일입니다. 즉, 해고를 통보하기 최소 30일 전에 근로자에게 예고해야 하며, 그렇지 않을 경우 30일분의 평균임금을 해고 예고수당으로 지급해야 합니다.\n",
        "\n",
        "혹시 구체적인 상황이나 해고 사유에 따라 달라질 수 있으니, 더 자세한 법률 상담이 필요하시면 말씀해 주세요.\n",
        "\n",
        "생성된 답변에서 '평균임금이 아니라 통상임금'으로 오류가 발생했습니다.\n",
        "\n",
        "현재의 RAG agent는 참고 문서 자체를 사용하지 않을 수도 있고, 첫 번째 테스트부터 오답이 발생하여 법률 도메인에 적합하지 않다고 판단하여 더 이상 개선하지 않았습니다. 하지만 프롬프트를 좀 더 면밀하게 수정한다면, 생성된 응답의 정확도를 향상시킬 수 있을 것으로 보입니다.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}