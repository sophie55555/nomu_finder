# -*- coding: utf-8 -*-
"""20250421.case30(GPT.pattern, text-embedding-ada-002).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15m9iFHWIM__6ZBYPaAxC2HNPS3mef1ZG

base_folder_path = "/content/drive/MyDrive/nomu_dataset3" # <<< ì›ë³¸ ë°ì´í„° í´ë” ê²½ë¡œ í™•ì¸!
    result_dir = "/content/drive/MyDrive/nomu_rag_result"    # <<< ê²°ê³¼ ì €ì¥ í´ë” ê²½ë¡œ í™•ì¸!

# 0: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (OpenAI ì¶”ê°€)
"""

# === ë‹¨ê³„ 0: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì˜ì¡´ì„± ì¶©ëŒ í•´ê²° ë²„ì „ + OpenAI ì¶”ê°€) ===
print("--- ë‹¨ê³„ 0: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì‹œì‘ (OpenAI ì¶”ê°€) ---")
!pip install -qU \
    langchain langchain-core langchain-community langchain-openai openai \
    pypdf openpyxl xlrd unstructured faiss-cpu sentence-transformers \
    pdf2image pillow pdfminer.six rank_bm25 pillow-heif jq \
    google-api-python-client google-auth-httplib2 google-auth-oauthlib gspread \
    ragas datasets \
    pandas==2.2.2 \
    PyPDF2 \
    fsspec==2025.3.2 # <<< fsspec ë²„ì „ì€ í™˜ê²½ì— ë”°ë¼ ì¡°ì • í•„ìš”, ì›ë˜ ë²„ì „ ì‚¬ìš©

# google-ai-generativelanguageëŠ” OpenAI ì‚¬ìš© ì‹œ í•„ìˆ˜ëŠ” ì•„ë‹˜ (í•„ìš”ì‹œ ìœ ì§€)
# !pip install -qU google-ai-generativelanguage==0.6.15

print("\n[ì•Œë¦¼] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜/ì—…ë°ì´íŠ¸ ì™„ë£Œ. langchain-openai, openai ì¶”ê°€ë¨.")

"""# 1: ê¸°ë³¸ ë° í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (OpenAI LLM ì„í¬íŠ¸ ì¶”ê°€)"""

# === ë‹¨ê³„ 1: ê¸°ë³¸ ë° í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ===

import os
import glob
import subprocess
import sys
import warnings
import time
import pandas as pd
from google.colab import drive, auth, userdata
import PyPDF2 # ëª…ì‹œì  ì„í¬íŠ¸
import json
import pickle
import torch
import numpy as np
from tqdm.notebook import tqdm
import gspread
from google.auth import default as google_auth_default
from datasets import Dataset
import re
import traceback # ì˜¤ë¥˜ ìƒì„¸ ì¶œë ¥ì„ ìœ„í•´ ì¶”ê°€

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain_core.documents import Document
from langchain_community.document_loaders import (
    PyPDFLoader, UnstructuredExcelLoader, CSVLoader,
    UnstructuredFileLoader, DirectoryLoader, GoogleDriveLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_google_genai import GoogleGenerativeAIEmbeddings # Google ì„ë² ë”© (í•„ìš” ì‹œ ìœ ì§€)
from langchain_community.embeddings import HuggingFaceEmbeddings # HuggingFace ì„ë² ë”© (ìœ ì§€)
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
# === LLM ì„í¬íŠ¸ ë³€ê²½ ===
# from langchain_google_genai import ChatGoogleGenerativeAI # <<< ì‚­ì œ ë˜ëŠ” ì£¼ì„ ì²˜ë¦¬
from langchain_openai import ChatOpenAI # <<< OpenAI LLM í´ë˜ìŠ¤ ì„í¬íŠ¸
# =======================
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# RAGAS ê´€ë ¨ ì„í¬íŠ¸ (ê¸°ì¡´ ìœ ì§€)
try:
    from ragas import evaluate
    from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy
    from ragas.llms import LangchainLLMWrapper
    from ragas.embeddings import LangchainEmbeddingsWrapper
except ImportError:
    print("!! Ragas ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í‰ê°€ ë‹¨ê³„ ì „ì— ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    LangchainLLMWrapper = None; LangchainEmbeddingsWrapper = None; context_precision = None; context_recall = None; faithfulness = None; answer_relevancy = None

# ê¸°íƒ€ í‰ê°€ ê´€ë ¨ ì„í¬íŠ¸ (ê¸°ì¡´ ìœ ì§€)
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
# import google.generativeai as genai # OpenAI ì‚¬ìš© ì‹œ í•„ìˆ˜ëŠ” ì•„ë‹˜

warnings.filterwarnings("ignore") # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°

print("--- ë‹¨ê³„ 1: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ (ChatOpenAI ì„í¬íŠ¸ë¨) ---")

"""# 2: í™˜ê²½ ì„¤ì • (OpenAI API í‚¤ ì¶”ê°€)

base_folder_path = "/content/drive/MyDrive/nomu_dataset3" # <<< ì›ë³¸ ë°ì´í„° í´ë” ê²½ë¡œ í™•ì¸!
    result_dir = "/content/drive/MyDrive/nomu_rag_result"    # <<< ê²°ê³¼ ì €ì¥ í´ë” ê²½ë¡œ í™•ì¸!
    # <<< nomu_dataset3 í´ë”ì˜ ì‹¤ì œ IDë¡œ ë³€ê²½í•˜ê±°ë‚˜ í™•ì¸! >>>
ì˜ˆ)
    https://drive.google.com/drive/u/0/folders/1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U

target_folder_id = "1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U"
"""

# === ë‹¨ê³„ 2: í™˜ê²½ ì„¤ì • (Drive ë§ˆìš´íŠ¸, API í‚¤, ê²½ë¡œ) ===
print("\n--- ë‹¨ê³„ 2: í™˜ê²½ ì„¤ì • ì‹œì‘ ---")

# --- Google Drive ë§ˆìš´íŠ¸ (ê¸°ì¡´ ìœ ì§€) ---
DRIVE_MOUNTED = False
try:
    drive.mount('/content/drive', force_remount=False)
    DRIVE_MOUNTED = True
    print("[ì„±ê³µ] Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ.")
except Exception as e:
    print(f"[ì‹¤íŒ¨] Google Drive ë§ˆìš´íŠ¸ ì˜¤ë¥˜: {e}")

# --- Google ì¸ì¦ (Colab ì‚¬ìš©ì ì¸ì¦ - í•„ìš”ì‹œ ìœ ì§€) ---
# Google Drive Loader ë“±ì„ ì‚¬ìš©í•œë‹¤ë©´ ì¸ì¦ ìœ ì§€ í•„ìš”
try:
    auth.authenticate_user()
    print("[ì„±ê³µ] Google Colab ì‚¬ìš©ì ì¸ì¦ ì™„ë£Œ.")
except Exception as e:
    print(f"[ì‹¤íŒ¨] Google Colab ì¸ì¦ ì˜¤ë¥˜: {e}")

# === API í‚¤ ì„¤ì • ë³€ê²½ ===
# --- OpenAI API í‚¤ ì„¤ì • ---
OPENAI_API_KEY = None
try:
    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY') # Colab Secrets ìš°ì„  í™•ì¸
    if not OPENAI_API_KEY: OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY') # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if not OPENAI_API_KEY: raise ValueError("OpenAI API í‚¤ë¥¼ Colab Secrets ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
    print("[ì„±ê³µ] OpenAI API í‚¤ ë¡œë“œ ë° ì„¤ì • ì™„ë£Œ.")
except Exception as e:
    print(f"[ì‹¤íŒ¨] OpenAI API í‚¤ ë¡œë“œ/ì„¤ì • ì˜¤ë¥˜: {e}")
    print("   !! OpenAI API í‚¤ ì—†ì´ëŠ” ì´í›„ LLM, RAGAS í‰ê°€ ë“± ì‚¬ìš© ë¶ˆê°€ !!")

# --- Google AI API í‚¤ ì„¤ì • (ì„ íƒ ì‚¬í•­) ---
# ë§Œì•½ Google Embedding ë“±ì„ ê³„ì† ì‚¬ìš©í•œë‹¤ë©´ ìœ ì§€, ì•„ë‹ˆë©´ ì œê±° ê°€ëŠ¥
# GOOGLE_API_KEY = None
# try:
#     GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
#     if not GOOGLE_API_KEY: GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')
#     if GOOGLE_API_KEY:
#         os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY
#         import google.generativeai as genai
#         genai.configure(api_key=GOOGLE_API_KEY)
#         print("[ì •ë³´] Google API í‚¤ ë¡œë“œ ë° ì„¤ì • ì™„ë£Œ (ì„ íƒ ì‚¬í•­).")
#     # else: print("[ì •ë³´] Google API í‚¤ ë¡œë“œ ì•ˆ ë¨ (ì„ íƒ ì‚¬í•­).") # í‚¤ ì—†ì–´ë„ ì˜¤ë¥˜ ì•„ë‹˜
# except Exception as e: print(f"[ê²½ê³ ] Google API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì„ íƒ ì‚¬í•­): {e}")
# =========================

# --- ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ ìœ ì§€) ---
if DRIVE_MOUNTED:
    base_folder_path = "/content/drive/MyDrive/nomu_dataset3"
    result_dir = "/content/drive/MyDrive/nomu_rag_result"
else:
    base_folder_path = "./nomu_data_local"
    result_dir = "./nomu_rag_result_local"

print(f"ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰ ê²½ë¡œ: {base_folder_path}")
print(f"ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {result_dir}")
os.makedirs(base_folder_path, exist_ok=True)
os.makedirs(result_dir, exist_ok=True)

# Google Sheets í´ë” ID (í•„ìš”ì‹œ ìœ ì§€)
target_folder_id = "1FIA_HGeYbRVaH_USuxUdjhFAoReUTo9U"
print(f"Google Sheets ê²€ìƒ‰ ëŒ€ìƒ í´ë” ID: {target_folder_id}")

# --- ê²°ê³¼ íŒŒì¼ ê²½ë¡œ (ê¸°ì¡´ ìœ ì§€) ---
vs_status_file = os.path.join(result_dir, "vectorstore_build_status.json")
vs_checkpoint_path = os.path.join(result_dir, "faiss_index_nomu_checkpoint")
vs_final_save_path = os.path.join(result_dir, "faiss_index_nomu_final")
bm25_data_save_path = os.path.join(result_dir, "split_texts_for_bm25.pkl")

print("--- ë‹¨ê³„ 2: í™˜ê²½ ì„¤ì • ì™„ë£Œ (OpenAI API í‚¤ ì„¤ì •ë¨) ---")

"""# 3: ë°ì´í„° ë¡œë”©"""

# === ë‹¨ê³„ 3: ë°ì´í„° ë¡œë”©  ===

print("\n--- ë‹¨ê³„ 3: ë°ì´í„° ë¡œë”© ì‹œì‘ ---")

loaded_documents = []
loading_errors = {}

# --- 3.1. Google Sheets ë¡œë”© ---
print("\n--- 3.1 Google Sheets íŒŒì¼ ë¡œë”© ---")
if not target_folder_id or "YOUR_" in target_folder_id:
    print("  âš ï¸ ê²½ê³ : Google Drive í´ë” IDê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ Google Sheet ë¡œë”© ê±´ë„ˆ<0xEB><0x9A><A9>ë‹ˆë‹¤.")
elif not DRIVE_MOUNTED:
     print("  âš ï¸ ê²½ê³ : Google Driveê°€ ë§ˆìš´íŠ¸ë˜ì§€ ì•Šì•„ Google Sheet ë¡œë”© ê±´ë„ˆ<0xEB><0x9A><A9>ë‹ˆë‹¤.")
else:
    try:
        gsheet_loader = GoogleDriveLoader(folder_id=target_folder_id, file_types=["sheet"], recursive=True)
        print(f"  GoogleDriveLoaderë¡œ í´ë” '{target_folder_id}' ë¡œë”© ì¤‘...")
        gsheet_docs = gsheet_loader.load()
        if gsheet_docs:
            print(f"  [ì„±ê³µ] Google Sheets ë¡œë”© ({len(gsheet_docs)}ê°œ ì¡°ê°).")
            for doc in gsheet_docs:
                doc.metadata['file_type'] = 'google_sheet'
                if 'source' not in doc.metadata and 'id' in doc.metadata:
                    doc.metadata['source'] = f"https://docs.google.com/spreadsheets/d/{doc.metadata['id']}"
            loaded_documents.extend(gsheet_docs)
        else: print("  [ì •ë³´] í•´ë‹¹ í´ë”ì— Google Sheets íŒŒì¼ ì—†ìŒ.")
    except ImportError as ie: print(f"  âŒ ì„í¬íŠ¸ ì˜¤ë¥˜: {ie}. ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í•„ìš”."); loading_errors['google_sheets'] = str(ie)
    except Exception as e: error_msg = f"{type(e).__name__}: {e}"; print(f"  âŒ ë¡œë”© ì˜¤ë¥˜: {error_msg}"); loading_errors['google_sheets'] = error_msg

# --- 3.2. ë‹¤ë¥¸ íŒŒì¼ í˜•ì‹ ë¡œë”© (PDF, Excel, CSV, TXT ë“±) ---
print("\n--- 3.2 ë‹¤ë¥¸ íŒŒì¼ í˜•ì‹ ë¡œë”© (DirectoryLoader) ---")
if not DRIVE_MOUNTED and not os.path.exists(base_folder_path):
    print(f"  âš ï¸ ê²½ê³ : Drive ë¯¸ë§ˆìš´íŠ¸ ë° ë¡œì»¬ ê²½ë¡œ({base_folder_path}) ì—†ìŒ. íŒŒì¼ ë¡œë”© ê±´ë„ˆ<0xEB><0x9A><A9>ë‹ˆë‹¤.")
else:
    LOADER_MAPPING = { ".pdf": (PyPDFLoader, {}), ".xlsx": (UnstructuredExcelLoader, {"mode": "single"}), ".xls": (UnstructuredExcelLoader, {"mode": "single"}), ".csv": (CSVLoader, {"encoding": "utf-8"}), ".txt": (UnstructuredFileLoader, {}) }
    supported_extensions = list(LOADER_MAPPING.keys())
    for ext in supported_extensions:
        print(f"\n  '{ext}' í™•ì¥ì ë¡œë”© ({base_folder_path})...")
        loader_cls, loader_args = LOADER_MAPPING[ext]
        try:
            loader = DirectoryLoader( base_folder_path, glob=f"**/*{ext}", loader_cls=loader_cls, loader_kwargs=loader_args, recursive=True, show_progress=True, use_multithreading=True, silent_errors=False )
            docs = loader.load()
            if docs:
                print(f"    [ì„±ê³µ] '{ext}' ë¡œë”© ({len(docs)}ê°œ ì¡°ê°).")
                for doc in docs: doc.metadata['file_type'] = ext.lstrip('.')
                loaded_documents.extend(docs)
            else: print(f"    [ì •ë³´] '{ext}' íŒŒì¼ ì—†ìŒ.")
        except ImportError as ie: error_msg = f"{ie}"; print(f"    âŒ ì„í¬íŠ¸ ì˜¤ë¥˜: {error_msg}"); loading_errors[f'loader_{ext}'] = f"ImportError: {error_msg}"
        except Exception as e: error_msg = f"{type(e).__name__}: {e}"; print(f"    âŒ ë¡œë”© ì˜¤ë¥˜: {error_msg}"); loading_errors[f'loader_{ext}'] = error_msg

# --- 3.3. ë¡œë”© ê²°ê³¼ ìš”ì•½ ---
print(f"\n--- ìµœì¢… ë¡œë“œëœ ë¬¸ì„œ ì¡°ê° ìˆ˜: {len(loaded_documents)} ---")
doc_type_counts = {}
for doc in loaded_documents: file_type = doc.metadata.get('file_type', 'unknown'); doc_type_counts[file_type] = doc_type_counts.get(file_type, 0) + 1
print("\n--- ë¡œë“œëœ ë¬¸ì„œ íƒ€ì…ë³„ ê°œìˆ˜ ---");
if doc_type_counts: [print(f"  - {f_type}: {count}ê°œ ì¡°ê°") for f_type, count in sorted(doc_type_counts.items())]
else: print("  ë¡œë“œëœ ë¬¸ì„œ ì—†ìŒ.")
if loading_errors: print("\n--- ë¡œë”© ì˜¤ë¥˜ ìš”ì•½ ---"); [print(f"  - {src}: {err}") for src, err in loading_errors.items()]
if loaded_documents:
    print("\n--- ì²« ë¡œë“œ ë¬¸ì„œ ìƒ˜í”Œ ---")
    try: first_doc = loaded_documents[0]; print(f"  íƒ€ì…: {first_doc.metadata.get('file_type')}\n  ë©”íƒ€ë°ì´í„°: {first_doc.metadata}\n  ë‚´ìš©(200ì): {first_doc.page_content[:200]}...")
    except Exception as e: print(f"  !! ìƒ˜í”Œ ì¶œë ¥ ì˜¤ë¥˜: {e}")
else: print("\në¡œë“œëœ ë¬¸ì„œ ì—†ìŒ. ê²½ë¡œ, íŒŒì¼ í˜•ì‹, ê¶Œí•œ í™•ì¸ í•„ìš”.")
print("\n--- ë‹¨ê³„ 3: ë°ì´í„° ë¡œë”© ì™„ë£Œ ---")

"""# 4: í…ìŠ¤íŠ¸ ë¶„í•  (Chunking, íŒ¨í„´ ê¸°ë°˜ + ê¸¸ì´ ì œí•œ)"""

# === ë‹¨ê³„ 4: í…ìŠ¤íŠ¸ ë¶„í•  (íŒ¨í„´ ê¸°ë°˜ + ê¸¸ì´ ì œí•œ) ===
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document # Document í´ë˜ìŠ¤ ì„í¬íŠ¸ í™•ì¸

print("\n--- ë‹¨ê³„ 4: í…ìŠ¤íŠ¸ ë¶„í•  ì‹œì‘ (íŒ¨í„´ ê¸°ë°˜ ì ìš©) ---")

split_texts = []
chunk_size_setting = 500
chunk_overlap_setting = 100
# ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ì‚¬ìš©í•  fallback ìŠ¤í”Œë¦¬í„°
fallback_splitter = RecursiveCharacterTextSplitter(
    chunk_size=chunk_size_setting,
    chunk_overlap=chunk_overlap_setting
)

if 'loaded_documents' in locals() and loaded_documents:
    print(f"[ì •ë³´] íŒ¨í„´ ê¸°ë°˜ ì²­í‚¹ ì‹œë„ (ë²•ë¥  PDF ëŒ€ìƒ). ê¸°ì¤€ í¬ê¸°={chunk_size_setting}, ì¤‘ì²©={chunk_overlap_setting}")
    processed_docs_count = 0
    skipped_docs_count = 0

    for doc in tqdm(loaded_documents, desc="ë¬¸ì„œ ì²­í‚¹ ì¤‘"):
        doc_content = doc.page_content
        doc_metadata = doc.metadata
        file_type = doc_metadata.get('file_type', 'unknown')

        # --- PDF ë¬¸ì„œ(ë²•ë¥  ë¬¸ì„œë¡œ ê°„ì£¼)ì—ë§Œ íŒ¨í„´ ê¸°ë°˜ ì ìš© ---
        # íŒŒì¼ íƒ€ì…ì´ 'pdf'ê°€ ì•„ë‹ˆê±°ë‚˜ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ ìŠ¤í”Œë¦¬í„° ì‚¬ìš©
        if file_type != 'pdf' or not doc_content.strip():
            if doc_content.strip(): # ë‚´ìš©ì´ ìˆì„ ë•Œë§Œ ë¶„í• 
                 try:
                     sub_chunks = fallback_splitter.split_text(doc_content)
                     for chunk_text in sub_chunks:
                         split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))
                     processed_docs_count += 1
                 except Exception as e:
                      print(f"\n!! ê¸°ë³¸ ë¶„í•  ì˜¤ë¥˜ (íƒ€ì…: {file_type}, ì†ŒìŠ¤: {doc_metadata.get('source', 'N/A')}): {e}")
                      skipped_docs_count += 1
            else:
                skipped_docs_count += 1
            continue # ë‹¤ìŒ ë¬¸ì„œë¡œ ì´ë™

        # --- íŒ¨í„´ ê¸°ë°˜ ì²­í‚¹ ë¡œì§ (PDF ëŒ€ìƒ) ---
        try:
            # 1. "ì œXì¡°" íŒ¨í„´ìœ¼ë¡œ ë¨¼ì € í¬ê²Œ ë‚˜ëˆ„ê¸°
            # ì •ê·œí‘œí˜„ì‹: 'ì œ' + (ì„ íƒì  ê³µë°±) + ìˆ«ì + (ì„ íƒì  ê³µë°±) + 'ì¡°'
            # re.splitì€ êµ¬ë¶„ìë„ ê²°ê³¼ì— í¬í•¨ì‹œí‚¤ë¯€ë¡œ, ì´ë¥¼ í™œìš©í•˜ì—¬ ì¡° ì œëª©ì„ ìœ ì§€
            preliminary_chunks_with_title = re.split(r'(ì œ\s?\d+\s?ì¡°)', doc_content)

            current_chunk_content = ""
            # ì²« ë¶€ë¶„ ì²˜ë¦¬ (ì²« 'ì œXì¡°' ì´ì „ ë‚´ìš©)
            if preliminary_chunks_with_title[0].strip():
                 current_chunk_content = preliminary_chunks_with_title[0].strip()

            # 'ì œXì¡°' ì œëª©ê³¼ ê·¸ ë‚´ìš©ì„ ë¬¶ì–´ì„œ ì²˜ë¦¬
            for i in range(1, len(preliminary_chunks_with_title), 2):
                title = preliminary_chunks_with_title[i] # ì˜ˆ: "ì œ1ì¡°"
                content = preliminary_chunks_with_title[i+1] if (i+1) < len(preliminary_chunks_with_title) else ""

                article_block = title + content # "ì œ1ì¡° ë‚´ìš©..."

                # ì´ì „ ì²­í¬ê°€ ìˆê³  + í˜„ì¬ ì¡°í•­ì„ í•©ì³ë„ í¬ê¸°ë¥¼ ë„˜ì§€ ì•Šìœ¼ë©´ í•©ì¹¨
                # (ì§§ì€ ì¡°í•­ë“¤ì´ í•©ì³ì§€ëŠ” íš¨ê³¼)
                if current_chunk_content and (len(current_chunk_content) + len(article_block) <= chunk_size_setting):
                    current_chunk_content += "\n\n" + article_block # ë¬¸ë‹¨ êµ¬ë¶„ ì¶”ê°€
                else:
                    # ì´ì „ ì²­í¬ê°€ ë„ˆë¬´ ê¸¸ì—ˆê±°ë‚˜, í•©ì¹˜ë©´ ê¸¸ì–´ì§€ëŠ” ê²½ìš°
                    # ì´ì „ ì²­í¬ë¥¼ ìµœì¢… ì²˜ë¦¬í•˜ê³  í˜„ì¬ ì¡°í•­ìœ¼ë¡œ ìƒˆ ì²­í¬ ì‹œì‘
                    if current_chunk_content: # ì´ì „ ì²­í¬ ë‚´ìš©ì´ ìˆìœ¼ë©´
                        # ì´ì „ ì²­í¬ê°€ ìµœëŒ€ í¬ê¸°ë¥¼ ë„˜ëŠ”ì§€ í™•ì¸
                        if len(current_chunk_content) > chunk_size_setting:
                            # ë„˜ìœ¼ë©´ fallback ìŠ¤í”Œë¦¬í„°ë¡œ ì¬ë¶„í• 
                            sub_chunks = fallback_splitter.split_text(current_chunk_content)
                            for chunk_text in sub_chunks:
                                split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))
                        else:
                            # ì•ˆ ë„˜ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€
                            split_texts.append(Document(page_content=current_chunk_content, metadata=doc_metadata.copy()))

                    # í˜„ì¬ ì¡°í•­ìœ¼ë¡œ ìƒˆ ì²­í¬ ì‹œì‘
                    current_chunk_content = article_block.strip() # ìƒˆ ì²­í¬ ì‹œì‘

            # ë§ˆì§€ë§‰ ë‚¨ì€ ì²­í¬ ì²˜ë¦¬
            if current_chunk_content:
                 if len(current_chunk_content) > chunk_size_setting:
                     sub_chunks = fallback_splitter.split_text(current_chunk_content)
                     for chunk_text in sub_chunks:
                         split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))
                 else:
                     split_texts.append(Document(page_content=current_chunk_content, metadata=doc_metadata.copy()))

            processed_docs_count += 1

        except Exception as e:
            print(f"\n!! íŒ¨í„´ ê¸°ë°˜ ë¶„í•  ì˜¤ë¥˜ (ì†ŒìŠ¤: {doc_metadata.get('source', 'N/A')}): {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë¬¸ì„œëŠ” ê¸°ë³¸ ìŠ¤í”Œë¦¬í„°ë¡œ ì²˜ë¦¬ ì‹œë„ (ì„ íƒ ì‚¬í•­)
            try:
                sub_chunks = fallback_splitter.split_text(doc_content)
                for chunk_text in sub_chunks:
                     split_texts.append(Document(page_content=chunk_text, metadata=doc_metadata.copy()))
                print(f"  -> ì˜¤ë¥˜ ë°œìƒí•˜ì—¬ ê¸°ë³¸ ë¶„í•  ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•¨.")
                processed_docs_count += 1
            except Exception as fallback_e:
                 print(f"  -> ê¸°ë³¸ ë¶„í•  ë°©ì‹ë„ ì‹¤íŒ¨: {fallback_e}")
                 skipped_docs_count += 1

    print(f"\n[ì„±ê³µ] ì´ {processed_docs_count}ê°œ ì›ë³¸ ì¡°ê° ì²˜ë¦¬ -> {len(split_texts)}ê°œ ì²­í¬ ë¶„í•  ì™„ë£Œ.")
    if skipped_docs_count > 0: print(f"[ê²½ê³ ] {skipped_docs_count}ê°œ ì›ë³¸ ì¡°ê° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒí•˜ì—¬ ê±´ë„ˆ<0xEB><0x9B><0x81>.")

    if split_texts:
         print("\nì²« ë¶„í•  ì²­í¬ ìƒ˜í”Œ:")
         print(f"  ë‚´ìš©(200ì): {split_texts[0].page_content[:200]}...")
         print(f"  ë©”íƒ€ë°ì´í„°: {split_texts[0].metadata}")
         # í¬ê¸° í™•ì¸ (Fallback ì ìš© í›„ì—ë„ í´ ìˆ˜ ìˆìŒ)
         oversized = [len(c.page_content) for c in split_texts if len(c.page_content) > chunk_size_setting + chunk_overlap_setting] # ì˜¤ë²„ë© ê³ ë ¤
         if oversized: print(f"\n[ê²½ê³ ] {len(oversized)}ê°œ ì²­í¬ê°€ ìµœëŒ€ í¬ê¸°({chunk_size_setting}ì)ë¥¼ ì•½ê°„ ì´ˆê³¼í•  ìˆ˜ ìˆìŒ (ìµœëŒ€ {max(oversized)}ì).")
         else: print(f"\n[ì •ë³´] ëª¨ë“  ì²­í¬ í¬ê¸° ë¹„êµì  ì •ìƒ.")
    else: print("[ì •ë³´] ìƒì„±ëœ ì²­í¬ ì—†ìŒ.")

else:
    print("!! ë¶„í• í•  ë¡œë“œëœ ë¬¸ì„œ ì—†ìŒ.")

print("--- ë‹¨ê³„ 4: í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ ---")

"""# 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • (Hugging Face: skt/kobert-base-v1)"""

!pip install -U sentence-transformers transformers huggingface-hub --quiet

pip install -U langchain-openai openai

"""# --- í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì„¤ì • (KR-BERT ê³„ì—´ ëª¨ë¸ ì„ íƒ) ---
# ë‹¤ì–‘í•œ í•œêµ­ì–´ BERT ê¸°ë°˜ ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ (ì„±ëŠ¥/ë¦¬ì†ŒìŠ¤ ê³ ë ¤í•˜ì—¬ ì„ íƒ)
# ì˜ˆì‹œ ëª¨ë¸:
# 1. 'skt/kobert-base-v1': SKT KoBERT (ë„ë¦¬ ì‚¬ìš©ë¨)
# 2. 'kykim/bert-kor-base': ê¹€ê¸°í˜„ë‹˜ BERT ëª¨ë¸
# 3. 'jhgan/ko-sbert-nli': í•œêµ­ì–´ ë¬¸ì¥ ìœ ì‚¬ë„ í•™ìŠµ ëª¨ë¸ (SBERT ê³„ì—´, ì¶”ì²œ)
# 4. 'snunlp/KR-BERT-char16424': ì„œìš¸ëŒ€ KR-BERT (ë¬¸ì ë‹¨ìœ„)
# 5. 'beomi/kcbert-base': Beomië‹˜ KcBERT
"""

# === ë‹¨ê³„ 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • (OpenAI: text-embedding-ada-002) ===
print("\n--- ë‹¨ê³„ 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì‹œì‘ (OpenAI: text-embedding-ada-002) ---")

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    # OpenAI ì„ë² ë”©ì„ ìœ„í•œ í´ë˜ìŠ¤ ì„í¬íŠ¸
    # pip install -U langchain-openai  <-- ë¨¼ì € ì´ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
    from langchain_openai import OpenAIEmbeddings
    print("[ì •ë³´] langchain_openai.OpenAIEmbeddings ì„í¬íŠ¸ í™•ì¸.")
except ImportError as e:
    print(f"!! [ì˜¤ë¥˜] í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print("   langchain-openai, openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    # í•„ìš”í•œ í´ë˜ìŠ¤ê°€ ì—†ìœ¼ë©´ ì´í›„ ì§„í–‰ ë¶ˆê°€
    OpenAIEmbeddings = None

embeddings = None # ìµœì¢… ì„ë² ë”© ê°ì²´ ë³€ìˆ˜ ì´ˆê¸°í™”

if OpenAIEmbeddings: # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì„±ê³µ ì‹œ ì§„í–‰
    try:
        # ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì§€ì •
        openai_model_name = "text-embedding-ada-002"
        print(f"[ì •ë³´] OpenAI ì„ë² ë”© ëª¨ë¸ ({openai_model_name}) ì„¤ì • ì‹œë„...")
        print("[ì •ë³´] ë‹¨ê³„ 2ì—ì„œ ì„¤ì •ëœ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        # <<< OpenAIEmbeddings í´ë˜ìŠ¤ ì‚¬ìš© (API í‚¤ ìë™ ê°ì§€) >>>
        # ë‹¨ê³„ 2ì—ì„œ os.environ["OPENAI_API_KEY"] ê°€ ì„¤ì •ë˜ì—ˆìœ¼ë¯€ë¡œ,
        # API í‚¤ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬í•  í•„ìš” ì—†ì´ ìë™ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        embeddings = OpenAIEmbeddings(
            model=openai_model_name # ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ë§Œ ì§€ì •
            # openai_api_key íŒŒë¼ë¯¸í„° ìƒëµ
        )
        # ------------------------------------------------------

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (API í‚¤ ìœ íš¨ì„± ë° í†µì‹  í™•ì¸)
        print("[ì •ë³´] ì„ë² ë”© ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ (OpenAI API í˜¸ì¶œ)...")
        _ = embeddings.embed_query("í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.")
        print(f"[ì„±ê³µ] OpenAI ì„ë² ë”© ëª¨ë¸ ({openai_model_name}) ì„¤ì • ë° í…ŒìŠ¤íŠ¸ ì™„ë£Œ.")

    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] OpenAI ì„ë² ë”© ëª¨ë¸ ({openai_model_name}) ì„¤ì • ì‹¤íŒ¨: {e}")
        print("   - ë‹¨ê³„ 2ì—ì„œ OpenAI API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.") # <<< ì˜¤ë¥˜ ë©”ì‹œì§€ ìˆ˜ì •
        print("   - OpenAI API í‚¤ ìì²´ì˜ ìœ íš¨ì„± ë° í• ë‹¹ëŸ‰(quota)ì„ í™•ì¸í•˜ì„¸ìš”.")
        print("   - ì¸í„°ë„· ì—°ê²° ìƒíƒœ ë° OpenAI ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print("   - ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬(langchain-openai, openai) ì„¤ì¹˜ ë° í˜¸í™˜ì„±ì„ í™•ì¸í•˜ì„¸ìš”.")
        embeddings = None # ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì •
else:
    print("!! í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬(OpenAIEmbeddings) ì„í¬íŠ¸ ì‹¤íŒ¨. ì„ë² ë”© ëª¨ë¸ ì„¤ì • ë¶ˆê°€.")

print("--- ë‹¨ê³„ 5: ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì™„ë£Œ ---")

# OpenAI ëª¨ë¸ì€ ì™¸ë¶€ APIë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë¡œì»¬ GPU/CPU í™•ì¸ì€ ë¶ˆí•„ìš”í•©ë‹ˆë‹¤.

"""# ë‹¨ê³„ 6: Vector Store êµ¬ì¶• (FAISS)

--- 6.2: ê²½ë¡œ ì„¤ì • ---
ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: /content/drive/MyDrive/nomu_rag_result
  - ìƒíƒœ íŒŒì¼: /content/drive/MyDrive/nomu_rag_result/vectorstore_build_status.json
  - ì²´í¬í¬ì¸íŠ¸ í´ë” (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_checkpoint
  - ìµœì¢… ì¸ë±ìŠ¤ í´ë” (FAISS): /content/drive/MyDrive/nomu_rag_result/faiss_index_nomu_final
  - BM25ìš© ë°ì´í„° íŒŒì¼: /content/drive/MyDrive/nomu_rag_result/split_texts_for_bm25.pkl
"""

# -*- coding: utf-8 -*-
# === ë‹¨ê³„ 6: Vector Store êµ¬ì¶• (FAISS) ë° BM25ìš© ë°ì´í„° ì €ì¥ ===
# ì²´í¬í¬ì¸íŒ… ë°©ì‹ì„ save_localë¡œ ë³€ê²½

import time
import os
import json
# import pickle # pickleì€ BM25 ë°ì´í„° ì €ì¥ì—ë§Œ ì‚¬ìš©
import torch
import traceback
from tqdm.auto import tqdm
# LangChain ê´€ë ¨ ì„í¬íŠ¸ í™•ì¸
if 'FAISS' not in locals() or 'Document' not in locals():
    from langchain_community.vectorstores import FAISS
    from langchain_core.documents import Document
    print("âš ï¸ FAISS ë˜ëŠ” Document í´ë˜ìŠ¤ ì¬ì„í¬íŠ¸ë¨.")
if 'HuggingFaceEmbeddings' not in locals() and 'GoogleGenerativeAIEmbeddings' not in locals():
    # ì‚¬ìš© ì¤‘ì¸ ì„ë² ë”© í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤.
    from langchain_community.embeddings import HuggingFaceEmbeddings # ì˜ˆì‹œ
    # from langchain_google_genai import GoogleGenerativeAIEmbeddings # ì˜ˆì‹œ
    print("âš ï¸ ì„ë² ë”© í´ë˜ìŠ¤ ì¬ì„í¬íŠ¸ë¨.")


print("\n--- ë‹¨ê³„ 6: Vector Store êµ¬ì¶• ë° BM25 ë°ì´í„° ì €ì¥ ì‹œì‘ ---")

# --- 6.1: ì„¤ì • ë° ì…ë ¥ ë³€ìˆ˜ í™•ì¸ ---
print("\n--- 6.1: ì„¤ì • ë° ì…ë ¥ ë³€ìˆ˜ í™•ì¸ ---")
vectorstore = None
batch_size = 500 # ë°°ì¹˜ í¬ê¸° í™•ì¸
sleep_time = 5
max_retries = 3
total_chunks = 0
start_index = 0
loaded_from_checkpoint = False

if 'split_texts' not in locals() or not isinstance(split_texts, list) or not split_texts: print("!! ì˜¤ë¥˜: 'split_texts' ì—†ìŒ. ì¤‘ë‹¨."); exit()
elif 'embeddings' not in locals() or embeddings is None: print("!! ì˜¤ë¥˜: 'embeddings' ì—†ìŒ. ì¤‘ë‹¨."); exit()
else: total_chunks = len(split_texts); print(f"âœ… ì…ë ¥ í™•ì¸: ì´ {total_chunks}ê°œ ì²­í¬ ë° ì„ë² ë”© ëª¨ë¸ í™•ì¸ë¨.")

# --- 6.2: ê²½ë¡œ ì„¤ì • ë° ë””ë ‰í† ë¦¬ ìƒì„± ---
print("\n--- 6.2: ê²½ë¡œ ì„¤ì • ---")
if 'result_dir' not in locals() or not result_dir: result_dir = "/content/drive/MyDrive/nomu_rag_result"; print(f"âš ï¸ result_dir ë³€ìˆ˜ ì—†ì–´ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •: {result_dir}"); os.makedirs(result_dir, exist_ok=True)
else: print(f"ğŸ“‚ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {result_dir}"); os.makedirs(result_dir, exist_ok=True)

vs_status_file = os.path.join(result_dir, "vectorstore_build_status.json")
# <<< ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ í´ë”ë¡œ ë³€ê²½ (save_local ì‚¬ìš©) >>>
vs_checkpoint_path = os.path.join(result_dir, "faiss_index_nomu_checkpoint") # .pkl ëŒ€ì‹  í´ë”ëª…
vs_final_save_path = os.path.join(result_dir, "faiss_index_nomu_final")
bm25_data_save_path = os.path.join(result_dir, "split_texts_for_bm25.pkl")

print(f"  - ìƒíƒœ íŒŒì¼: {vs_status_file}")
print(f"  - ì²´í¬í¬ì¸íŠ¸ í´ë” (FAISS): {vs_checkpoint_path}") # <<< ì´ë¦„ ë³€ê²½
print(f"  - ìµœì¢… ì¸ë±ìŠ¤ í´ë” (FAISS): {vs_final_save_path}")
print(f"  - BM25ìš© ë°ì´í„° íŒŒì¼: {bm25_data_save_path}")

# --- 6.3: GPU í™•ì¸ --- (ì´ì „ê³¼ ë™ì¼)
print("\n--- 6.3: ì¥ì¹˜ í™•ì¸ ---")
device = 'cuda' if torch.cuda.is_available() else 'cpu'; print(f"â„¹ï¸ ì‚¬ìš©í•  ì¥ì¹˜: {device}");
if device == 'cpu': print("âš ï¸ ê²½ê³ : GPU ì‚¬ìš© ë¶ˆê°€. ì‹œê°„ ì†Œìš” ì˜ˆìƒ.")

# --- 6.4: ì´ì „ ì‘ì—… ìƒíƒœ ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ---
if total_chunks > 0:
    print("\n--- 6.4: ì´ì „ ì‘ì—… ìƒíƒœ ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ---")
    try:
        # <<< ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ëŒ€ì‹  í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ >>>
        if os.path.exists(vs_status_file) and os.path.isdir(vs_checkpoint_path):
            with open(vs_status_file, 'r') as f_status: status_data = json.load(f_status); last_processed_index = status_data.get('last_processed_index', -1); start_index = last_processed_index + 1; print(f"  - ìƒíƒœ ë¡œë“œ ì™„ë£Œ. ë§ˆì§€ë§‰ ì¸ë±ìŠ¤: {last_processed_index}")

            # <<< pickle.load ëŒ€ì‹  FAISS.load_local ì‚¬ìš© >>>
            print(f"  - ì²´í¬í¬ì¸íŠ¸ ì¸ë±ìŠ¤ ë¡œë”©: {vs_checkpoint_path}")
            if 'embeddings' in locals() and embeddings:
                vectorstore = FAISS.load_local(
                    folder_path=vs_checkpoint_path,
                    embeddings=embeddings,
                    allow_dangerous_deserialization=True # ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†ŒìŠ¤ì¼ ë•Œë§Œ
                )
                print(f"  - FAISS ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ. {vectorstore.index.ntotal} ë²¡í„° í¬í•¨.")
                loaded_from_checkpoint = True
            else:
                print("  !! ì˜¤ë¥˜: ì„ë² ë”© í•¨ìˆ˜('embeddings')ê°€ ì—†ì–´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘.")
                vectorstore = None; start_index = 0; loaded_from_checkpoint = False

            if start_index >= total_chunks: print("  âœ… ì´ì „ FAISS ì‘ì—… ì™„ë£Œë¨.")
            elif loaded_from_checkpoint: print(f"  â–¶ï¸ {start_index}ë²ˆ ì¸ë±ìŠ¤ë¶€í„° FAISS ì‘ì—… ì¬ê°œ.")
            # else ë¸”ë¡ì€ ìœ„ì—ì„œ ì²˜ë¦¬ë¨

        else: print(f"  - ì´ì „ ìƒíƒœ íŒŒì¼ ë˜ëŠ” ì²´í¬í¬ì¸íŠ¸ í´ë” ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘."); vectorstore = None; start_index = 0
    except Exception as e: print(f"  âš ï¸ ìƒíƒœ/ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}. ì²˜ìŒë¶€í„° ì‹œì‘."); traceback.print_exc(); vectorstore = None; start_index = 0
else: print("â„¹ï¸ ì²˜ë¦¬í•  ì²­í¬ ì—†ìŒ.")

# --- 6.5: FAISS Vector Store êµ¬ì¶• ---
if total_chunks > 0 and start_index < total_chunks:
    print(f"\n--- 6.5: FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì‹œì‘ (ì´ {total_chunks} ì¤‘ {start_index}ë¶€í„°, ë°°ì¹˜ {batch_size}) ---")
    all_processed_successfully = True
    try:
        progress_bar = tqdm(range(start_index, total_chunks, batch_size), initial=start_index // batch_size, total=-(total_chunks // -batch_size), desc="FAISS êµ¬ì¶• ì¤‘")
        for i in progress_bar:
            batch_start_idx = i; batch_end_idx = min(i + batch_size, total_chunks); batch_docs = split_texts[batch_start_idx:batch_end_idx]
            for attempt in range(max_retries):
                try:
                    if i == start_index and not loaded_from_checkpoint:
                        progress_bar.set_description(f"ì²« ë°°ì¹˜({batch_start_idx}-{batch_end_idx-1}) ìƒì„± ì¤‘")
                        vectorstore = FAISS.from_documents(batch_docs, embeddings)
                        print(f"\n   - ì²« ë°°ì¹˜ FAISS ìƒì„± ì™„ë£Œ (ë²¡í„° {vectorstore.index.ntotal}ê°œ)")
                    elif vectorstore is not None:
                        progress_bar.set_description(f"ë°°ì¹˜({batch_start_idx}-{batch_end_idx-1}) ì¶”ê°€ ì¤‘")
                        vectorstore.add_documents(batch_docs)
                        print(f"\n   - ë°°ì¹˜ FAISS ì¶”ê°€ ì™„ë£Œ (ì´ ë²¡í„° {vectorstore.index.ntotal}ê°œ)")
                    else: raise ValueError("Vectorstore ê°ì²´ None ìƒíƒœ.")

                    current_processed_index = batch_end_idx - 1
                    try:
                        # <<< ì²´í¬í¬ì¸íŒ…: ìƒíƒœ ì €ì¥ í›„ save_local í˜¸ì¶œ >>>
                        with open(vs_status_file, 'w') as f_status: json.dump({'last_processed_index': current_processed_index}, f_status)
                        # ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ë®ì–´ì“°ê¸° (ì´ì „ íŒŒì¼ ì‚­ì œë¨)
                        vectorstore.save_local(vs_checkpoint_path)
                        # print(f"  ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ (í´ë”: {vs_checkpoint_path}, ì¸ë±ìŠ¤: {current_processed_index})")
                    except Exception as e_save: print(f"  âš ï¸ ì§„í–‰ ìƒí™©(ì²´í¬í¬ì¸íŠ¸) ì €ì¥ ì‹¤íŒ¨: {e_save}")
                    break # ì„±ê³µ ì‹œ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ
                except Exception as e:
                    print(f"\n!! ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}): {type(e).__name__} - {e}")
                    traceback.print_exc()
                    if attempt < max_retries - 1: wait_time = sleep_time * (attempt + 2); progress_bar.set_description(f"ì˜¤ë¥˜, {wait_time}ì´ˆ í›„ ì¬ì‹œë„..."); print(f"  ... {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ..."); time.sleep(wait_time)
                    else: print("!! ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼. êµ¬ì¶• ì¤‘ë‹¨."); all_processed_successfully = False; raise RuntimeError(f"ìµœëŒ€ ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
            if not all_processed_successfully: break
            if batch_end_idx < total_chunks: time.sleep(sleep_time) # API Rate Limit

        # --- ë£¨í”„ ì™„ë£Œ í›„ ìµœì¢… ì²˜ë¦¬ ---
        if all_processed_successfully:
            print("\n[ì„±ê³µ] ëª¨ë“  FAISS ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ.")
            # --- 6.6: ìµœì¢… ê²°ê³¼ ì €ì¥ ---
            print("\n--- 6.6: ìµœì¢… ê²°ê³¼ ì €ì¥ ---")
            # 1. ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥ (save_local)
            print(f"ğŸ’¾ ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥: {vs_final_save_path}")
            try:
                if vectorstore: vectorstore.save_local(vs_final_save_path); print(f"âœ… ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ.")
                else: print("âš ï¸ ìµœì¢… ì €ì¥ ì‹œ VectorStore ê°ì²´ ì—†ìŒ.")
            except Exception as e: print(f"âŒ ìµœì¢… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {e}"); traceback.print_exc()
            # 2. BM25ìš© ë°ì´í„° ì €ì¥ (pickle)
            print(f"ğŸ’¾ BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° ì €ì¥: {bm25_data_save_path}")
            try:
                if 'split_texts' in locals() and split_texts:
                    # <<< pickle ì„í¬íŠ¸ í™•ì¸ >>>
                    import pickle
                    with open(bm25_data_save_path, 'wb') as f_texts: pickle.dump(split_texts, f_texts); print(f"âœ… BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° ({len(split_texts)}ê°œ ì²­í¬) ì €ì¥ ì™„ë£Œ.")
                else: print("âš ï¸ BM25ìš© ë°ì´í„°('split_texts') ì—†ì–´ ì €ì¥ ë¶ˆê°€.")
            except Exception as e: print(f"âŒ split_texts ì €ì¥ ì‹¤íŒ¨: {e}"); traceback.print_exc()
            # (ì„ íƒ) ì¤‘ê°„ íŒŒì¼ ì‚­ì œ
            # try: ... (ì‚­ì œ ë¡œì§) ... except ...

    except Exception as e_main_loop: print(f"\n!! FAISS êµ¬ì¶• ì‹¤íŒ¨: {e_main_loop}"); print(f"[ì •ë³´] ë§ˆì§€ë§‰ ì„±ê³µ ì§€ì  ë°ì´í„°ëŠ” '{vs_checkpoint_path}' í´ë”ì— ìˆì„ ìˆ˜ ìˆìŒ.")

# --- ì´ì „ ì‹¤í–‰ì—ì„œ ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ì²˜ë¦¬ ---
elif total_chunks > 0 and start_index >= total_chunks:
     print("\nâœ… FAISS Vector Store êµ¬ì¶• ì‘ì—…ì´ ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤.")
     # ì™„ë£Œëœ ìƒíƒœì—ì„œë„ ìµœì¢… íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì €ì¥ ì‹œë„
     # 1. ìµœì¢… FAISS ì¸ë±ìŠ¤ í™•ì¸ ë° ì €ì¥
     if not os.path.isdir(vs_final_save_path): # <<< í´ë” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ >>>
         if vectorstore: # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œëœ vectorstoreê°€ ìˆë‹¤ë©´
             print(f"\nğŸ’¾ ìµœì¢… FAISS ì¸ë±ìŠ¤ í´ë” ì—†ì–´ ì €ì¥ ì‹œë„: '{vs_final_save_path}'") # <<< print ë¬¸ ë¶„ë¦¬
             # <<< try...except ë¸”ë¡ì„ ìƒˆ ì¤„ì—ì„œ ì‹œì‘ >>>
             try:
                 vectorstore.save_local(vs_final_save_path)
                 print("âœ… ì €ì¥ ì™„ë£Œ.")
             except Exception as e:
                 print(f"âŒ ì €ì¥ ì˜¤ë¥˜: {e}")
                 traceback.print_exc()
         else:
             print(f"âš ï¸ ìµœì¢… FAISS ì¸ë±ìŠ¤ í´ë” ì—†ê³ , ë¡œë“œëœ vectorstoreë„ ì—†ì–´ ì €ì¥ ë¶ˆê°€.")

     # 2. BM25ìš© ë°ì´í„° í™•ì¸ ë° ì €ì¥
     if not os.path.exists(bm25_data_save_path):
         if 'split_texts' in locals() and split_texts:
             print(f"\nğŸ’¾ BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼({bm25_data_save_path}) ì—†ì–´ ì €ì¥ ì‹œë„...") # <<< print ë¬¸ ë¶„ë¦¬
             # <<< try...except ë¸”ë¡ì„ ìƒˆ ì¤„ì—ì„œ ì‹œì‘ >>>
             try:
                 # <<< pickle ì„í¬íŠ¸ í™•ì¸ (í•„ìš”ì‹œ) >>>
                 import pickle
                 with open(bm25_data_save_path, 'wb') as f:
                     pickle.dump(split_texts, f)
                 print("âœ… ì €ì¥ ì™„ë£Œ.")
             except Exception as e:
                 print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
                 traceback.print_exc()
         else:
             print(f"âš ï¸ BM25ìš© ë°ì´í„° íŒŒì¼ ì—†ê³ , split_texts ë³€ìˆ˜ë„ ì—†ì–´ ì €ì¥ ë¶ˆê°€.")

print("\n--- ë‹¨ê³„ 6: Vector Store ì²˜ë¦¬ ë° BM25 ë°ì´í„° ì €ì¥ ì™„ë£Œ ---")

"""# ë‹¨ê³„ 7: ì €ì¥ëœ ì¸ë±ìŠ¤ ë° ë°ì´í„° ë¡œë“œ (ì¬ì‹¤í–‰ ì‹œ ì‚¬ìš©)

â°**ì„ë² ë”© ëª¨ë¸ì€ ë¡œë“œì— í•„ìš”í•˜ë¯€ë¡œ ë‹¤ì‹œ ì •ì˜ (ë‹¨ê³„ 5ì™€ ë™ì¼í•´ì•¼ í•¨) **

ì´ ë¸”ë¡ì€ ìµœì´ˆ ì‹¤í–‰ ì‹œì—ëŠ” ì‹¤í–‰í•  í•„ìš”ê°€ ì—†ìœ¼ë‚˜,

ë…¸íŠ¸ë¶ì„ ë‹¤ì‹œ ì—´ê³  ë‹¨ê³„ 6ê¹Œì§€ì˜ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

# === ë‹¨ê³„ 7: ì €ì¥ëœ ì¸ë±ìŠ¤ ë° ë°ì´í„° ë¡œë“œ (ì¬ì‹¤í–‰ ì‹œ ì‚¬ìš©) ===

# ì´ ë¸”ë¡ì€ ìµœì´ˆ ì‹¤í–‰ ì‹œì—ëŠ” ì‹¤í–‰í•  í•„ìš”ê°€ ì—†ìœ¼ë‚˜,
# ë…¸íŠ¸ë¶ì„ ë‹¤ì‹œ ì—´ê³  ë‹¨ê³„ 6ê¹Œì§€ì˜ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•˜ê³  ì‹¶ì„ ë•Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
print("\n--- ë‹¨ê³„ 7: ì €ì¥ëœ ì¸ë±ìŠ¤ ë° ë°ì´í„° ë¡œë“œ ì‹œë„ ---")

loaded_vectorstore = None # ë¡œë“œëœ FAISS ê°ì²´
loaded_split_texts = None # ë¡œë“œëœ BM25ìš© í…ìŠ¤íŠ¸

# ì„ë² ë”© ëª¨ë¸ì€ ë¡œë“œì— í•„ìš”í•˜ë¯€ë¡œ ë‹¤ì‹œ ì •ì˜ (ë‹¨ê³„ 5ì™€ ë™ì¼í•´ì•¼ í•¨)
print("â³ ë¡œë“œì— í•„ìš”í•œ ì„ë² ë”© ëª¨ë¸ ì¬ì„¤ì • ì¤‘...")
load_embeddings = None
if 'embeddings' in locals() and embeddings: # ì´ì „ ë‹¨ê³„ì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆë‹¤ë©´ ì¬ì‚¬ìš©
    load_embeddings = embeddings
    print("[ì •ë³´] ì´ì „ ë‹¨ê³„ì˜ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©.")
else: # ì•„ë‹ˆë¼ë©´ ìƒˆë¡œ ë¡œë“œ ì‹œë„
    try:
        hf_model_name = "snunlp/KR-BERT-char16424"
        hf_encode_kwargs = {'normalize_embeddings': True}
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        load_embeddings = HuggingFaceEmbeddings(model_name=hf_model_name, model_kwargs={'device': device}, encode_kwargs=hf_encode_kwargs)
        _ = load_embeddings.embed_query("ë¡œë“œ í…ŒìŠ¤íŠ¸")
        print(f"[ì„±ê³µ] ë¡œë“œìš© ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({hf_model_name}, ì¥ì¹˜: {device}).")
    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] ë¡œë“œìš© ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ
if load_embeddings is not None:
    if os.path.exists(vs_final_save_path):
        print(f"ğŸ’¾ ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ë¡œë”© ì¤‘: {vs_final_save_path}")
        try:
            loaded_vectorstore = FAISS.load_local(folder_path=vs_final_save_path, embeddings=load_embeddings, allow_dangerous_deserialization=True)
            print(f"âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ ({loaded_vectorstore.index.ntotal}ê°œ ë²¡í„°).")
        except Exception as e: print(f"âŒ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì˜¤ë¥˜: {e}"); print(traceback.format_exc())
    else: print(f"âŒ ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ ê²½ë¡œ ì—†ìŒ: {vs_final_save_path}")
else: print("âš ï¸ ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì•ˆ ë¨. FAISS ì¸ë±ìŠ¤ ë¡œë“œ ë¶ˆê°€.")

# BM25ìš© ë°ì´í„° ë¡œë“œ
if os.path.exists(bm25_data_save_path):
    print(f"ğŸ’¾ ì €ì¥ëœ BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘: {bm25_data_save_path}")
    try:
        with open(bm25_data_save_path, 'rb') as f: loaded_split_texts = pickle.load(f)
        print(f"âœ… BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(loaded_split_texts)}ê°œ ì²­í¬).")
    except Exception as e: print(f"âŒ BM25 ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}"); traceback.print_exc()
else: print(f"âŒ ì €ì¥ëœ BM25ìš© í…ìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ ì—†ìŒ: {bm25_data_save_path}")

print("--- ë‹¨ê³„ 7: ì €ì¥ëœ ì¸ë±ìŠ¤ ë° ë°ì´í„° ë¡œë“œ ì™„ë£Œ ---")

"""# ë‹¨ê³„ 8: Retriever ì„¤ì • (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)"""

# === ë‹¨ê³„ 8: Retriever ì„¤ì • (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰) ===

print("\n--- ë‹¨ê³„ 8: Retriever ì„¤ì • ì‹œì‘ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰) ---")

# ì‚¬ìš©í•  VectorStoreì™€ split_texts ê²°ì •
# ë¡œë“œ ì„±ê³µ ì‹œ ë¡œë“œëœ ê²ƒ ì‚¬ìš©, ì•„ë‹ˆë©´ êµ¬ì¶• ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ê²ƒ ì‚¬ìš©
final_vectorstore = None
final_split_texts = None # BM25ìš©

if 'loaded_vectorstore' in locals() and loaded_vectorstore:
    final_vectorstore = loaded_vectorstore
    print("[ì •ë³´] ë¡œë“œëœ FAISS VectorStore ì‚¬ìš©.")
elif 'vectorstore' in locals() and vectorstore:
    final_vectorstore = vectorstore
    print("[ì •ë³´] ìƒˆë¡œ ìƒì„±ëœ FAISS VectorStore ì‚¬ìš©.")
else:
    print("!! ì˜¤ë¥˜: ì‚¬ìš© ê°€ëŠ¥í•œ FAISS VectorStore ê°ì²´ ì—†ìŒ.")

if 'loaded_split_texts' in locals() and loaded_split_texts:
    final_split_texts = loaded_split_texts
    print("[ì •ë³´] ë¡œë“œëœ split_texts ë°ì´í„° ì‚¬ìš© (BM25ìš©).")
elif 'split_texts' in locals() and split_texts:
    final_split_texts = split_texts
    print("[ì •ë³´] í˜„ì¬ ì„¸ì…˜ì˜ split_texts ë°ì´í„° ì‚¬ìš© (BM25ìš©).")
else:
    print("!! ì˜¤ë¥˜: BM25ìš© split_texts ë°ì´í„° ì—†ìŒ.")

retriever = None # ìµœì¢… ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”

if final_vectorstore:
    # Dense Retriever (FAISS)
    faiss_retriever = final_vectorstore.as_retriever(search_kwargs={'k': 6})
    print(f"- Dense Retriever (FAISS) ì„¤ì • ì™„ë£Œ (k={faiss_retriever.search_kwargs.get('k')}).")

    if final_split_texts: # BM25ìš© ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ í•˜ì´ë¸Œë¦¬ë“œ ì‹œë„
        try:
            # Sparse Retriever (BM25)
            bm25_retriever = BM25Retriever.from_documents(final_split_texts)
            bm25_retriever.k = 6
            print(f"- Sparse Retriever (BM25) ì„¤ì • ì™„ë£Œ (k={bm25_retriever.k}).")
            # Ensemble Retriever
            ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.4, 0.6])
            retriever = ensemble_retriever
            print(f"- Ensemble Retriever ì„¤ì • ì™„ë£Œ (Weights: BM25=0.7, FAISS=0.3).")
        except Exception as e:
            print(f"!! BM25/Ensemble ì„¤ì • ì‹¤íŒ¨: {e}. Dense Retrieverë§Œ ì‚¬ìš©.")
            retriever = faiss_retriever # Fallback
    else:
        print("âš ï¸ BM25 ë°ì´í„° ì—†ì–´ Dense Retriever(FAISS)ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        retriever = faiss_retriever # Fallback
else:
    print("!! Vector Store ì¤€ë¹„ ì•ˆ ë¨. Retriever ì„¤ì • ë¶ˆê°€.")

print("--- ë‹¨ê³„ 8: Retriever ì„¤ì • ì™„ë£Œ ---")

"""# 9: LLM ì„¤ì • (OpenAI - gpt-4.1-nano)"""

# === ë‹¨ê³„ 9: LLM ì„¤ì • (OpenAI - gpt-4.1-nano) ===

print("\n--- ë‹¨ê³„ 9: LLM ì„¤ì • ì‹œì‘ (OpenAI) ---")
llm = None
# OpenAI API í‚¤ ì„¤ì • ì—¬ë¶€ í™•ì¸ (ë‹¨ê³„ 2ì—ì„œ ì„¤ì •ë¨)
if 'OPENAI_API_KEY' in os.environ and os.environ["OPENAI_API_KEY"]:
    try:
        # === ì‚¬ìš©í•  OpenAI ëª¨ë¸ ì§€ì • ===
        # llm_model_name = "gpt-4o" # ìµœì‹  ëª¨ë¸ (ì„±ëŠ¥ê³¼ ì†ë„ ê· í˜•)
        llm_model_name = "gpt-4.1-nano" # ë§Œì•½ ë” ì‘ê³  ë¹ ë¥¸ ëª¨ë¸ì´ í•„ìš”í•˜ë‹¤ë©´ ê³ ë ¤ (í˜„ì¬ëŠ” gpt-4oê°€ ê°€ì¥ ìœ ì‚¬)
        # llm_model_name = "gpt-3.5-turbo" # ì†ë„/ë¹„ìš© ìš°ì„  ì‹œ ê³ ë ¤
        # ==============================

        llm = ChatOpenAI(
            model=llm_model_name,
            temperature=0 # ë‹µë³€ì˜ ì°½ì˜ì„± ì¡°ì ˆ (ë‚®ì„ìˆ˜ë¡ ê²°ì •ì )
            # max_tokens=1024 # í•„ìš”ì‹œ ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜ ì œí•œ
        )
        print(f"[ì„±ê³µ] OpenAI ({llm.model_name}) LLM ë¡œë”© ì™„ë£Œ.") # .model ëŒ€ì‹  .model_name ì‚¬ìš©

    except ImportError:
         print("!! [ì˜¤ë¥˜] langchain-openai ë˜ëŠ” openai ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
         print("   ë‹¨ê³„ 0ì˜ ì„¤ì¹˜ ëª…ë ¹ì„ í™•ì¸í•˜ê³  ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] OpenAI LLM ë¡œë”© ì‹¤íŒ¨: {e}")
        traceback.print_exc() # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥
else:
    print("!! [ì˜¤ë¥˜] OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM ë¡œë“œ ë¶ˆê°€.")

print("--- ë‹¨ê³„ 9: LLM ì„¤ì • ì™„ë£Œ ---")

"""# 10: RAG Chain/íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

(((((ë¦¬íŠ¸ë¦¬ë²„ í™•ì¸í•  ë•Œë§Œ ì‹¤í–‰)))))Retriever ë°˜í™˜ê°’ í™•ì¸:
retriever ìì²´ê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ëŠ” ê²ƒì€ ì•„ë‹Œì§€ í™•ì¸
"""

if 'retriever' in locals() and retriever:
    test_query = "ê·¼ë¡œì í•´ê³  ì˜ˆê³ "
    try:
        retrieved_docs = retriever.invoke(test_query)
        print(f"\n[Retriever í…ŒìŠ¤íŠ¸ ê²°ê³¼] '{test_query}'ì— ëŒ€í•´ {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨:")
        if retrieved_docs:
             for i, doc in enumerate(retrieved_docs[:3]): # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
                 print(f"--- ë¬¸ì„œ {i+1} ---")
                 print(doc.page_content[:100] + "...")
                 print(doc.metadata)
        else:
             print(" -> ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ!")
    except Exception as e:
        print(f"!! Retriever í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
else:
    print("!! Retriever ê°ì²´ê°€ ì—†ì–´ í…ŒìŠ¤íŠ¸ ë¶ˆê°€.")

# === ë‹¨ê³„ 10: RAG Chain/íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ===
# (ì½”ë“œ ë³€ê²½ ì—†ìŒ - ë‹¨ê³„ 9ì—ì„œ ë³€ê²½ëœ llm ê°ì²´ë¥¼ ì‚¬ìš©)
print("\n--- ë‹¨ê³„ 10: RAG Chain êµ¬ì¶• ì‹œì‘ ---")
qa_chain = None
if llm and retriever: # llmì´ ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ë¡œ ì¤€ë¹„ë¨
    template = """ë‹¹ì‹ ì€ í•œêµ­ì˜ ë…¸ë¬´ ê·œì • ë° ê´€ë ¨ ë¬¸ì„œì— ê¸°ë°˜í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ìœ ìš©í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹­ì‹œì˜¤. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    ë§Œì•½ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë§Œìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ì—†ë‹¤ë©´, "
    ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…í™•íˆ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
    ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

    ì»¨í…ìŠ¤íŠ¸:
    {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€ (í•œêµ­ì–´):"""
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
    try:
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm, # ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
        )
        print("[ì„±ê³µ] RetrievalQA Chain êµ¬ì¶• ì™„ë£Œ.")
    except Exception as e: print(f"!! RAG Chain êµ¬ì¶• ì‹¤íŒ¨: {e}")
else: print("!! LLM ë˜ëŠ” Retriever ì¤€ë¹„ ì•ˆ ë¨. RAG Chain êµ¬ì¶• ë¶ˆê°€.")
print("--- ë‹¨ê³„ 10: RAG Chain êµ¬ì¶• ì™„ë£Œ ---")

"""# ë‹¨ê³„ 11: RAG ì‹œìŠ¤í…œ ì‹¤í–‰ (ì§ˆì˜-ì‘ë‹µ)

query = "ê·¼ë¡œìë¥¼ í•´ê³ í•˜ë ¤ë©´ ëª‡ì¼ ì „ì— ì˜ˆê³ ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?" # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
"""

# === ë‹¨ê³„ 11: RAG ì‹œìŠ¤í…œ ì‹¤í–‰ (ì§ˆì˜-ì‘ë‹µ) ===
# (ì½”ë“œ ë³€ê²½ ì—†ìŒ - ë‹¨ê³„ 10ì—ì„œ ìƒì„±ëœ qa_chain ì‚¬ìš©)
print("\n--- ë‹¨ê³„ 11: RAG ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹œì‘ ---")
query = None # ê²°ê³¼ ì €ì¥ìš©
result = None
if qa_chain:
    query = "ê·¼ë¡œìë¥¼ í•´ê³ í•˜ë ¤ë©´ ëª‡ì¼ ì „ì— ì˜ˆê³ ë¥¼ í•´ì•¼ í•˜ë‚˜ìš”?" # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
    print(f"\nì…ë ¥ ì§ˆë¬¸: {query}")
    try:
        print("ë‹µë³€ ìƒì„± ì¤‘..."); start_qa_time = time.time()
        # qa_chainì€ ë‚´ë¶€ì ìœ¼ë¡œ ChatOpenAIë¥¼ ì‚¬ìš©í•˜ê²Œ ë¨
        result = qa_chain.invoke({"query": query})
        end_qa_time = time.time(); print(f"ë‹µë³€ ìƒì„± ì™„ë£Œ ({end_qa_time - start_qa_time:.2f}ì´ˆ)")
        print("\n[ìµœì¢… ë‹µë³€]:"); print(result.get("result", "N/A"))
        print("\n[ì°¸ê³  ë¬¸ì„œ (Source Documents)]:")
        # ... (ë‚˜ë¨¸ì§€ ì¶œë ¥ ì½”ë“œ ë™ì¼) ...
    except Exception as e: print(f"!! RAG Chain ì‹¤í–‰ ì˜¤ë¥˜: {e}"); traceback.print_exc()
else: print("!! RAG Chain ì¤€ë¹„ ì•ˆ ë¨. ì‹¤í–‰ ë¶ˆê°€.")
print("\n--- ë‹¨ê³„ 11: RAG ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ ---")

"""# 12. RAGAS Retriever í‰ê°€

ì‚¬ìš©í•  Google Sheet íŒŒì¼ ì´ë¦„

GSHEET_FILE_NAME = "filtered_qa_dataset"

QUESTION_COLUMN = "question"

ANSWER_COLUMN = "answer"             

GROUND_TRUTHS_COLUMN = "ground_truths"

ìƒíƒœ ì €ì¥: result_dir ì•„ë˜ì˜ ragas_eval_exec_status.json (ë§ˆì§€ë§‰ ì²˜ë¦¬ ì¸ë±ìŠ¤ ì €ì¥)

ê²°ê³¼ ì €ì¥: result_dir ì•„ë˜ì˜ ragas_eval_exec_results_partial.csv (ë°°ì¹˜ë³„ ê²°ê³¼ ëˆ„ì  ì €ì¥)

ì „ì²´ ê²½ë¡œ (ì˜ˆì‹œ): /content/drive/MyDrive/nomu_rag_result/ragas_eval_exec_status.json

# [ë””ë²„ê¹…] ë‹¨ì¼ í•­ëª© RAGAS í‰ê°€ í…ŒìŠ¤íŠ¸
"""

# --- 12.5 ì½”ë“œ ìˆ˜ì • (í…ŒìŠ¤íŠ¸ìš©) ---
print("\n--- [ë””ë²„ê¹…] ë‹¨ì¼ í•­ëª© RAGAS í‰ê°€ í…ŒìŠ¤íŠ¸ ---")
if not qa_df.empty and retriever_available and llm_available and evaluate:
    # ì²« ë²ˆì§¸ ë°ì´í„° í•­ëª© ì¤€ë¹„
    test_index = 0
    test_row = qa_df.iloc[test_index]
    question_text = str(test_row.get(QUESTION_COLUMN, "")).strip()
    answer_text = str(test_row.get(ANSWER_COLUMN, "")).strip() if answer_column_exists else ""
    ground_truths_input = test_row.get(GROUND_TRUTHS_COLUMN, "[]") if ground_truths_column_exists else "[]"
    ground_truths_list = []
    retrieved_contexts = []
    single_eval_data = None

    if question_text:
        try:
            # Ground Truths íŒŒì‹±
            if ground_truths_column_exists and isinstance(ground_truths_input, str) and ground_truths_input.strip():
                try: parsed_list = json.loads(ground_truths_input); ground_truths_list = [str(item).strip() for item in parsed_list if isinstance(parsed_list, list) and item is not None and str(item).strip()] if isinstance(parsed_list, list) else [ground_truths_input.strip()]
                except: ground_truths_list = [ground_truths_input.strip()]
            elif isinstance(ground_truths_input, list): ground_truths_list = [str(item).strip() for item in ground_truths_input if item is not None and str(item).strip()]
            print(f"  [ë””ë²„ê¹…] Parsed ground_truths: {ground_truths_list}") # íŒŒì‹± ê²°ê³¼ í™•ì¸

            # Retriever ì‹¤í–‰
            retrieved_docs = retriever.invoke(question_text)
            retrieved_contexts = [doc.page_content for doc in retrieved_docs]
            print(f"  [ë””ë²„ê¹…] Retrieved contexts count: {len(retrieved_contexts)}") # ì»¨í…ìŠ¤íŠ¸ ìˆ˜ í™•ì¸

            single_eval_data = {
                "question": question_text,
                "contexts": retrieved_contexts,
                "ground_truths": ground_truths_list,
                "ground_truth": answer_text # answer ì»¬ëŸ¼ ì´ë¦„ í™•ì¸ í•„ìš”
            }

        except Exception as prep_e:
            print(f"!! ë‹¨ì¼ í•­ëª© ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {prep_e}")

        # ë‹¨ì¼ ë°ì´í„° í‰ê°€ ì‹¤í–‰
        if single_eval_data:
            try:
                print("  ë‹¨ì¼ í•­ëª© RAGAS í‰ê°€ ì‹¤í–‰ (raise_exceptions=True)...")
                single_dataset = Dataset.from_list([single_eval_data])
                metrics_to_evaluate = [context_precision, context_recall]
                if not all(metrics_to_evaluate): raise ImportError("Ragas ë©”íŠ¸ë¦­ ì„í¬íŠ¸ ì‹¤íŒ¨")

                single_result = evaluate(
                    dataset=single_dataset,
                    metrics=metrics_to_evaluate,
                    llm=ragas_llm,
                    embeddings=ragas_embeddings if embeddings_available else None,
                    raise_exceptions=True # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ê³  í‘œì‹œ
                )
                print("  [ì„±ê³µ] ë‹¨ì¼ í•­ëª© í‰ê°€ ì™„ë£Œ:")
                print(single_result)
            except Exception as single_eval_e:
                print(f"!! [ì˜¤ë¥˜] ë‹¨ì¼ í•­ëª© RAGAS í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:")
                traceback.print_exc() # ì „ì²´ ì˜¤ë¥˜ ìŠ¤íƒ ì¶œë ¥
    else:
        print("!! í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
else:
    print("!! ë‹¨ì¼ í•­ëª© í‰ê°€ ì‹¤í–‰ì— í•„ìš”í•œ ê°ì²´ ë˜ëŠ” ë°ì´í„° ë¶€ì¡±.")

# --- ì›ë˜ ë°°ì¹˜ í‰ê°€ëŠ” ì¼ë‹¨ ì£¼ì„ ì²˜ë¦¬ ---
# print("\n--- 12.5: RAGAS í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ ë° ì²´í¬í¬ì¸íŒ…) ---")
# ... (ê¸°ì¡´ ë°°ì¹˜ í‰ê°€ ì½”ë“œ) ...

"""# ë‹¨ê³„ 12: RAGAS Retriever í‰ê°€ (ì²´í¬í¬ì¸íŒ… ë° ì¬ê°œ ê¸°ëŠ¥ ì¶”ê°€)"""

# -*- coding: utf-8 -*-
# === ë‹¨ê³„ 12: RAGAS Retriever í‰ê°€ (ì²´í¬í¬ì¸íŒ…, ì¬ê°œ, ì§€ì—° ì‹œê°„ ì¶”ê°€) ===

# --- 12.0: í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì¬í™•ì¸ ---
print("\n--- ë‹¨ê³„ 12: RAGAS Retriever í‰ê°€ ì‹œì‘ (ì²´í¬í¬ì¸íŒ… ë° ì§€ì—° ì‹œê°„ ì ìš©) ---")
import os
import pandas as pd
import gspread
# google.colab ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” Colab í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
try:
    from google.colab import auth
    print("[ì •ë³´] Google Colab í™˜ê²½ ê°ì§€ë¨.")
    COLAB_ENV = True
except ImportError:
    print("[ì •ë³´] Google Colab í™˜ê²½ ì•„ë‹˜. ë¡œì»¬ ì¸ì¦ í•„ìš”.")
    COLAB_ENV = False
from google.auth import default as google_auth_default
import time
from tqdm.notebook import tqdm # Colab/Jupyter í™˜ê²½ìš© ì§„í–‰ ë°”
import numpy as np
import json
import warnings
import traceback # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥

# RAGAS ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
try:
    from datasets import Dataset
    from ragas import evaluate
    from ragas.metrics import context_precision, context_recall # í•„ìš”í•œ ë©”íŠ¸ë¦­
    from ragas.llms import LangchainLLMWrapper
    from ragas.embeddings import LangchainEmbeddingsWrapper
    print("[ì •ë³´] RAGAS ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í™•ì¸.")
    RAGAS_AVAILABLE = True
except ImportError as e:
    print(f"!! [ì˜¤ë¥˜] RAGAS ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    Dataset = None; evaluate = None; context_precision = None; context_recall = None;
    LangchainLLMWrapper = None; LangchainEmbeddingsWrapper = None
    RAGAS_AVAILABLE = False

warnings.filterwarnings("ignore") # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ì„ íƒ ì‚¬í•­)

# --- 12.1: Google ì¸ì¦ ë° gspread í´ë¼ì´ì–¸íŠ¸ í™•ì¸ ---
print("\n--- 12.1: Google ì¸ì¦ ë° gspread í´ë¼ì´ì–¸íŠ¸ í™•ì¸ ---")
gc = None
try:
    if COLAB_ENV:
        auth.authenticate_user() # Colab ì‚¬ìš©ì ì¸ì¦
    creds, _ = google_auth_default() # ê¸°ë³¸ ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    gc = gspread.authorize(creds)
    print("[ì„±ê³µ] Google ì¸ì¦ ë° gspread í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ.")
except Exception as e:
    print(f"!! [ì‹¤íŒ¨] Google ì¸ì¦/gspread ì˜¤ë¥˜: {e}. Google Sheet ë¡œë“œ ë¶ˆê°€.")

# --- 12.2: í•„ìˆ˜ ê°ì²´ í™•ì¸ ë° RAGAS ë˜í¼ ì´ˆê¸°í™” ---
# ì´ì „ì— ì •ì˜ëœ llm, retriever, embeddings ê°ì²´ê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
print("\n--- 12.2: í•„ìˆ˜ ê°ì²´ í™•ì¸ ë° RAGAS ë˜í¼ ì´ˆê¸°í™” ---")
llm_available = 'llm' in locals() and llm is not None
retriever_available = 'retriever' in locals() and retriever is not None
embeddings_available = 'embeddings' in locals() and embeddings is not None
ragas_llm = None; ragas_embeddings = None

if RAGAS_AVAILABLE: # RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ ì‹œ
    if llm_available:
        if LangchainLLMWrapper:
            try:
                model_name_display = getattr(llm, 'model', getattr(llm, 'model_name', 'N/A'))
                ragas_llm = LangchainLLMWrapper(llm)
                print(f"[ì„±ê³µ] RAGAS LLM ë˜í¼ ì´ˆê¸°í™” (ëª¨ë¸: {model_name_display})")
            except Exception as e:
                print(f"!! [ì˜¤ë¥˜] RAGAS LLM ë˜í¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"); llm_available = False
    else: print("!! [ì˜¤ë¥˜] LLM ê°ì²´('llm') ì—†ìŒ.")

    if embeddings_available:
        if LangchainEmbeddingsWrapper:
            try:
                ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)
                print("[ì„±ê³µ] RAGAS Embedding ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ.")
            except Exception as e:
                print(f"!! [ê²½ê³ ] RAGAS Embedding ë˜í¼ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}."); embeddings_available = False
    else: print("[ê²½ê³ ] Embedding ê°ì²´('embeddings') ì—†ìŒ. ì¼ë¶€ ë©”íŠ¸ë¦­ ì‚¬ìš© ë¶ˆê°€.")
else:
    print("!! RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨. RAGAS í‰ê°€ ë¶ˆê°€.")

# --- 12.3: í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (Google Sheet: filtered_qa_dataset) ---
print("\n--- 12.3: í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ---")
GSHEET_FILE_NAME = "filtered_qa_dataset" # ì‚¬ìš©í•  Google Sheet íŒŒì¼ ì´ë¦„
QUESTION_COLUMN = "question"; ANSWER_COLUMN = "answer"; GROUND_TRUTHS_COLUMN = "ground_truths"
qa_df = pd.DataFrame(); ground_truths_column_exists = False; answer_column_exists = False
if gc: # Google ì¸ì¦ ì„±ê³µ ì‹œì—ë§Œ ì‹œë„
    print(f"Google Sheet '{GSHEET_FILE_NAME}' ë¡œë“œ ì‹œë„...")
    try:
        spreadsheet = gc.open(GSHEET_FILE_NAME)
        worksheet = spreadsheet.get_worksheet(0) # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
        print("  ì›Œí¬ì‹œíŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘..."); time.sleep(1) # API í˜¸ì¶œ ì‹œê°„ ê³ ë ¤
        data = worksheet.get_all_values()
        if len(data) > 1: # í—¤ë” í¬í•¨ 2ì¤„ ì´ìƒì¼ ë•Œ
            header = data[0]
            qa_df = pd.DataFrame(data[1:], columns=header)
            print(f"  [ì„±ê³µ] Google Sheet ë¡œë“œ ì™„ë£Œ: {len(qa_df)}ê°œ í–‰"); print(f"     ì»¬ëŸ¼ ëª©ë¡: {qa_df.columns.tolist()}")
            # í•„ìˆ˜ ì»¬ëŸ¼ ë° ì˜µì…˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            if QUESTION_COLUMN not in qa_df.columns: raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ '{QUESTION_COLUMN}' ì—†ìŒ.")
            if GROUND_TRUTHS_COLUMN in qa_df.columns:
                ground_truths_column_exists = True; print(f"  [ì •ë³´] '{GROUND_TRUTHS_COLUMN}' ì»¬ëŸ¼ í™•ì¸.")
            else: print(f"  !! [ê²½ê³ ] '{GROUND_TRUTHS_COLUMN}' ì»¬ëŸ¼ ì—†ìŒ (context_recall ê³„ì‚° ë¶ˆê°€).")
            if ANSWER_COLUMN in qa_df.columns:
                answer_column_exists = True; print(f"  [ì •ë³´] '{ANSWER_COLUMN}' ì»¬ëŸ¼ í™•ì¸.")
            else: print(f"  !! [ê²½ê³ ] '{ANSWER_COLUMN}' ì»¬ëŸ¼ ì—†ìŒ (ì¼ë¶€ ë©”íŠ¸ë¦­ ì‚¬ìš© ë¶ˆê°€).")
        else: print("  !! [ì˜¤ë¥˜] Google Sheet ë°ì´í„° ì—†ìŒ (í—¤ë”ë§Œ ìˆê±°ë‚˜ ë¹„ì–´ ìˆìŒ)."); qa_df = pd.DataFrame()
    except gspread.exceptions.SpreadsheetNotFound: print(f"  !! [ì˜¤ë¥˜] Google Sheet '{GSHEET_FILE_NAME}' ì°¾ì„ ìˆ˜ ì—†ìŒ."); qa_df = pd.DataFrame()
    except Exception as e: print(f"  !! [ì˜¤ë¥˜] Google Sheet ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}"); traceback.print_exc(); qa_df = pd.DataFrame()
else: print("!! Google Sheet í´ë¼ì´ì–¸íŠ¸(gc) ì¤€ë¹„ ì•ˆ ë¨. ë°ì´í„° ë¡œë“œ ë¶ˆê°€.")

# --- 12.4: ì²´í¬í¬ì¸íŠ¸ ì„¤ì • ë° ë¡œë“œ (ë³µì›) ---
print("\n--- 12.4: ì²´í¬í¬ì¸íŠ¸ ì„¤ì • ë° ë¡œë“œ ---")

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ì´ì „ì— ì •ì˜ë˜ì—ˆë‹¤ê³  ê°€ì •)
# ì˜ˆì‹œ: result_dir = "/content/drive/MyDrive/nomu_rag_result"
result_dir_valid = False
status_file = None
partial_results_file = None
if 'result_dir' in locals() and isinstance(result_dir, str) and os.path.isdir(result_dir):
     print(f"ê²°ê³¼ ì €ì¥ ê²½ë¡œ í™•ì¸: {result_dir}")
     result_dir_valid = True
     # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
     status_file = os.path.join(result_dir, "ragas_eval_retriever_status.json")
     partial_results_file = os.path.join(result_dir, "ragas_eval_retriever_results_partial.csv")
     print(f"ìƒíƒœ íŒŒì¼ ê²½ë¡œ: {status_file}")
     print(f"ë¶€ë¶„ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ: {partial_results_file}")
else:
     print("!! [ê²½ê³ ] ê²°ê³¼ ì €ì¥ ê²½ë¡œ(result_dir)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì•„ ì²´í¬í¬ì¸íŒ… ë¶ˆê°€.")

start_index = 0 # í‰ê°€ ì‹œì‘í•  ì¸ë±ìŠ¤ ì´ˆê¸°í™”
all_results_list = [] # ì´ì „ ê²°ê³¼ ëˆ„ì ìš© ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

if result_dir_valid: # ì €ì¥ ê²½ë¡œê°€ ìœ íš¨í•  ë•Œë§Œ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„
    # ìƒíƒœ íŒŒì¼ ë¡œë“œ ì‹œë„
    if os.path.exists(status_file):
        try:
            with open(status_file, 'r', encoding='utf-8') as f:
                status_data = json.load(f)
                last_processed_index = status_data.get('last_processed_index', -1)
                # ìœ íš¨í•œ ì¸ë±ìŠ¤ì´ê³  ë°ì´í„°ì…‹ í¬ê¸°ë³´ë‹¤ ì‘ì„ ë•Œë§Œ ì¬ê°œ ì¸ë±ìŠ¤ ì„¤ì •
                if isinstance(last_processed_index, int) and -1 <= last_processed_index < len(qa_df):
                    start_index = last_processed_index + 1
                    print(f"  [ì •ë³´] ìƒíƒœ íŒŒì¼ ë¡œë“œ. ë§ˆì§€ë§‰ ì²˜ë¦¬ ì¸ë±ìŠ¤: {last_processed_index}. {start_index}ë¶€í„° ì¬ê°œ.")
                else:
                     print(f"  [ê²½ê³ ] ìƒíƒœ íŒŒì¼ì˜ ì¸ë±ìŠ¤({last_processed_index})ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ. ì²˜ìŒë¶€í„° ì‹œì‘.")
                     start_index = 0
        except (json.JSONDecodeError, TypeError, Exception) as e:
            print(f"  [ê²½ê³ ] ìƒíƒœ íŒŒì¼ ë¡œë“œ/íŒŒì‹± ì˜¤ë¥˜: {e}. ì²˜ìŒë¶€í„° ì‹œì‘.")
            start_index = 0

    # ë¶€ë¶„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹œë„ (ì¬ê°œ ì‹œì—ë§Œ í•„ìš”)
    if start_index > 0:
        if os.path.exists(partial_results_file):
            try:
                df_partial = pd.read_csv(partial_results_file, encoding='utf-8-sig')
                if not df_partial.empty:
                    # DataFrameì„ ë¦¬ìŠ¤íŠ¸[ë”•ì…”ë„ˆë¦¬] í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ all_results_listì— ì €ì¥
                    all_results_list = df_partial.to_dict('records')
                    # ë¡œë“œëœ ê²°ê³¼ ìˆ˜ì™€ ì‹œì‘ ì¸ë±ìŠ¤ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                    if len(all_results_list) != start_index:
                         print(f"  [ê²½ê³ ] ë¡œë“œëœ ê²°ê³¼ ìˆ˜({len(all_results_list)})ì™€ ìƒíƒœ íŒŒì¼ ì¸ë±ìŠ¤({start_index-1}) ë¶ˆì¼ì¹˜!")
                         print("     -> ì´ì „ ê²°ê³¼ëŠ” ìœ ì§€í•˜ì§€ë§Œ, ì ì¬ì  ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„± ìˆìŒ.")
                         # ì´ ê²½ìš°, start_indexë¥¼ ë¡œë“œëœ ê²°ê³¼ ìˆ˜ì— ë§ì¶œì§€, ìƒíƒœ íŒŒì¼ì„ ìš°ì„ í• ì§€ ê²°ì • í•„ìš”
                         # ì—¬ê¸°ì„œëŠ” ìƒíƒœ íŒŒì¼ì„ ìš°ì„ í•¨ (ìƒíƒœ íŒŒì¼ì´ ë§ˆì§€ë§‰ ì„±ê³µ ì§€ì ì„ ë” ì •í™•íˆ ë‚˜íƒ€ë‚¼ ê°€ëŠ¥ì„±)
                    else:
                         print(f"  [ì •ë³´] ì´ì „ ê²°ê³¼ {len(all_results_list)}ê°œ ë¡œë“œ ì™„ë£Œ.")
                else:
                     print("  [ì •ë³´] ë¶€ë¶„ ê²°ê³¼ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
                     start_index = 0; all_results_list = []
            except Exception as e:
                print(f"  [ê²½ê³ ] ë¶€ë¶„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}. ì´ì „ ê²°ê³¼ ì—†ì´ ì‹œì‘.")
                start_index = 0; all_results_list = []
        else:
             print("  [ê²½ê³ ] ìƒíƒœ íŒŒì¼ì€ ìˆìœ¼ë‚˜ ë¶€ë¶„ ê²°ê³¼ íŒŒì¼ ì—†ìŒ. ì²˜ìŒë¶€í„° ì‹œì‘.")
             start_index = 0; all_results_list = []
else:
    print("!! ì²´í¬í¬ì¸íŒ… ë¹„í™œì„±í™” (ì €ì¥ ê²½ë¡œ ë¬¸ì œ).")


# --- 12.5: RAGAS í‰ê°€ ë°ì´í„° ì¤€ë¹„ ---
# (ì´ì „ ì½”ë“œì™€ ë™ì¼ - ë°ì´í„° ì¤€ë¹„ ë¡œì§ì€ ì²´í¬í¬ì¸íŠ¸ì™€ ë¬´ê´€)
print("\n--- 12.5: RAGAS í‰ê°€ ë°ì´í„° ì¤€ë¹„ ---")
eval_data_list = []
if not qa_df.empty and retriever_available:
    print(f"ì´ {len(qa_df)}ê°œ ì§ˆë¬¸ì— ëŒ€í•´ Contexts ê²€ìƒ‰ ë° ë°ì´í„° í˜•ì‹ ë³€í™˜ ì¤‘...")
    for index, row in tqdm(qa_df.iterrows(), total=qa_df.shape[0], desc="RAGAS ë°ì´í„° ì¤€ë¹„"):
        question_text = row.get(QUESTION_COLUMN)
        if pd.isna(question_text) or not str(question_text).strip(): continue
        question_text = str(question_text).strip()
        answer_text = str(row.get(ANSWER_COLUMN, "")).strip() if answer_column_exists else "N/A"
        ground_truths_input = row.get(GROUND_TRUTHS_COLUMN) if ground_truths_column_exists else None
        retrieved_contexts = []
        ground_truths_list = []
        try:
            if ground_truths_column_exists and ground_truths_input is not None:
                current_gt_input = str(ground_truths_input).strip()
                if current_gt_input:
                    try:
                        if current_gt_input.startswith('[') and current_gt_input.endswith(']'):
                            parsed_list = json.loads(current_gt_input)
                            if isinstance(parsed_list, list):
                                ground_truths_list = [str(item).strip() for item in parsed_list if item is not None and str(item).strip()]
                            else: ground_truths_list = [current_gt_input]
                        else: ground_truths_list = [current_gt_input]
                    except json.JSONDecodeError: ground_truths_list = [current_gt_input]
                    except Exception as json_e: ground_truths_list = [current_gt_input]; print(f"GT íŒŒì‹± ì˜¤ë¥˜: {json_e}")

            retrieved_docs = retriever.invoke(question_text)
            retrieved_contexts = [doc.page_content for doc in retrieved_docs]
            eval_item = {"question": question_text, "contexts": retrieved_contexts, "ground_truths": ground_truths_list, "ground_truth": answer_text}
            eval_data_list.append(eval_item)
        except Exception as e:
            print(f"\n!! ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ (ì§ˆë¬¸: '{question_text}'): {e}"); traceback.print_exc()
            eval_data_list.append({"question": question_text, "contexts": [], "ground_truths": [], "ground_truth": ""})
    print(f"[ì™„ë£Œ] RAGAS í‰ê°€ ë°ì´í„° {len(eval_data_list)}ê°œ ì¤€ë¹„ ì™„ë£Œ.")
else:
    if qa_df.empty: print("!! í‰ê°€ ë°ì´í„°(qa_df)ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¡œë“œë˜ì§€ ì•Šì•„ ë°ì´í„° ì¤€ë¹„ ë¶ˆê°€.")
    if not retriever_available: print("!! Retrieverê°€ ì—†ì–´ Contextsë¥¼ ê²€ìƒ‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë°ì´í„° ì¤€ë¹„ ë¶ˆê°€.")


# --- 12.6: Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ---
print("\n--- 12.6: Hugging Face Dataset ë³€í™˜ ---")
eval_dataset = None
if eval_data_list and Dataset: # Dataset í´ë˜ìŠ¤ ë¡œë“œ ì„±ê³µ ì‹œ
    try:
        # eval_data_listì˜ ëª¨ë“  í•­ëª©ì´ RAGASì—ì„œ ìš”êµ¬í•˜ëŠ” í‚¤ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
        # ì˜ˆ: all(k in item for k in ["question", "contexts", "ground_truths"] for item in eval_data_list)
        eval_dataset = Dataset.from_list(eval_data_list)
        print("[ì„±ê³µ] Hugging Face Datasetìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ.")
        print(f"   ë°ì´í„°ì…‹ ì»¬ëŸ¼: {eval_dataset.column_names}")
        # print(f"   ë°ì´í„°ì…‹ ë¯¸ë¦¬ë³´ê¸°:\n{eval_dataset[:3]}") # ìƒ˜í”Œ í™•ì¸
    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] Dataset ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}"); traceback.print_exc(); eval_dataset = None
else: print("!! [ì •ë³´] ìƒì„±ëœ í‰ê°€ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ Dataset í´ë˜ìŠ¤ ë¡œë“œ ì‹¤íŒ¨í•˜ì—¬ ë³€í™˜ ë¶ˆê°€.")

# --- 12.7: RAGAS í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬, ì²´í¬í¬ì¸íŒ…, ì§€ì—° ì‹œê°„ ì¶”ê°€) ---
print("\n--- 12.7: RAGAS í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬, ì²´í¬í¬ì¸íŒ…, ì§€ì—° ì‹œê°„ ì¶”ê°€) ---")
evaluation_df = None # ìµœì¢… ê²°ê³¼ DataFrame ì´ˆê¸°í™”
eval_batch_size = 10   # ===> ë°°ì¹˜ í¬ê¸° ì„¤ì • (API ì œí•œ ê³ ë ¤, í•„ìš”ì‹œ 1ë¡œ) <===
# seconds_to_wait_between_batches = 65 # ===> ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) <===(í•„ìš”í•˜ë©´ ì‹¤í–‰)
# seconds_to_wait_on_error = 30    # ===> ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ë°°ì¹˜ ì „ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) <===(í•„ìš”í•˜ë©´ ì‹¤í–‰)

# í‰ê°€ ì‹¤í–‰ ì¡°ê±´ í™•ì¸
evaluation_possible = (
    RAGAS_AVAILABLE and
    llm_available and
    retriever_available and # RetrieverëŠ” ë°ì´í„° ì¤€ë¹„ì—ì„œ ì‚¬ìš©ë˜ì—ˆìœ¼ë¯€ë¡œ ì¬í™•ì¸
    eval_dataset and        # ë°ì´í„°ì…‹ ë³€í™˜ ì„±ê³µ ì—¬ë¶€
    evaluate and context_precision and context_recall and
    result_dir_valid      # ì²´í¬í¬ì¸íŒ… ê²½ë¡œ ìœ íš¨ì„± í™•ì¸
)

if evaluation_possible:
    # ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ ì •ì˜
    metrics_to_evaluate = []
    if context_precision: metrics_to_evaluate.append(context_precision)
    else: print("[ê²½ê³ ] context_precision ë©”íŠ¸ë¦­ ì‚¬ìš© ë¶ˆê°€.")
    if context_recall and ground_truths_column_exists: metrics_to_evaluate.append(context_recall)
    elif context_recall: print("[ê²½ê³ ] context_recall ë©”íŠ¸ë¦­ ì‚¬ìš© ë¶ˆê°€ (ground_truths ì»¬ëŸ¼ ì—†ìŒ).")
    else: print("[ê²½ê³ ] context_recall ë©”íŠ¸ë¦­ ìì²´ ì‚¬ìš© ë¶ˆê°€.")

    if not metrics_to_evaluate:
        print("!! [ì˜¤ë¥˜] í‰ê°€í•  ìœ íš¨í•œ RAGAS ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"í‰ê°€ ì§€í‘œ: {[m.name for m in metrics_to_evaluate]}")
        # start_index ìœ íš¨ì„± ì¬í™•ì¸ (ë°ì´í„°ì…‹ í¬ê¸°ë³´ë‹¤ í¬ë©´ ì•ˆë¨)
        if start_index >= len(eval_dataset):
            print(f"[ì •ë³´] start_index({start_index})ê°€ ë°ì´í„°ì…‹ í¬ê¸°({len(eval_dataset)})ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìœ¼ë¯€ë¡œ, ëª¨ë“  í‰ê°€ê°€ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            # ì´ ê²½ìš°, ì´ì „ ê²°ê³¼ë¥¼ ë¡œë“œí–ˆìœ¼ë¯€ë¡œ ì•„ë˜ ë¡œì§ì€ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ
        else:
            print(f"ì´ {len(eval_dataset)}ê°œ ì¤‘ {start_index}ë¶€í„° {eval_batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬...")
            overall_progress = tqdm(total=len(eval_dataset), initial=start_index, desc="ì „ì²´ í‰ê°€ ì§„í–‰ë¥ ")

            # start_indexë¶€í„° ëê¹Œì§€ eval_batch_size ê°„ê²©ìœ¼ë¡œ ë°˜ë³µ
            for i in range(start_index, len(eval_dataset), eval_batch_size):
                batch_end_index = min(i + eval_batch_size, len(eval_dataset))
                print(f"\n--- ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ì¸ë±ìŠ¤ {i} ~ {batch_end_index - 1} ---")
                # Dataset ê°ì²´ëŠ” ìŠ¬ë¼ì´ì‹± ëŒ€ì‹  select ì‚¬ìš©
                batch_dataset = eval_dataset.select(range(i, batch_end_index))

                if len(batch_dataset) == 0:
                     print("  [ì •ë³´] í˜„ì¬ ë°°ì¹˜ê°€ ë¹„ì–´ìˆì–´ ê±´ë„ˆ<0xEB><0x9B><0x81>ë‹ˆë‹¤.")
                     continue

                batch_success = False # í˜„ì¬ ë°°ì¹˜ ì„±ê³µ ì—¬ë¶€ í”Œë˜ê·¸
                try:
                    # RAGAS í‰ê°€ ì‹¤í–‰
                    print(f"  RAGAS í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ í¬ê¸°: {len(batch_dataset)})...")
                    eval_start_time = time.time()
                    batch_result = evaluate(
                        dataset=batch_dataset,
                        metrics=metrics_to_evaluate,
                        llm=ragas_llm,
                        embeddings=ragas_embeddings if embeddings_available else None,
                        raise_exceptions=False # ì˜¤ë¥˜ ì‹œ NaN ë°˜í™˜ ìœ ì§€
                    )
                    eval_end_time = time.time()
                    print(f"  RAGAS evaluate() í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„: {eval_end_time - eval_start_time:.2f}ì´ˆ")

                    # ê²°ê³¼ ì²˜ë¦¬ ë° ì €ì¥
                    if batch_result:
                        batch_result_df = batch_result.to_pandas()
                        print(f"  ë°°ì¹˜ í‰ê°€ ì™„ë£Œ. ê²°ê³¼ {len(batch_result_df)}ê°œ í–‰ ìƒì„±.")

                        # ê²°ê³¼ ëˆ„ì 
                        all_results_list.extend(batch_result_df.to_dict('records'))

                        # ë¶€ë¶„ ê²°ê³¼ë¥¼ CSVì— ì¶”ê°€ (í—¤ë”ëŠ” íŒŒì¼ ì—†ì„ ë•Œë§Œ ì“°ê¸°)
                        is_first_write = not os.path.exists(partial_results_file) if partial_results_file else True
                        if partial_results_file:
                            try:
                                batch_result_df.to_csv(partial_results_file, mode='a', header=is_first_write, index=False, encoding='utf-8-sig')
                                print(f"  ë¶€ë¶„ ê²°ê³¼ë¥¼ '{os.path.basename(partial_results_file)}'ì— ì¶”ê°€ ì €ì¥ ì™„ë£Œ.")
                            except Exception as csv_e: print(f"!! ë¶€ë¶„ ê²°ê³¼ CSV ì €ì¥ ì˜¤ë¥˜: {csv_e}")

                        # ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸ (í˜„ì¬ ë°°ì¹˜ì˜ ë§ˆì§€ë§‰ ì¸ë±ìŠ¤ë¡œ)
                        last_processed_in_batch = batch_end_index - 1
                        if status_file:
                             try:
                                 with open(status_file, 'w', encoding='utf-8') as f: json.dump({'last_processed_index': last_processed_in_batch}, f)
                                 print(f"  ìƒíƒœ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ (ë§ˆì§€ë§‰ ì²˜ë¦¬ ì¸ë±ìŠ¤: {last_processed_in_batch}).")
                             except Exception as status_save_e: print(f"!! ìƒíƒœ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {status_save_e}")

                        # tqdm ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        overall_progress.update(len(batch_dataset))
                        batch_success = True # ë°°ì¹˜ ì„±ê³µ

                    else:
                        print("!! ë°°ì¹˜ í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                        overall_progress.update(len(batch_dataset)) # ê²°ê³¼ ì—†ì–´ë„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸

                except Exception as e:
                    print(f"!! [ì˜¤ë¥˜] ë°°ì¹˜ {i}~{batch_end_index-1} RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                    print("   ë‹¤ìŒ ë°°ì¹˜ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. API í• ë‹¹ëŸ‰, ë„¤íŠ¸ì›Œí¬, ë°ì´í„° í˜•ì‹ ë“±ì„ í™•ì¸í•˜ì„¸ìš”.")
                    traceback.print_exc()
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì§„í–‰ ìƒí™©ì€ ê¸°ë¡ (ì˜¤ë¥˜ ë‚œ ë°°ì¹˜ëŠ” ê²°ê³¼ì— í¬í•¨ ì•ˆ ë¨)
                    last_processed_before_error = i - 1 # ì˜¤ë¥˜ ë‚œ ë°°ì¹˜ì˜ ì‹œì‘ ì´ì „ ì¸ë±ìŠ¤
                    if last_processed_before_error >= 0 and status_file:
                        try:
                            with open(status_file, 'w', encoding='utf-8') as f: json.dump({'last_processed_index': last_processed_before_error}, f)
                            print(f"  (ì˜¤ë¥˜ ë°œìƒ ì „ ë§ˆì§€ë§‰ ì²˜ë¦¬ ì¸ë±ìŠ¤: {last_processed_before_error}ë¡œ ìƒíƒœ ì €ì¥)")
                        except Exception as status_save_e: print(f"!! ìƒíƒœ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {status_save_e}")
                    overall_progress.update(len(batch_dataset)) # ì˜¤ë¥˜ë‚œ ë°°ì¹˜ë„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸

                # ===> ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•´ ëŒ€ê¸° (ì •ìƒ/ì˜¤ë¥˜ ìƒí™© êµ¬ë¶„) <===(ë°°ì¹˜ ê´€ë ¨ ì½”ë“œ - í•„ìš”í•˜ë©´ ì‹¤í–‰)
                # # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹ ê²½ìš°ì—ë§Œ ëŒ€ê¸°
                # if batch_end_index < len(eval_dataset):
                #     wait_time = seconds_to_wait_between_batches if batch_success else seconds_to_wait_on_error
                #     wait_reason = "ì •ìƒ ì²˜ë¦¬ ì™„ë£Œ" if batch_success else "ì˜¤ë¥˜ ë°œìƒ"
                #     print(f"\n  {wait_reason}. ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•´ {wait_time}ì´ˆ ëŒ€ê¸°...")
                #     time.sleep(wait_time)

            # ë£¨í”„ ì™„ë£Œ í›„ tqdm ë‹«ê¸°
            overall_progress.close()

            # ìµœì¢… DataFrame ìƒì„±
            if all_results_list:
                try:
                    evaluation_df = pd.DataFrame(all_results_list)
                    print(f"\n[ì„±ê³µ] ì „ì²´ {len(evaluation_df)}ê°œ í•­ëª©ì— ëŒ€í•œ RAGAS í‰ê°€ ê²°ê³¼ í†µí•© ì™„ë£Œ.")
                    # ìµœì¢… ê²°ê³¼ íŒŒì¼ ì €ì¥ (ì„ íƒ ì‚¬í•­)
                    if result_dir_valid:
                        final_results_file = os.path.join(result_dir, "ragas_eval_retriever_results_final.csv")
                        try:
                            evaluation_df.to_csv(final_results_file, index=False, encoding='utf-8-sig')
                            print(f"  ìµœì¢… ê²°ê³¼ê°€ '{os.path.basename(final_results_file)}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        except Exception as final_csv_e:
                             print(f"!! ìµœì¢… ê²°ê³¼ CSV ì €ì¥ ì˜¤ë¥˜: {final_csv_e}")
                except Exception as df_e:
                    print(f"!! ìµœì¢… DataFrame ìƒì„± ì¤‘ ì˜¤ë¥˜: {df_e}")
            else:
                print("\n[ì •ë³´] ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

else:
    # í‰ê°€ ì‹¤í–‰ ë¶ˆê°€ ì‚¬ìœ  ìš”ì•½
    missing_components = []
    if not RAGAS_AVAILABLE: missing_components.append("RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬")
    if not llm_available: missing_components.append("LLM (ë˜ëŠ” RAGAS ë˜í¼)")
    if not retriever_available: missing_components.append("Retriever") # ì¶”ê°€
    if not eval_dataset: missing_components.append("í‰ê°€ ë°ì´í„°ì…‹")
    if not (context_precision or context_recall): missing_components.append("RAGAS ë©”íŠ¸ë¦­")
    if not result_dir_valid: missing_components.append("ìœ íš¨í•œ ê²°ê³¼ ì €ì¥ ê²½ë¡œ(result_dir)")
    print(f"!! {', '.join(missing_components)} ì¤€ë¹„ ì•ˆ ë¨. RAGAS í‰ê°€ ë¶ˆê°€.")

# --- 12.8: ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½ ---
# (ì¶œë ¥ ë°©ì‹ ìˆ˜ì •: ì „ì²´ ê²°ê³¼ í‘œì‹œ)
print("\n--- 12.8: RAGAS Retriever ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½ ---")
if evaluation_df is not None and not evaluation_df.empty:
    metrics_to_summarize = []
    if context_precision and 'context_precision' in evaluation_df.columns: metrics_to_summarize.append('context_precision')
    if context_recall and 'context_recall' in evaluation_df.columns: metrics_to_summarize.append('context_recall')

    if metrics_to_summarize:
        # === ìˆ˜ì •ëœ ë¶€ë¶„: ì „ì²´ ê²°ê³¼ ì¶œë ¥ ===
        print("\n=== ì „ì²´ í‰ê°€ ê²°ê³¼ ===") # <<< í—¤ë” í…ìŠ¤íŠ¸ ë³€ê²½
        with pd.option_context('display.max_rows', None, # ëª¨ë“  í–‰ ì¶œë ¥
                               'display.max_columns', None, # ëª¨ë“  ì—´ ì¶œë ¥
                               'display.width', 1000):       # ë„ˆë¹„ ì„¤ì • (í•„ìš”ì‹œ ì¡°ì •)
             # <<< .head() ì œê±°í•˜ì—¬ ì „ì²´ DataFrame ì¶œë ¥ >>>
             print(evaluation_df[metrics_to_summarize].round(4).to_string(na_rep='NaN'))
        # ==================================

        print("\n=== ì§€í‘œë³„ í†µê³„ ===") # <<< ì´ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
        stats = evaluation_df[metrics_to_summarize].describe().round(4)
        print(stats)

        nan_counts = evaluation_df[metrics_to_summarize].isna().sum() # <<< ì´ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
        if nan_counts.sum() > 0:
             print("\n[ì°¸ê³ ] NaN ë°œìƒ í˜„í™©:")
             print(nan_counts[nan_counts > 0])
        else: print("\n[ì •ë³´] ëª¨ë“  ìœ íš¨ í•­ëª©ì—ì„œ ì ìˆ˜ê°€ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.")

    else: print("\n!! evaluation_dfì— ìš”ì•½í•  ìœ íš¨í•œ ë©”íŠ¸ë¦­ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    print(f"\n**ì°¸ê³ :**") # <<< ì´ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
    print(f"  - 'context_recall': Google Sheet '{GROUND_TRUTHS_COLUMN}' ì»¬ëŸ¼ ê¸°ì¤€.")
    print(f"  - 'context_precision': '{QUESTION_COLUMN}', '{ANSWER_COLUMN}'(ìˆì„ ê²½ìš°) ì»¬ëŸ¼ ì°¸ì¡° ê°€ëŠ¥.")
else: print("RAGAS í‰ê°€ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìš”ì•½í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# <<< ë§ˆì§€ë§‰ printë¬¸ì˜ ì„¤ëª…ë„ í˜„ì¬ ìƒíƒœì— ë§ê²Œ ìœ ì§€ ë˜ëŠ” ìˆ˜ì • >>>
print("\n--- ë‹¨ê³„ 12: Retriever ì„±ëŠ¥ í‰ê°€ (RAGAS) ì™„ë£Œ (ì²´í¬í¬ì¸íŒ… ë° ì‹œê°„ ì§€ì—° ì ìš©) ---")

# # --- 12.8: ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½ ---(ìƒìœ„ 5ê°œ ìƒ˜í”Œë§Œ í™•ì¸í•  ë•Œ)
# # (ì´ì „ ì½”ë“œì™€ ê±°ì˜ ë™ì¼ - ë³€ìˆ˜ëª…/ë¡œê·¸ ì•½ê°„ ìˆ˜ì •)
# print("\n--- 12.8: RAGAS Retriever ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½ ---")
# if evaluation_df is not None and not evaluation_df.empty:
#     metrics_to_summarize = []
#     if context_precision and 'context_precision' in evaluation_df.columns: metrics_to_summarize.append('context_precision')
#     if context_recall and 'context_recall' in evaluation_df.columns: metrics_to_summarize.append('context_recall')

#     if metrics_to_summarize:
#         print("\n=== ì „ì²´ ê²°ê³¼ (ìƒìœ„ 5ê°œ ìƒ˜í”Œ) ===")
#         with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
#              print(evaluation_df[metrics_to_summarize].round(4).head().to_string(na_rep='NaN'))

#         print("\n=== ì§€í‘œë³„ í†µê³„ ===")
#         stats = evaluation_df[metrics_to_summarize].describe().round(4)
#         print(stats)

#         nan_counts = evaluation_df[metrics_to_summarize].isna().sum()
#         if nan_counts.sum() > 0:
#              print("\n[ì°¸ê³ ] NaN ë°œìƒ í˜„í™©:")
#              print(nan_counts[nan_counts > 0])
#         else: print("\n[ì •ë³´] ëª¨ë“  ìœ íš¨ í•­ëª©ì—ì„œ ì ìˆ˜ê°€ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.")

#     else: print("\n!! evaluation_dfì— ìš”ì•½í•  ìœ íš¨í•œ ë©”íŠ¸ë¦­ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

#     print(f"\n**ì°¸ê³ :**")
#     print(f"  - 'context_recall': Google Sheet '{GROUND_TRUTHS_COLUMN}' ì»¬ëŸ¼ ê¸°ì¤€.")
#     print(f"  - 'context_precision': '{QUESTION_COLUMN}', '{ANSWER_COLUMN}'(ìˆì„ ê²½ìš°) ì»¬ëŸ¼ ì°¸ì¡° ê°€ëŠ¥.")
# else: print("RAGAS í‰ê°€ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìš”ì•½í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# print("\n--- ë‹¨ê³„ 12: Retriever ì„±ëŠ¥ í‰ê°€ (RAGAS) ì™„ë£Œ (ì²´í¬í¬ì¸íŒ… ë° ì‹œê°„ ì§€ì—° ì ìš©) ---")

"""# (ì˜µì…˜)12.9: ì €ì¥ëœ RAGAS í‰ê°€ ê²°ê³¼ ì „ì²´ ë³´ê¸°"""

# === ë‹¨ê³„ 12.9: ì €ì¥ëœ RAGAS í‰ê°€ ê²°ê³¼ ì „ì²´ ë³´ê¸° ===

import pandas as pd
import os
import numpy as np # NaN ê°’ ì²˜ë¦¬ ìœ„í•´ ì„í¬íŠ¸

print("\n--- ë‹¨ê³„ 12.9: ì €ì¥ëœ RAGAS í‰ê°€ ê²°ê³¼ ë¡œë“œ ë° ì „ì²´ ë³´ê¸° ---")

# --- ê²°ê³¼ íŒŒì¼ ê²½ë¡œ í™•ì¸ ---
# ì´ì „ ë‹¨ê³„ì—ì„œ ì •ì˜ëœ result_dir ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
# ê·¸ë¦¬ê³  ì €ì¥ëœ íŒŒì¼ ì´ë¦„ (final ë˜ëŠ” partial)ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
result_dir_valid = False
evaluation_df = None # ê²°ê³¼ë¥¼ ë‹´ì„ DataFrame ì´ˆê¸°í™”
results_file_path = None # ì‹¤ì œë¡œ ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ

if 'result_dir' in locals() and isinstance(result_dir, str) and os.path.isdir(result_dir):
    print(f"ê²°ê³¼ ì €ì¥ ê²½ë¡œ í™•ì¸: {result_dir}")
    result_dir_valid = True
    # ìµœì¢… ê²°ê³¼ íŒŒì¼ê³¼ ë¶€ë¶„ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ ì •ì˜
    final_results_file = os.path.join(result_dir, "ragas_eval_retriever_results_final.csv")
    partial_results_file = os.path.join(result_dir, "ragas_eval_retriever_results_partial.csv")

    # ìµœì¢… ê²°ê³¼ íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ë¡œë“œ, ì—†ìœ¼ë©´ ë¶€ë¶„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹œë„
    if os.path.exists(final_results_file):
        results_file_path = final_results_file
        print(f"ìµœì¢… ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹œë„: {os.path.basename(results_file_path)}")
    elif os.path.exists(partial_results_file):
        results_file_path = partial_results_file
        print(f"ë¶€ë¶„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹œë„: {os.path.basename(results_file_path)}")
    else:
        print(f"!! [ì˜¤ë¥˜] ê²°ê³¼ ë””ë ‰í† ë¦¬({result_dir})ì—ì„œ ìµœì¢… ë˜ëŠ” ë¶€ë¶„ ê²°ê³¼ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

else:
    print("!! [ê²½ê³ ] ê²°ê³¼ ì €ì¥ ê²½ë¡œ(result_dir)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# --- ê²°ê³¼ íŒŒì¼ ë¡œë“œ ë° ì²˜ë¦¬ ---
if results_file_path:
    try:
        evaluation_df = pd.read_csv(results_file_path, encoding='utf-8-sig') # BOM ë¬¸ì œ ë°©ì§€ ìœ„í•´ utf-8-sig ì‚¬ìš©
        print(f"[ì„±ê³µ] '{os.path.basename(results_file_path)}' íŒŒì¼ ë¡œë“œ ì™„ë£Œ. ì´ {len(evaluation_df)}ê°œ í–‰.")

        # DataFrameì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
        if evaluation_df.empty:
            print("!! [ì •ë³´] ë¡œë“œëœ í‰ê°€ ê²°ê³¼ íŒŒì¼ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
            evaluation_df = None # ë¹„ì–´ ìˆìœ¼ë©´ None ì²˜ë¦¬

    except FileNotFoundError:
        print(f"!! [ì˜¤ë¥˜] íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_file_path}")
        evaluation_df = None
    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] CSV íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        evaluation_df = None

# --- ë¡œë“œëœ ê²°ê³¼ ì¶œë ¥ ë° ìš”ì•½ (ë‹¨ê³„ 12.8 ì½”ë“œ ì¬ì‚¬ìš© ë° ìˆ˜ì •) ---
if evaluation_df is not None and not evaluation_df.empty:
    # ì¶œë ¥í•  ë©”íŠ¸ë¦­ ì»¬ëŸ¼ í™•ì¸
    metrics_to_summarize = []
    # í•„ìš”í•œ ë©”íŠ¸ë¦­ ì„í¬íŠ¸ ì—¬ë¶€ë„ í™•ì¸ (ì´ì „ ì…€ì—ì„œ ë¡œë“œ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŒ)
    context_precision_available = 'context_precision' in locals() and context_precision is not None
    context_recall_available = 'context_recall' in locals() and context_recall is not None

    if context_precision_available and 'context_precision' in evaluation_df.columns: metrics_to_summarize.append('context_precision')
    if context_recall_available and 'context_recall' in evaluation_df.columns: metrics_to_summarize.append('context_recall')

    if metrics_to_summarize:
        print(f"\n=== ì „ì²´ í‰ê°€ ê²°ê³¼ ({', '.join(metrics_to_summarize)}) ===")
        with pd.option_context('display.max_rows', None, # ëª¨ë“  í–‰ ì¶œë ¥
                               'display.max_columns', None, # ëª¨ë“  ì—´ ì¶œë ¥
                               'display.width', 1000):       # ë„ˆë¹„ ì„¤ì •
             # ì „ì²´ DataFrameì˜ í•´ë‹¹ ì»¬ëŸ¼ ì¶œë ¥
             print(evaluation_df[metrics_to_summarize].round(4).to_string(na_rep='NaN'))

        print("\n=== ì§€í‘œë³„ í†µê³„ ===")
        stats = evaluation_df[metrics_to_summarize].describe().round(4)
        print(stats)

        nan_counts = evaluation_df[metrics_to_summarize].isna().sum()
        if nan_counts.sum() > 0:
             print("\n[ì°¸ê³ ] NaN ë°œìƒ í˜„í™©:")
             print(nan_counts[nan_counts > 0])
        else: print("\n[ì •ë³´] ëª¨ë“  ìœ íš¨ í•­ëª©ì—ì„œ ì ìˆ˜ê°€ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.")

    else:
        print("\n!! ë¡œë“œëœ DataFrameì— ìš”ì•½í•  ìœ íš¨í•œ RAGAS ë©”íŠ¸ë¦­ ì»¬ëŸ¼('context_precision', 'context_recall')ì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {evaluation_df.columns.tolist()}")

    # ì°¸ê³  ì •ë³´ëŠ” í•„ìš”í•˜ë©´ ìœ ì§€
    # print(f"\n**ì°¸ê³ :**")
    # print(f"  - 'context_recall': Google Sheet '{GROUND_TRUTHS_COLUMN}' ì»¬ëŸ¼ ê¸°ì¤€.")
    # print(f"  - 'context_precision': '{QUESTION_COLUMN}', '{ANSWER_COLUMN}'(ìˆì„ ê²½ìš°) ì»¬ëŸ¼ ì°¸ì¡° ê°€ëŠ¥.")

else:
    print("\nì €ì¥ëœ í‰ê°€ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆê±°ë‚˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆì–´ ê²°ê³¼ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

print("\n--- ë‹¨ê³„ 12.9: ì €ì¥ëœ RAGAS í‰ê°€ ê²°ê³¼ ë³´ê¸° ì™„ë£Œ ---")

"""# (ì˜µì…˜)*****ì²´í¬í¬ì¸íŠ¸ëŠ” ì—†ì§€ë§Œ ì—ëŸ¬ì—†ì´ ì˜ ì‹¤í–‰ë˜ëŠ” RAGAS Retriever í‰ê°€ ì½”ë“œì„."""

# -*- coding: utf-8 -*-
# === ë‹¨ê³„ 12: RAGAS Retriever í‰ê°€ (filtered_qa_dataset ì‚¬ìš© + ì‹œê°„ ì§€ì—° ì¶”ê°€) ===

# ê¸°ëŠ¥:
# 1. Google Sheetì—ì„œ í‰ê°€ ë°ì´í„°ì…‹(question, answer, ground_truths) ë¡œë“œ
# 2. ê° ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ Retrieverë¥¼ ì‹¤í–‰í•˜ì—¬ Contexts ê²€ìƒ‰
# 3. RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Context Precision ë° Context Recall í‰ê°€ (ë°°ì¹˜ ì²˜ë¦¬ ë° ì§€ì—° ì‹œê°„ ì ìš©)
# 4. ê°œë³„ ì§ˆë¬¸ë³„ ì ìˆ˜ ë° ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥

# --- 12.0: í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì¬í™•ì¸ ---
print("\n--- ë‹¨ê³„ 12: RAGAS Retriever í‰ê°€ ì‹œì‘ (ì‹œê°„ ì§€ì—° ì ìš©) ---") # ë¡œê·¸ ìˆ˜ì •
import os
import pandas as pd
import gspread
# google.colab ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” Colab í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
# ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ë‹¤ë¥¸ ì¸ì¦ ë°©ì‹ì„ ì‚¬ìš©í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
try:
    from google.colab import auth
    print("[ì •ë³´] Google Colab í™˜ê²½ ê°ì§€ë¨.")
    COLAB_ENV = True
except ImportError:
    print("[ì •ë³´] Google Colab í™˜ê²½ ì•„ë‹˜. ë¡œì»¬ ì¸ì¦ í•„ìš”.")
    COLAB_ENV = False
from google.auth import default as google_auth_default
import time
from tqdm.notebook import tqdm # Colab/Jupyter í™˜ê²½ìš© ì§„í–‰ ë°”
import numpy as np
import json
import warnings
import traceback # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥

# RAGAS ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë©”ì‹œì§€ ì¶œë ¥)
try:
    from datasets import Dataset
    from ragas import evaluate
    from ragas.metrics import context_precision, context_recall
    from ragas.llms import LangchainLLMWrapper
    from ragas.embeddings import LangchainEmbeddingsWrapper
    print("[ì •ë³´] RAGAS ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í™•ì¸.")
    RAGAS_AVAILABLE = True
except ImportError as e:
    print(f"!! [ì˜¤ë¥˜] RAGAS ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    Dataset = None; evaluate = None; context_precision = None; context_recall = None;
    LangchainLLMWrapper = None; LangchainEmbeddingsWrapper = None
    RAGAS_AVAILABLE = False

warnings.filterwarnings("ignore") # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ì„ íƒ ì‚¬í•­)

# --- 12.1: Google ì¸ì¦ ë° gspread í´ë¼ì´ì–¸íŠ¸ í™•ì¸ ---
print("\n--- 12.1: Google ì¸ì¦ ë° gspread í´ë¼ì´ì–¸íŠ¸ í™•ì¸ ---")
gc = None # gspread í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    if COLAB_ENV:
        auth.authenticate_user() # Colab ì‚¬ìš©ì ì¸ì¦
    creds, _ = google_auth_default() # ê¸°ë³¸ ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    gc = gspread.authorize(creds)
    print("[ì„±ê³µ] Google ì¸ì¦ ë° gspread í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ.")
except Exception as e:
    print(f"!! [ì‹¤íŒ¨] Google ì¸ì¦ ë˜ëŠ” gspread í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}.")

# --- 12.2: í•„ìˆ˜ ê°ì²´ í™•ì¸ ë° RAGAS ë˜í¼ ì´ˆê¸°í™” ---
print("\n--- 12.2: í•„ìˆ˜ ê°ì²´ í™•ì¸ ë° RAGAS ë˜í¼ ì´ˆê¸°í™” ---")

# ì´ì „ì— ì •ì˜ëœ llm, retriever, embeddings ê°ì²´ê°€ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.
llm_available = 'llm' in locals() and llm is not None
retriever_available = 'retriever' in locals() and retriever is not None
embeddings_available = 'embeddings' in locals() and embeddings is not None
ragas_llm = None
ragas_embeddings = None

if RAGAS_AVAILABLE: # RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì„í¬íŠ¸ë˜ì—ˆì„ ë•Œë§Œ
    if llm_available:
        if LangchainLLMWrapper:
            try:
                model_name_display = getattr(llm, 'model', getattr(llm, 'model_name', 'N/A'))
                ragas_llm = LangchainLLMWrapper(llm) # í‚¤ì›Œë“œ ì¸ì ì—†ì´ ê°ì²´ë§Œ ì „ë‹¬
                print(f"[ì„±ê³µ] RAGAS LLM ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {model_name_display})")
            except Exception as e:
                print(f"!! [ì˜¤ë¥˜] RAGAS LLM ë˜í¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"); llm_available = False
    else: print("!! [ì˜¤ë¥˜] LLM ê°ì²´('llm')ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    if embeddings_available:
        if LangchainEmbeddingsWrapper:
            try:
                ragas_embeddings = LangchainEmbeddingsWrapper(embeddings) # í‚¤ì›Œë“œ ì¸ì ì—†ì´ ê°ì²´ë§Œ ì „ë‹¬
                print("[ì„±ê³µ] RAGAS Embedding ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ.")
            except Exception as e:
                print(f"!! [ê²½ê³ ] RAGAS Embedding ë˜í¼ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}."); embeddings_available = False
    else: print("[ê²½ê³ ] Embedding ê°ì²´('embeddings') ì—†ìŒ. ì¼ë¶€ ë©”íŠ¸ë¦­ ì‚¬ìš© ë¶ˆê°€.")
else:
    print("!! RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨. RAGAS í‰ê°€ ë¶ˆê°€.")

# --- 12.3: í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (Google Sheet: filtered_qa_dataset) ---
# (ì´ì „ ì½”ë“œì™€ ë™ì¼ - ë³€ê²½ ì—†ìŒ)
print("\n--- 12.3: í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ---")
GSHEET_FILE_NAME = "filtered_qa_dataset"
QUESTION_COLUMN = "question"; ANSWER_COLUMN = "answer"; GROUND_TRUTHS_COLUMN = "ground_truths"
qa_df = pd.DataFrame(); ground_truths_column_exists = False; answer_column_exists = False
if gc:
    print(f"Google Sheet '{GSHEET_FILE_NAME}' ë¡œë“œ ì‹œë„...")
    try:
        spreadsheet = gc.open(GSHEET_FILE_NAME); worksheet = spreadsheet.get_worksheet(0)
        print("  ì›Œí¬ì‹œíŠ¸ ë¡œë“œ ì¤‘..."); data = worksheet.get_all_values()
        if len(data) > 1:
            qa_df = pd.DataFrame(data[1:], columns=data[0])
            print(f"  [ì„±ê³µ] Google Sheet ë¡œë“œ ì™„ë£Œ: {len(qa_df)}ê°œ í–‰"); print(f"     ì»¬ëŸ¼ ëª©ë¡: {qa_df.columns.tolist()}")
            if QUESTION_COLUMN not in qa_df.columns: raise ValueError(f"'{QUESTION_COLUMN}' ì»¬ëŸ¼ì´ ì‹œíŠ¸ì— ì—†ìŠµë‹ˆë‹¤.")
            if GROUND_TRUTHS_COLUMN in qa_df.columns: ground_truths_column_exists = True
            else: print(f"  !! [ê²½ê³ ] '{GROUND_TRUTHS_COLUMN}' ì»¬ëŸ¼ ì—†ìŒ. context_recall í‰ê°€ ê²°ê³¼ëŠ” 0ì´ ë©ë‹ˆë‹¤.")
            if ANSWER_COLUMN in qa_df.columns: answer_column_exists = True
            else: print(f"  !! [ê²½ê³ ] '{ANSWER_COLUMN}' ì»¬ëŸ¼ ì—†ìŒ. context_precision ë“± ì¼ë¶€ ë©”íŠ¸ë¦­ ê²°ê³¼ì— ì˜í–¥ ê°€ëŠ¥.")
        else: print("  !! [ì˜¤ë¥˜] Google Sheetì— í—¤ë” ì™¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."); qa_df = pd.DataFrame()
    except gspread.exceptions.SpreadsheetNotFound: print(f"  !! [ì˜¤ë¥˜] Google Sheet '{GSHEET_FILE_NAME}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."); qa_df = pd.DataFrame()
    except Exception as e: print(f"  !! [ì˜¤ë¥˜] Google Sheet ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}"); traceback.print_exc(); qa_df = pd.DataFrame()
else: print("!! Google Sheet í´ë¼ì´ì–¸íŠ¸(gc)ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ë°ì´í„°ì…‹ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- 12.4: RAGAS í‰ê°€ ë°ì´í„° ì¤€ë¹„ ---
# (ì´ì „ ì½”ë“œì™€ ë™ì¼ - ë³€ê²½ ì—†ìŒ, Ground Truths íŒŒì‹± ë¡œì§ í¬í•¨)
print("\n--- 12.4: RAGAS í‰ê°€ ë°ì´í„° ì¤€ë¹„ ---")
eval_data_list = []
if not qa_df.empty and retriever_available:
    print(f"ì´ {len(qa_df)}ê°œ ì§ˆë¬¸ì— ëŒ€í•´ Contexts ê²€ìƒ‰ ë° ë°ì´í„° í˜•ì‹ ë³€í™˜ ì¤‘...")
    for index, row in tqdm(qa_df.iterrows(), total=qa_df.shape[0], desc="RAGAS ë°ì´í„° ì¤€ë¹„"):
        question_text = row.get(QUESTION_COLUMN)
        if pd.isna(question_text) or not str(question_text).strip(): continue
        question_text = str(question_text).strip()
        answer_text = str(row.get(ANSWER_COLUMN, "")).strip() if answer_column_exists else ""
        ground_truths_input = row.get(GROUND_TRUTHS_COLUMN, "[]") if ground_truths_column_exists else "[]"
        retrieved_contexts = []
        ground_truths_list = []
        try:
            if ground_truths_column_exists and isinstance(ground_truths_input, str) and ground_truths_input.strip():
                try:
                    parsed_list = json.loads(ground_truths_input)
                    if isinstance(parsed_list, list):
                        ground_truths_list = [str(item).strip() for item in parsed_list if item is not None and str(item).strip()]
                    else: ground_truths_list = [ground_truths_input.strip()]
                except (json.JSONDecodeError, TypeError): ground_truths_list = [ground_truths_input.strip()]
            elif isinstance(ground_truths_input, list): ground_truths_list = [str(item).strip() for item in ground_truths_input if item is not None and str(item).strip()]

            retrieved_docs = retriever.invoke(question_text)
            retrieved_contexts = [doc.page_content for doc in retrieved_docs]
            eval_item = {"question": question_text, "contexts": retrieved_contexts, "ground_truths": ground_truths_list, "ground_truth": answer_text}
            eval_data_list.append(eval_item)
        except Exception as e:
            print(f"\n!! ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ (ì§ˆë¬¸: '{question_text}'): {e}")
            traceback.print_exc()
            eval_data_list.append({"question": question_text, "contexts": [], "ground_truths": [], "ground_truth": ""})
    print(f"[ì™„ë£Œ] RAGAS í‰ê°€ ë°ì´í„° {len(eval_data_list)}ê°œ ì¤€ë¹„ ì™„ë£Œ.")
else:
    if qa_df.empty: print("!! í‰ê°€ ë°ì´í„°(qa_df)ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¡œë“œë˜ì§€ ì•Šì•„ ë°ì´í„° ì¤€ë¹„ ë¶ˆê°€.")
    if not retriever_available: print("!! Retrieverê°€ ì—†ì–´ Contextsë¥¼ ê²€ìƒ‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë°ì´í„° ì¤€ë¹„ ë¶ˆê°€.")

# --- 12.5: Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ---
# (ì´ì „ ì½”ë“œì™€ ë™ì¼ - ë³€ê²½ ì—†ìŒ)
print("\n--- 12.5: Hugging Face Dataset ë³€í™˜ ---")
eval_dataset = None
if eval_data_list and Dataset: # Dataset í´ë˜ìŠ¤ ë¡œë“œ ì„±ê³µ ì‹œ
    try:
        eval_dataset = Dataset.from_list(eval_data_list)
        print("[ì„±ê³µ] Hugging Face Datasetìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ.")
        print(f"   ë°ì´í„°ì…‹ ì»¬ëŸ¼: {eval_dataset.column_names}")
        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸ (ì„ íƒ ì‚¬í•­)
        # num_samples_to_show = min(3, len(eval_dataset))
        # print(f"\n   ë°ì´í„°ì…‹ ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ {num_samples_to_show}ê°œ):")
        # for i in range(num_samples_to_show): print(f"   --- í•­ëª© #{i} ---\n{eval_dataset[i]}\n")
    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] Dataset ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}"); traceback.print_exc(); eval_dataset = None
else: print("!! [ì •ë³´] ìƒì„±ëœ í‰ê°€ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ Dataset í´ë˜ìŠ¤ ë¡œë“œ ì‹¤íŒ¨í•˜ì—¬ ë³€í™˜ ë¶ˆê°€.")

# --- 12.6: RAGAS í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ ë° ì‹œê°„ ì§€ì—° ì¶”ê°€) ---
print("\n--- 12.6: RAGAS í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ ì²˜ë¦¬ ë° ì‹œê°„ ì§€ì—° ì¶”ê°€) ---")
evaluation_df = None # í‰ê°€ ê²°ê³¼ ì €ì¥ DataFrame ì´ˆê¸°í™”
all_results_list = [] # ë°°ì¹˜ ê²°ê³¼ ëˆ„ì ìš© ë¦¬ìŠ¤íŠ¸
eval_batch_size = 3   # ===> ë°°ì¹˜ í¬ê¸° ì„¤ì • (API ì œí•œ ê³ ë ¤, í•„ìš”ì‹œ 1ë¡œ) <===
seconds_to_wait_between_batches = 30 # ===> ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) <===
seconds_to_wait_on_error = 30    # ===> ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ë°°ì¹˜ ì „ ëŒ€ê¸° ì‹œê°„ (ì´ˆ) <===

# í‰ê°€ ì‹¤í–‰ ì¡°ê±´ í™•ì¸
evaluation_possible = (
    RAGAS_AVAILABLE and # RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ ì—¬ë¶€
    llm_available and   # LLM ë˜í¼ ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
    eval_dataset and    # ë°ì´í„°ì…‹ ë³€í™˜ ì„±ê³µ ì—¬ë¶€
    evaluate and context_precision and context_recall # Ragas í•¨ìˆ˜/ë©”íŠ¸ë¦­ í™•ì¸
)

if evaluation_possible:
    # ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ ì •ì˜
    metrics_to_evaluate = []
    if context_precision: metrics_to_evaluate.append(context_precision)
    else: print("[ê²½ê³ ] context_precision ë©”íŠ¸ë¦­ ì‚¬ìš© ë¶ˆê°€.")
    if context_recall and ground_truths_column_exists: metrics_to_evaluate.append(context_recall)
    elif context_recall: print("[ê²½ê³ ] context_recall ë©”íŠ¸ë¦­ ì‚¬ìš© ë¶ˆê°€ (ground_truths ì»¬ëŸ¼ ì—†ìŒ).")
    else: print("[ê²½ê³ ] context_recall ë©”íŠ¸ë¦­ ìì²´ ì‚¬ìš© ë¶ˆê°€.")

    if not metrics_to_evaluate:
        print("!! [ì˜¤ë¥˜] í‰ê°€í•  ìœ íš¨í•œ RAGAS ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"í‰ê°€ ì§€í‘œ: {[m.name for m in metrics_to_evaluate]}")
        print(f"ì´ {len(eval_dataset)}ê°œ í•­ëª©ì„ {eval_batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬...")

        # tqdmìœ¼ë¡œ ì „ì²´ ì§„í–‰ë¥  í‘œì‹œ (ë°ì´í„°ì…‹ í¬ê¸° ê¸°ì¤€)
        # ì—¬ê¸°ì„œëŠ” ë°°ì¹˜ ë£¨í”„ ë‚´ì—ì„œ ì—…ë°ì´íŠ¸í•˜ëŠ” ëŒ€ì‹  ì „ì²´ ë£¨í”„ ê¸°ì¤€ìœ¼ë¡œ ìƒì„±
        overall_progress = tqdm(total=len(eval_dataset), desc="ì „ì²´ í‰ê°€ ì§„í–‰ë¥ ")

        # ì „ì²´ ë°ì´í„°ì…‹ì„ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        for i in range(0, len(eval_dataset), eval_batch_size):
            batch_end_index = min(i + eval_batch_size, len(eval_dataset))
            print(f"\n--- ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: ì¸ë±ìŠ¤ {i} ~ {batch_end_index - 1} ---")
            batch_dataset = eval_dataset.select(range(i, batch_end_index)) # í˜„ì¬ ë°°ì¹˜ ìŠ¬ë¼ì´ì‹±

            if len(batch_dataset) == 0:
                 print("  [ì •ë³´] í˜„ì¬ ë°°ì¹˜ê°€ ë¹„ì–´ìˆì–´ ê±´ë„ˆ<0xEB><0x9B><0x81>ë‹ˆë‹¤.")
                 continue

            try:
                # RAGAS í‰ê°€ ì‹¤í–‰
                print(f"  RAGAS í‰ê°€ ì‹¤í–‰ (ë°°ì¹˜ í¬ê¸°: {len(batch_dataset)})...")
                eval_start_time = time.time()
                batch_result = evaluate(
                    dataset=batch_dataset,           # í˜„ì¬ ë°°ì¹˜ ë°ì´í„°ì…‹
                    metrics=metrics_to_evaluate,    # í‰ê°€ ì§€í‘œ ë¦¬ìŠ¤íŠ¸
                    llm=ragas_llm,                  # Ragas LLM ë˜í¼
                    embeddings=ragas_embeddings if embeddings_available else None,
                    # raise_exceptions=False ê¸°ë³¸ê°’ ì‚¬ìš© (ê°œë³„ ì˜¤ë¥˜ ì‹œ NaN)
                    # batch_size ì¸ìëŠ” evaluate í•¨ìˆ˜ì— ì§ì ‘ ì—†ìŒ (ë°ì´í„°ì…‹ í¬ê¸°ë¡œ ê²°ì •ë¨)
                )
                eval_end_time = time.time()
                print(f"  RAGAS evaluate() í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„: {eval_end_time - eval_start_time:.2f}ì´ˆ")

                # ê²°ê³¼ ì²˜ë¦¬
                if batch_result:
                    batch_result_df = batch_result.to_pandas()
                    print(f"  ë°°ì¹˜ í‰ê°€ ì™„ë£Œ. ê²°ê³¼ {len(batch_result_df)}ê°œ í–‰ ìƒì„±.")
                    all_results_list.extend(batch_result_df.to_dict('records')) # ê²°ê³¼ ëˆ„ì 
                    overall_progress.update(len(batch_dataset)) # ì„±ê³µ ì‹œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                else:
                    print("!! ë°°ì¹˜ í‰ê°€ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    # ê²°ê³¼ê°€ ì—†ì–´ë„ ì²˜ë¦¬í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    overall_progress.update(len(batch_dataset))

                # ===> ì •ìƒ ì²˜ë¦¬ í›„, ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•´ ì§€ì •ëœ ì‹œê°„ë§Œí¼ ëŒ€ê¸° <===
                # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹ ê²½ìš°ì—ë§Œ ëŒ€ê¸°
                if batch_end_index < len(eval_dataset):
                    print(f"\n  ì •ìƒ ì²˜ë¦¬ ì™„ë£Œ. API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•´ {seconds_to_wait_between_batches}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(seconds_to_wait_between_batches)

            except Exception as e:
                print(f"!! [ì˜¤ë¥˜] ë°°ì¹˜ {i}~{batch_end_index-1} RAGAS í‰ê°€ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print("   ë‹¤ìŒ ë°°ì¹˜ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. API í• ë‹¹ëŸ‰, ë„¤íŠ¸ì›Œí¬, ë°ì´í„° í˜•ì‹ ë“±ì„ í™•ì¸í•˜ì„¸ìš”.")
                traceback.print_exc()
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ í•´ë‹¹ ë°°ì¹˜ë§Œí¼ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ê±´ë„ˆë›´ ê²ƒìœ¼ë¡œ ì²˜ë¦¬)
                overall_progress.update(len(batch_dataset))
                # ===> ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•´ ì ì‹œ ëŒ€ê¸° <===
                # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹ ê²½ìš°ì—ë§Œ ëŒ€ê¸°
                if batch_end_index < len(eval_dataset):
                     print(f"  ì˜¤ë¥˜ ë°œìƒ. ë‹¤ìŒ ë°°ì¹˜ë¥¼ ìœ„í•´ {seconds_to_wait_on_error}ì´ˆ ëŒ€ê¸°...")
                     time.sleep(seconds_to_wait_on_error)

        # ë£¨í”„ ì™„ë£Œ í›„ tqdm ë‹«ê¸°
        overall_progress.close()

        # ìµœì¢… DataFrame ìƒì„±
        if all_results_list:
            try:
                evaluation_df = pd.DataFrame(all_results_list)
                print(f"\n[ì„±ê³µ] ì „ì²´ {len(evaluation_df)}ê°œ í•­ëª©ì— ëŒ€í•œ RAGAS í‰ê°€ ê²°ê³¼ í†µí•© ì™„ë£Œ.")
                 # ìµœì¢… ê²°ê³¼ íŒŒì¼ ì €ì¥ (ì„ íƒ ì‚¬í•­, result_dir ë³€ìˆ˜ í•„ìš”)
                 # if 'result_dir' in locals() and result_dir and os.path.isdir(result_dir):
                 #     final_results_file = os.path.join(result_dir, "ragas_eval_retriever_results_final.csv")
                 #     evaluation_df.to_csv(final_results_file, index=False, encoding='utf-8-sig')
                 #     print(f"  ìµœì¢… ê²°ê³¼ê°€ '{os.path.basename(final_results_file)}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as df_e:
                 print(f"!! ìµœì¢… DataFrame ìƒì„± ì¤‘ ì˜¤ë¥˜: {df_e}")
        else:
            print("\n[ì •ë³´] ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

else:
    # í‰ê°€ ì‹¤í–‰ ë¶ˆê°€ ì‚¬ìœ  ìš”ì•½
    missing_components = []
    if not RAGAS_AVAILABLE: missing_components.append("RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬")
    if not llm_available: missing_components.append("LLM (ë˜ëŠ” RAGAS ë˜í¼)")
    if not eval_dataset: missing_components.append("í‰ê°€ ë°ì´í„°ì…‹")
    if not (context_precision or context_recall): missing_components.append("RAGAS ë©”íŠ¸ë¦­")
    print(f"!! {', '.join(missing_components)} ì¤€ë¹„ ì•ˆ ë¨. RAGAS í‰ê°€ ë¶ˆê°€.")

# --- 12.7: ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½ ---
# (ì´ì „ ì½”ë“œì™€ ë™ì¼ - ë³€ê²½ ì—†ìŒ)
print("\n--- 12.7: RAGAS Retriever ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½ ---")
if evaluation_df is not None and not evaluation_df.empty:
    existing_metrics = [m.name for m in metrics_to_evaluate if m and m.name in evaluation_df.columns]
    if existing_metrics:
        print("\n=== ì „ì²´ ê²°ê³¼ (ìš”ì•½) ===")
        with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
             print(evaluation_df[existing_metrics].round(4).to_string(na_rep='NaN'))
        print("\n=== ì§€í‘œë³„ í†µê³„ ===")
        stats = evaluation_df[existing_metrics].describe().round(4)
        print(stats)
        nan_counts = evaluation_df[existing_metrics].isna().sum()
        if nan_counts.sum() > 0:
             print("\n[ì°¸ê³ ] NaN ë°œìƒ í˜„í™©:")
             print(nan_counts[nan_counts > 0])
        else: print("\n[ì •ë³´] ëª¨ë“  í•­ëª©ì—ì„œ ìœ íš¨í•œ ì ìˆ˜ê°€ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else: print("\n!! evaluation_dfì— ìš”ì•½í•  ìœ íš¨í•œ ë©”íŠ¸ë¦­ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    print(f"\n**ì°¸ê³ :**")
    print(f"  - 'context_recall': Google Sheet '{GROUND_TRUTHS_COLUMN}' ì»¬ëŸ¼ ê¸°ì¤€.")
    print(f"  - 'context_precision': '{QUESTION_COLUMN}', '{ANSWER_COLUMN}'(ìˆì„ ê²½ìš°) ì»¬ëŸ¼ ì°¸ì¡° ê°€ëŠ¥.")
else: print("RAGAS í‰ê°€ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìš”ì•½í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

print("\n--- ë‹¨ê³„ 12: Retriever ì„±ëŠ¥ í‰ê°€ (RAGAS) ì™„ë£Œ (ì‹œê°„ ì§€ì—° ì ìš©) ---") # ë¡œê·¸ ìˆ˜ì •

# === ë‹¨ê³„ 12: RAGAS Retriever í‰ê°€ (filtered_qa_dataset ì‚¬ìš©) ===

# ê¸°ëŠ¥:
# 1. Google Sheetì—ì„œ í‰ê°€ ë°ì´í„°ì…‹(question, answer, ground_truths) ë¡œë“œ
# 2. ê° ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ Retrieverë¥¼ ì‹¤í–‰í•˜ì—¬ Contexts ê²€ìƒ‰
# 3. RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Context Precision ë° Context Recall í‰ê°€
# 4. ê°œë³„ ì§ˆë¬¸ë³„ ì ìˆ˜ ë° ì „ì²´ í‰ê·  ì ìˆ˜ ì¶œë ¥

# --- 12.0: í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì¬í™•ì¸ ---
print("\n--- ë‹¨ê³„ 12: RAGAS Retriever í‰ê°€ ì‹œì‘ ---")
import os
import pandas as pd
import gspread
from google.colab import auth
from google.auth import default as google_auth_default
import time
from tqdm.notebook import tqdm
import numpy as np
import json
import warnings
import traceback # ìƒì„¸ ì˜¤ë¥˜ ì¶œë ¥

# RAGAS ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë©”ì‹œì§€ ì¶œë ¥)
try:
    from datasets import Dataset
    from ragas import evaluate
    from ragas.metrics import context_precision, context_recall
    from ragas.llms import LangchainLLMWrapper
    from ragas.embeddings import LangchainEmbeddingsWrapper
    print("[ì •ë³´] RAGAS ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í™•ì¸.")
except ImportError as e:
    print(f"!! [ì˜¤ë¥˜] RAGAS ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print("   ì´ì „ ë‹¨ê³„ì—ì„œ `!pip install ragas datasets` ë“±ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    # í‰ê°€ ì§„í–‰ ë¶ˆê°€ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•´ None ì„¤ì • (ì‹¤ì œë¡œëŠ” ì˜¤ë¥˜ ë°œìƒ)
    Dataset = None; evaluate = None; context_precision = None; context_recall = None;
    LangchainLLMWrapper = None; LangchainEmbeddingsWrapper = None

warnings.filterwarnings("ignore") # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ì„ íƒ ì‚¬í•­)

# --- 12.1: Google ì¸ì¦ ë° gspread í´ë¼ì´ì–¸íŠ¸ í™•ì¸ ---
print("\n--- 12.1: Google ì¸ì¦ ë° gspread í´ë¼ì´ì–¸íŠ¸ í™•ì¸ ---")
gc = None # gspread í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    # Colab í™˜ê²½ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì´ì „ ì…€ì—ì„œ ì¸ì¦ ì™„ë£Œë¨
    creds, _ = google_auth_default()
    gc = gspread.authorize(creds)
    print("[ì„±ê³µ] Google ì¸ì¦ ë° gspread í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ.")
except Exception as e:
    print(f"!! [ì‹¤íŒ¨] Google ì¸ì¦ ë˜ëŠ” gspread í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}.")
    print("   ì´ì „ ë‹¨ê³„ì˜ Google ì¸ì¦ ì…€ì´ ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

# --- 12.2: í•„ìˆ˜ ê°ì²´ í™•ì¸ ë° RAGAS ë˜í¼ ì´ˆê¸°í™” ---
print("\n--- 12.2: í•„ìˆ˜ ê°ì²´ í™•ì¸ ë° RAGAS ë˜í¼ ì´ˆê¸°í™” ---")

# ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ LLM, Retriever, Embeddings ê°ì²´ í™•ì¸
llm_available = 'llm' in locals() and llm is not None
retriever_available = 'retriever' in locals() and retriever is not None
embeddings_available = 'embeddings' in locals() and embeddings is not None

ragas_llm = None
ragas_embeddings = None

# LLM ë˜í¼ ì´ˆê¸°í™”
if llm_available:
    if LangchainLLMWrapper: # RAGAS ì„í¬íŠ¸ ì„±ê³µ ì‹œ
        try:
            # <<< ìˆ˜ì •: í‚¤ì›Œë“œ ì¸ì 'llm=' ì œê±° >>>
            ragas_llm = LangchainLLMWrapper(llm)
            # -------------------------------------
            print(f"[ì„±ê³µ] RAGAS LLM ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {llm.model})")
        except Exception as e:
            print(f"!! [ì˜¤ë¥˜] RAGAS LLM ë˜í¼ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            llm_available = False # ë˜í¼ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ í‰ê°€ ë¶ˆê°€
    else:
        print("!! Ragas LLM ë˜í¼ í´ë˜ìŠ¤ ì„í¬íŠ¸ ì‹¤íŒ¨.")
        llm_available = False
else:
    print("!! [ì˜¤ë¥˜] LLM ê°ì²´('llm')ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# (ì´í•˜ Retriever ë° Embedding ë˜í¼ ì´ˆê¸°í™” ë¡œì§ì€ ë™ì¼)
if retriever_available: print("[ì •ë³´] Retriever ê°ì²´('retriever') í™•ì¸ ì™„ë£Œ.")
else: print("!! [ì˜¤ë¥˜] Retriever ê°ì²´('retriever') ì—†ìŒ.")

if embeddings_available:
    if LangchainEmbeddingsWrapper:
        try:
            # <<< ìˆ˜ì •: í‚¤ì›Œë“œ ì¸ì 'embeddings=' ì œê±° >>>
            ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)
            # -----------------------------------------
            print("[ì„±ê³µ] RAGAS Embedding ë˜í¼ ì´ˆê¸°í™” ì™„ë£Œ.")
        except Exception as e:
            print(f"!! [ê²½ê³ ] RAGAS Embedding ë˜í¼ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}.")
            embeddings_available = False
    else:
        print("!! Ragas Embedding ë˜í¼ í´ë˜ìŠ¤ ì„í¬íŠ¸ ì‹¤íŒ¨.")
        embeddings_available = False
else:
    print("[ê²½ê³ ] Embedding ê°ì²´('embeddings') ì—†ìŒ. ì¼ë¶€ Ragas ë©”íŠ¸ë¦­ ì‚¬ìš© ë¶ˆê°€.")

# --- 12.3: í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (Google Sheet: filtered_qa_dataset) ---
print("\n--- 12.3: í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ---")
# ì‚¬ìš©í•  Google Sheet íŒŒì¼ ì´ë¦„
GSHEET_FILE_NAME = "filtered_qa_dataset"
# ì‹œíŠ¸ ë‚´ ì»¬ëŸ¼ ì´ë¦„ ì •ì˜ (ì‹¤ì œ ì‹œíŠ¸ í—¤ë”ì™€ ì¼ì¹˜í•´ì•¼ í•¨)
QUESTION_COLUMN = "question"
ANSWER_COLUMN = "answer"             # context_precision ë“±ì—ì„œ ì‚¬ìš©ë  ìˆ˜ ìˆìŒ
GROUND_TRUTHS_COLUMN = "ground_truths" # context_recall í‰ê°€ì˜ í•µì‹¬

qa_df = pd.DataFrame() # ë°ì´í„° ë¡œë“œìš© DataFrame ì´ˆê¸°í™”
ground_truths_column_exists = False
answer_column_exists = False

# Google Sheet í´ë¼ì´ì–¸íŠ¸ê°€ ì¤€ë¹„ë˜ì—ˆì„ ë•Œë§Œ ë¡œë“œ ì‹œë„
if gc:
    print(f"Google Sheet '{GSHEET_FILE_NAME}' ë¡œë“œ ì‹œë„...")
    try:
        spreadsheet = gc.open(GSHEET_FILE_NAME)
        worksheet = spreadsheet.get_worksheet(0) # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš© ê°€ì •
        print("  ì›Œí¬ì‹œíŠ¸ ë¡œë“œ ì¤‘...")
        data = worksheet.get_all_values() # ì‹œíŠ¸ ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        if len(data) > 1: # í—¤ë” í¬í•¨ ìµœì†Œ 2ì¤„ ì´ìƒ
            qa_df = pd.DataFrame(data[1:], columns=data[0]) # DataFrame ìƒì„±
            print(f"  [ì„±ê³µ] Google Sheet ë¡œë“œ ì™„ë£Œ: {len(qa_df)}ê°œ í–‰")
            print(f"     ì»¬ëŸ¼ ëª©ë¡: {qa_df.columns.tolist()}")

            # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
            if QUESTION_COLUMN not in qa_df.columns: raise ValueError(f"'{QUESTION_COLUMN}' ì»¬ëŸ¼ì´ ì‹œíŠ¸ì— ì—†ìŠµë‹ˆë‹¤.")
            # ì„ íƒ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸ ë° í”Œë˜ê·¸ ì„¤ì •
            if GROUND_TRUTHS_COLUMN in qa_df.columns: ground_truths_column_exists = True
            else: print(f"  !! [ê²½ê³ ] '{GROUND_TRUTHS_COLUMN}' ì»¬ëŸ¼ ì—†ìŒ. context_recall í‰ê°€ ê²°ê³¼ëŠ” 0ì´ ë©ë‹ˆë‹¤.")
            if ANSWER_COLUMN in qa_df.columns: answer_column_exists = True
            else: print(f"  !! [ê²½ê³ ] '{ANSWER_COLUMN}' ì»¬ëŸ¼ ì—†ìŒ. context_precision ë“± ì¼ë¶€ ë©”íŠ¸ë¦­ ê²°ê³¼ì— ì˜í–¥ ê°€ëŠ¥.")
        else:
             print("  !! [ì˜¤ë¥˜] Google Sheetì— í—¤ë” ì™¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
             qa_df = pd.DataFrame() # ë¹ˆ DataFrame

    except gspread.exceptions.SpreadsheetNotFound:
         print(f"  !! [ì˜¤ë¥˜] Google Sheet '{GSHEET_FILE_NAME}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ì´ë¦„ ë° ê³µìœ  ê¶Œí•œ í™•ì¸ í•„ìš”.")
         qa_df = pd.DataFrame()
    except Exception as e:
        print(f"  !! [ì˜¤ë¥˜] Google Sheet ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        qa_df = pd.DataFrame()
else:
    print("!! Google Sheet í´ë¼ì´ì–¸íŠ¸(gc)ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ë°ì´í„°ì…‹ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# --- 12.4: RAGAS í‰ê°€ ë°ì´í„° ì¤€ë¹„ ---
print("\n--- 12.4: RAGAS í‰ê°€ ë°ì´í„° ì¤€ë¹„ ---")
eval_data_list = []
# DataFrameì´ ë¹„ì–´ìˆì§€ ì•Šê³ , Retrieverê°€ ì¤€ë¹„ë˜ì—ˆì„ ë•Œë§Œ ì§„í–‰
if not qa_df.empty and retriever_available:
    print(f"ì´ {len(qa_df)}ê°œ ì§ˆë¬¸ì— ëŒ€í•´ Contexts ê²€ìƒ‰ ë° ë°ì´í„° í˜•ì‹ ë³€í™˜ ì¤‘...")
    # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
    for index, row in tqdm(qa_df.iterrows(), total=qa_df.shape[0], desc="RAGAS ë°ì´í„° ì¤€ë¹„"):
        # ì§ˆë¬¸ ì¶”ì¶œ ë° ìœ íš¨ì„± ê²€ì‚¬
        question_text = row.get(QUESTION_COLUMN)
        if pd.isna(question_text) or not str(question_text).strip():
            # print(f"  - í–‰ #{index}: ì§ˆë¬¸ ë¹„ì–´ìˆì–´ ê±´ë„ˆ<0xEB><0x9B><0x81>ë‹ˆë‹¤.") # í•„ìš” ì‹œ ë¡œê·¸
            continue
        question_text = str(question_text).strip()

        # Answer ë° Ground Truths ì¶”ì¶œ
        answer_text = str(row.get(ANSWER_COLUMN, "")).strip() if answer_column_exists else ""
        ground_truths_input = row.get(GROUND_TRUTHS_COLUMN, "[]") if ground_truths_column_exists else "[]"

        retrieved_contexts = []
        ground_truths_list = []

        try:
            # --- Ground Truths íŒŒì‹± (ë¬¸ìì—´ -> Python ë¦¬ìŠ¤íŠ¸) ---
            # ground_truths ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ê³ , ë‚´ìš©ì´ ë¬¸ìì—´ì¼ ê²½ìš° íŒŒì‹± ì‹œë„
            if ground_truths_column_exists and isinstance(ground_truths_input, str) and ground_truths_input.strip():
                try:
                    # JSON ë°°ì—´ í˜•ì‹("[...]")ìœ¼ë¡œ ê°€ì •í•˜ê³  íŒŒì‹±
                    parsed_list = json.loads(ground_truths_input)
                    if isinstance(parsed_list, list):
                        # ë¦¬ìŠ¤íŠ¸ ë‚´ ê° í•­ëª©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ë¹ˆ ë¬¸ìì—´ ì œê±°
                        ground_truths_list = [str(item).strip() for item in parsed_list if item is not None and str(item).strip()]
                    else: # íŒŒì‹±ì€ ëìœ¼ë‚˜ ë¦¬ìŠ¤íŠ¸ ì•„ë‹˜ (ì˜ˆ: ë‹¨ìˆœ ë¬¸ìì—´)
                         ground_truths_list = [ground_truths_input.strip()]
                except (json.JSONDecodeError, TypeError):
                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ, ì›ë³¸ ë¬¸ìì—´ ìì²´ë¥¼ í•˜ë‚˜ì˜ ground truthë¡œ ê°„ì£¼
                    ground_truths_list = [ground_truths_input.strip()]
            # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° (DataFrameì— ì§ì ‘ ë¦¬ìŠ¤íŠ¸ê°€ ì €ì¥ëœ ê²½ìš°)
            elif isinstance(ground_truths_input, list):
                 ground_truths_list = [str(item).strip() for item in ground_truths_input if item is not None and str(item).strip()]
            # ê·¸ ì™¸ (ë¹„ì–´ ìˆê±°ë‚˜, ìˆ«ì ë“±) -> ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€

            # --- Retriever ì‹¤í–‰ ---
            # ê° ì§ˆë¬¸ì— ëŒ€í•´ ì„¤ì •ëœ retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰
            retrieved_docs = retriever.invoke(question_text)
            # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ page_contentë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ
            retrieved_contexts = [doc.page_content for doc in retrieved_docs]

            # --- RAGAS í‰ê°€ ë°ì´í„° í˜•ì‹ìœ¼ë¡œ êµ¬ì„± ---
            eval_item = {
                "question": question_text,         # RAGAS í•„ìˆ˜ í‚¤
                "contexts": retrieved_contexts,    # RAGAS í•„ìˆ˜ í‚¤
                "ground_truths": ground_truths_list, # RAGAS í•„ìˆ˜ í‚¤ (Context Recallìš©)
                "ground_truth": answer_text        # RAGAS ì„ íƒ í‚¤ (Faithfulness, Answer Relevancy ë“±ì—ì„œ ì‚¬ìš©)
            }
            eval_data_list.append(eval_item)

        # ë°ì´í„° ì¤€ë¹„ ì¤‘ ê°œë³„ í•­ëª© ì˜¤ë¥˜ ì²˜ë¦¬
        except Exception as e:
            print(f"\n!! ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ (ì§ˆë¬¸: '{question_text}'): {e}")
            traceback.print_exc()
            # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ë¹ˆ ë°ì´í„° ì¶”ê°€ (ì „ì²´ í‰ê°€ íë¦„ ìœ ì§€)
            eval_data_list.append({
                "question": question_text, "contexts": [], "ground_truths": [], "ground_truth": ""
            })

    print(f"[ì™„ë£Œ] RAGAS í‰ê°€ ë°ì´í„° {len(eval_data_list)}ê°œ ì¤€ë¹„ ì™„ë£Œ.")
else:
    if qa_df.empty: print("!! í‰ê°€ ë°ì´í„°(qa_df)ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¡œë“œë˜ì§€ ì•Šì•„ ë°ì´í„° ì¤€ë¹„ ë¶ˆê°€.")
    if not retriever_available: print("!! Retrieverê°€ ì—†ì–´ Contextsë¥¼ ê²€ìƒ‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë°ì´í„° ì¤€ë¹„ ë¶ˆê°€.")
    pass # ë°ì´í„° ì¤€ë¹„ ë¶ˆê°€

# --- 12.5: Hugging Face Dataset í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ---
print("\n--- 12.5: Hugging Face Dataset ë³€í™˜ ---")
eval_dataset = None
if eval_data_list:
    try:
        eval_dataset = Dataset.from_list(eval_data_list)
        print("[ì„±ê³µ] Hugging Face Datasetìœ¼ë¡œ ë³€í™˜ ì™„ë£Œ.")
        print(f"   ë°ì´í„°ì…‹ ì»¬ëŸ¼: {eval_dataset.column_names}")

        # === ìˆ˜ì •ëœ ë¶€ë¶„: ìµœëŒ€ 10ê°œ ìƒ˜í”Œ ì¶œë ¥ ===
        if len(eval_dataset) > 0:
            num_samples_to_show = min(10, len(eval_dataset)) # ìµœëŒ€ 10ê°œ ë˜ëŠ” ì‹¤ì œ ë°ì´í„°ì…‹ í¬ê¸°
            print(f"\n   ë°ì´í„°ì…‹ ìƒ˜í”Œ í™•ì¸ (ì²˜ìŒ {num_samples_to_show}ê°œ):")
            for i in range(num_samples_to_show):
                print(f"\n   --- í•­ëª© #{i} ---") # ê° í•­ëª© êµ¬ë¶„
                sample_item = eval_dataset[i]
                for key, value in sample_item.items():
                    print(f"     {key}: ", end="")
                    preview_limit = 150 # ë¯¸ë¦¬ë³´ê¸° ê¸€ì ìˆ˜
                    if isinstance(value, list):
                        # ë¦¬ìŠ¤íŠ¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                        preview = str(value)[:preview_limit] + ('...' if len(str(value)) > preview_limit else '')
                        print(f"(List, {len(value)} items) {preview}")
                    else:
                        # ë¬¸ìì—´ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                        preview = str(value)[:preview_limit] + ('...' if len(str(value)) > preview_limit else '')
                        print(f"{preview}")
        # === ìˆ˜ì • ì™„ë£Œ ===

    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] Dataset ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        eval_dataset = None # ì˜¤ë¥˜ ì‹œ None ì²˜ë¦¬
else:
     print("!! [ì •ë³´] ìƒì„±ëœ í‰ê°€ ë°ì´í„°ê°€ ì—†ì–´ Dataset ë³€í™˜ì„ ê±´ë„ˆ<0xEB><0x9B><0x81>ë‹ˆë‹¤.")


# --- 12.6: RAGAS í‰ê°€ ì‹¤í–‰ ---
print("\n--- 12.6: RAGAS í‰ê°€ ì‹¤í–‰ ---")
evaluation_df = None # í‰ê°€ ê²°ê³¼ ì €ì¥ DataFrame ì´ˆê¸°í™”
# í‰ê°€ ì‹¤í–‰ ì¡°ê±´ í™•ì¸
if llm_available and eval_dataset and ragas_llm and evaluate:
    # ì‚¬ìš©í•  í‰ê°€ ì§€í‘œ ì •ì˜
    metrics_to_evaluate = [context_precision, context_recall]
    print(f"í‰ê°€ ì§€í‘œ: {[m.name for m in metrics_to_evaluate if m]}") # None ì•„ë‹Œ ë©”íŠ¸ë¦­ ì´ë¦„ë§Œ ì¶œë ¥

    # í•„ìš”í•œ ë©”íŠ¸ë¦­ì´ ëª¨ë‘ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if not all(metrics_to_evaluate):
        print("!! [ì˜¤ë¥˜] í‰ê°€ì— í•„ìš”í•œ Ragas ë©”íŠ¸ë¦­(context_precision, context_recall) ì„í¬íŠ¸ ì‹¤íŒ¨.")
    else:
        try:
            # (ì°¸ê³ ) LLM Timeout/Retry ì„¤ì •ì€ í•„ìš”ì‹œ LangChain LLM ê°ì²´ ìƒì„± ì‹œì ì— í•˜ëŠ” ê²ƒì´ ë” ì•ˆì •ì ì¼ ìˆ˜ ìˆìŒ
            print("  (LLM Timeout/RetryëŠ” Langchain LLM ê°ì²´ ìƒì„± ì‹œ ì„¤ì • ê¶Œì¥)")

            # RAGAS í‰ê°€ ì‹¤í–‰
            print(f"RAGAS í‰ê°€ ì‹¤í–‰ (ë°ì´í„° {len(eval_dataset)}ê°œ, ë°°ì¹˜ í¬ê¸° 2)...")
            # evaluate í•¨ìˆ˜ í˜¸ì¶œ
            result_ragas = evaluate(
                dataset=eval_dataset,           # Hugging Face Dataset
                metrics=metrics_to_evaluate,    # í‰ê°€ ì§€í‘œ ë¦¬ìŠ¤íŠ¸
                llm=ragas_llm,                  # Ragas LLM ë˜í¼
                embeddings=ragas_embeddings if embeddings_available else None, # Ragas Embedding ë˜í¼ (ì—†ìœ¼ë©´ None)
                batch_size=2                    # ë°°ì¹˜ í¬ê¸° (API ì œí•œ ê³ ë ¤í•˜ì—¬ ì¡°ì ˆ)
                # is_async=False # ê¸°ë³¸ê°’ì€ Trueì´ë‚˜, Colab ë“± í™˜ê²½ë”°ë¼ Falseê°€ ì•ˆì •ì ì¼ ìˆ˜ ìˆìŒ
            )
            evaluation_df = result_ragas.to_pandas() # ê²°ê³¼ë¥¼ pandas DataFrameìœ¼ë¡œ
            print("[ì„±ê³µ] RAGAS í‰ê°€ ì™„ë£Œ.")

            # --- ê°œë³„ ì§ˆë¬¸ë³„ ê²°ê³¼ ì¶œë ¥ ---
            print("\n--- ê°œë³„ ì§ˆë¬¸ë³„ í‰ê°€ ê²°ê³¼ ---")
            for index, row in evaluation_df.iterrows():
                question = eval_dataset[index].get("question", f"ì§ˆë¬¸ #{index+1}")
                print(f"\n**ì§ˆë¬¸ {index + 1}:** {question}")
                for metric in metrics_to_evaluate:
                    metric_name = metric.name
                    if metric_name in row.index and not pd.isna(row[metric_name]):
                        print(f"  - {metric_name}: {row[metric_name]:.4f}") # ì†Œìˆ˜ì  4ìë¦¬
                    else:
                        print(f"  - {metric_name}: N/A (ê³„ì‚° ì‹¤íŒ¨)")
                print("-" * 30)

        except Exception as e:
            print(f"!! [ì˜¤ë¥˜] RAGAS í‰ê°€ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("   - LLM API í˜¸ì¶œ ì‹¤íŒ¨(Timeout, í• ë‹¹ëŸ‰ ì´ˆê³¼ ë“±) ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
            print("   - Google Cloud Console API í• ë‹¹ëŸ‰ í™•ì¸ ë˜ëŠ” batch_size=1 ì‹œë„ ê¶Œì¥.")
            traceback.print_exc()
else:
    # í‰ê°€ ì‹¤í–‰ ë¶ˆê°€ ì‚¬ìœ  ìš”ì•½
    missing_components = []
    if not llm_available: missing_components.append("LLM")
    if not eval_dataset: missing_components.append("í‰ê°€ ë°ì´í„°ì…‹")
    if not ragas_llm: missing_components.append("RAGAS LLM ë˜í¼")
    if not evaluate: missing_components.append("RAGAS evaluate í•¨ìˆ˜")
    print(f"!! {', '.join(missing_components)} ì¤€ë¹„ ì•ˆ ë¨. RAGAS í‰ê°€ ë¶ˆê°€.")

# --- 12.7: ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½ ---
print("\n--- 12.7: RAGAS Retriever ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½ ---")
if evaluation_df is not None:
    # í‰ê°€ëœ ë©”íŠ¸ë¦­ ì»¬ëŸ¼ë§Œ í•„í„°ë§
    existing_metrics = [m.name for m in metrics_to_evaluate if m and m.name in evaluation_df.columns]
    if existing_metrics:
        print("\n=== ì „ì²´ ê²°ê³¼ (ìš”ì•½) ===")
        print(evaluation_df[existing_metrics].round(4).to_string(na_rep='NaN')) # NaN ê°’ í‘œì‹œ

        # NaN ì œì™¸ í‰ê·  ê³„ì‚°
        avg_scores = evaluation_df[existing_metrics].mean(skipna=True)
        print("\n=== ì§€í‘œë³„ í‰ê·  ì ìˆ˜ (NaN ì œì™¸) ===")
        print(avg_scores.round(4))

        # NaN ë°œìƒ ìš”ì•½
        nan_counts = evaluation_df[existing_metrics].isna().sum()
        if nan_counts.sum() > 0:
             print("\n[ì°¸ê³ ] ì¼ë¶€ í•­ëª© NaN ë°œìƒ:")
             print(nan_counts[nan_counts > 0])
             print("  (LLM ì‘ë‹µ ì˜¤ë¥˜, Timeout ë“±ìœ¼ë¡œ ì ìˆ˜ ê³„ì‚° ë¶ˆê°€ ì‹œ ë°œìƒ ê°€ëŠ¥)")
    else:
        print("\n!! evaluation_dfì— í‘œì‹œí•  ë©”íŠ¸ë¦­ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    print(f"\n**ì°¸ê³ :**")
    print(f"  - 'context_recall': Google Sheet '{GROUND_TRUTHS_COLUMN}' ì»¬ëŸ¼ ê¸°ì¤€.")
    print(f"  - 'context_precision': '{QUESTION_COLUMN}', '{ANSWER_COLUMN}' ì»¬ëŸ¼ ì°¸ì¡° ê°€ëŠ¥.")
else:
    print("RAGAS í‰ê°€ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ìš”ì•½í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

print("\n--- ë‹¨ê³„ 12: Retriever ì„±ëŠ¥ í‰ê°€ (RAGAS) ì™„ë£Œ ---")

"""# 13: RAGAS ìƒì„±ëœ ë‹µë³€ í‰ê°€"""

# === ë‹¨ê³„ 13: RAGAS í‰ê°€ (ìƒì„±ëœ ë‹µë³€ ëŒ€ìƒ) ===
# ì´ ë¸”ë¡ì€ ë‹¨ê³„ 11ì—ì„œ RAG ì‹œìŠ¤í…œ ì‹¤í–‰(qa_chain.invoke) í›„ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

print("\n--- ë‹¨ê³„ 12: RAGAS í‰ê°€ ì‹œì‘ (Faithfulness, Answer Relevancy) ---")

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì´ì „ ë¸”ë¡ì—ì„œ ì„í¬íŠ¸í–ˆë”ë¼ë„ ëª…ì‹œì ìœ¼ë¡œ ë‹¤ì‹œ í™•ì¸)
import warnings
from datasets import Dataset
# RAGAS ê´€ë ¨ ì„í¬íŠ¸ (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë©”ì‹œì§€ ì¶œë ¥í•˜ë©° Noneìœ¼ë¡œ ì„¤ì •)
try:
    from ragas import evaluate
    from ragas.metrics import faithfulness, answer_relevancy
    from ragas.llms import LangchainLLMWrapper
    from ragas.embeddings import LangchainEmbeddingsWrapper
    print("[ì •ë³´] RAGAS ë° ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í™•ì¸.")
except ImportError as e:
    print(f"!! [ì˜¤ë¥˜] RAGAS ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    print("   ì´ì „ ë‹¨ê³„ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ê°€ ì„±ê³µí–ˆëŠ”ì§€, ëŸ°íƒ€ì„ ì¬ì‹œì‘ì´ í•„ìš”í•œì§€ í™•ì¸í•˜ì„¸ìš”.")
    evaluate = None; faithfulness = None; answer_relevancy = None;
    LangchainLLMWrapper = None; LangchainEmbeddingsWrapper = None

warnings.filterwarnings("ignore") # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°

# --- 13.1: í‰ê°€ì— í•„ìš”í•œ ê°ì²´ ë° ë°ì´í„° í™•ì¸ ---
print("\n--- 12.1: í‰ê°€ í•„ìš” ê°ì²´ í™•ì¸ ---")

# ì´ì „ ë‹¨ê³„(Step 11)ì—ì„œ ìƒì„±ëœ ê²°ê³¼ í™•ì¸
rag_execution_successful = ('query' in locals() and query and
                           'result' in locals() and isinstance(result, dict) and
                           'result' in result and 'source_documents' in result)

# í‰ê°€ì— í•„ìš”í•œ LLM ë° Embedding ëª¨ë¸ í™•ì¸ (ì´ì „ ë‹¨ê³„ì—ì„œ ì •ì˜ë¨)
llm_available = 'llm' in locals() and llm is not None
embeddings_available = 'embeddings' in locals() and embeddings is not None

if not rag_execution_successful:
    print("!! [ì˜¤ë¥˜] ì´ì „ ë‹¨ê³„(ë‹¨ê³„ 11)ì˜ RAG ì‹¤í–‰ ê²°ê³¼('query', 'result') ì—†ìŒ. í‰ê°€ ë¶ˆê°€.")
if not llm_available:
    print("!! [ì˜¤ë¥˜] í‰ê°€ìš© LLM ê°ì²´('llm') ì—†ìŒ. í‰ê°€ ë¶ˆê°€.")
if not embeddings_available:
    print("!! [ê²½ê³ ] í‰ê°€ìš© Embedding ê°ì²´('embeddings') ì—†ìŒ. 'answer_relevancy' ê³„ì‚° ë¶ˆê°€.")

# ëª¨ë“  í•„ìˆ˜ ìš”ì†Œê°€ ì¤€ë¹„ë˜ì—ˆëŠ”ì§€ ìµœì¢… í™•ì¸
ready_for_evaluation = (rag_execution_successful and llm_available and embeddings_available and
                        evaluate and faithfulness and answer_relevancy and
                        LangchainLLMWrapper and LangchainEmbeddingsWrapper) # RAGAS í•¨ìˆ˜/í´ë˜ìŠ¤ ì„í¬íŠ¸ í™•ì¸

# --- 13.2: RAGAS í‰ê°€ ë°ì´í„° ì¤€ë¹„ ---
eval_dataset = None
if ready_for_evaluation:
    print("\n--- 12.2: RAGAS í‰ê°€ ë°ì´í„° ì¤€ë¹„ ---")
    try:
        # Step 11ì˜ ê²°ê³¼ë¥¼ RAGASê°€ ìš”êµ¬í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        eval_data = {
            'question': [query],  # Step 11ì˜ ì§ˆë¬¸ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
            'answer': [result['result']], # Step 11ì˜ ìƒì„±ëœ ë‹µë³€ (ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
            'contexts': [[doc.page_content for doc in result['source_documents']]], # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ë¦¬ìŠ¤íŠ¸ (ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
            # 'ground_truth': ["ì—¬ê¸°ì— ì´ìƒì ì¸ ì •ë‹µ ë¬¸ìì—´ì„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤"] # (ì„ íƒ ì‚¬í•­) ë§Œì•½ ì´ìƒì ì¸ ì •ë‹µì´ ìˆë‹¤ë©´ ì¶”ê°€
        }
        eval_dataset = Dataset.from_dict(eval_data)
        print("[ì„±ê³µ] í‰ê°€ìš© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ (1ê°œ í•­ëª©).")
        # print("  ìƒ˜í”Œ ë°ì´í„°:", eval_dataset[0]) # ë””ë²„ê¹…ìš©
    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] í‰ê°€ ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        eval_dataset = None
        ready_for_evaluation = False # ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨ ì‹œ í‰ê°€ ë¶ˆê°€

# --- 13.3: RAGAS í‰ê°€ ì‹¤í–‰ ---
evaluation_results = None
if ready_for_evaluation:
    print("\n--- 12.3: RAGAS í‰ê°€ ì‹¤í–‰ ---")
    try:
        # RAGAS Metric ì •ì˜ (ì´ í‰ê°€ì—ì„œëŠ” ë‹µë³€ í’ˆì§ˆ ì§€í‘œë§Œ ì‚¬ìš©)
        metrics_to_evaluate = [
            faithfulness,     # ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•œê°€ (LLM í•„ìš”)
            answer_relevancy  # ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ê°€ (LLM ë° Embeddings í•„ìš”)
        ]
        print(f"í‰ê°€ ì§€í‘œ: {[m.name for m in metrics_to_evaluate]}")

        # RAGAS ë˜í¼ ì´ˆê¸°í™” (ì´ì „ Ragas í‰ê°€ ë¸”ë¡ê³¼ ìœ ì‚¬)
        # LLM ë˜í¼
        ragas_llm_eval = LangchainLLMWrapper(llm)
        # Embedding ë˜í¼
        ragas_embeddings_eval = LangchainEmbeddingsWrapper(embeddings)

        # í‰ê°€ ì‹¤í–‰
        print("RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘...")
        evaluation_results = evaluate(
            dataset=eval_dataset,
            metrics=metrics_to_evaluate,
            llm=ragas_llm_eval,
            embeddings=ragas_embeddings_eval # answer_relevancyì— í•„ìš”
            # is_async=False # Colab ë“±ì—ì„œ ë™ì‹œì„± ë¬¸ì œ ë°œìƒ ì‹œ False ì‹œë„
        )
        print("[ì„±ê³µ] RAGAS í‰ê°€ ì™„ë£Œ.")

    except Exception as e:
        print(f"!! [ì˜¤ë¥˜] RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("   - API í‚¤ ìœ íš¨ì„±, í• ë‹¹ëŸ‰(Rate Limit), ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸ í•„ìš”.")
        print("   - LLM/Embedding ëª¨ë¸ í˜¸í™˜ì„± ë˜ëŠ” RAGAS ë²„ì „ ë¬¸ì œì¼ ìˆ˜ ìˆìŒ.")
        traceback.print_exc() # ìƒì„¸ ì˜¤ë¥˜ ìŠ¤íƒ ì¶œë ¥
        evaluation_results = None
else:
    print("\n!! í•„ìˆ˜ ìš”ì†Œ ë¶€ì¡±ìœ¼ë¡œ RAGAS í‰ê°€ë¥¼ ê±´ë„ˆ<0xEB><0x9B><0x81>ë‹ˆë‹¤.")


# --- 13.4: í‰ê°€ ê²°ê³¼ ì¶œë ¥ ---
print("\n--- 12.4: RAGAS í‰ê°€ ê²°ê³¼ ---")
if evaluation_results:
    # Ragas ê²°ê³¼ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¼ ìˆ˜ ìˆìŒ (ìµœì‹  ë²„ì „ í™•ì¸ í•„ìš”)
    if isinstance(evaluation_results, dict):
        print("{")
        for metric_name, score in evaluation_results.items():
             # NaN ê°’ ì²˜ë¦¬ ì¶”ê°€
             score_str = f"{score:.4f}" if isinstance(score, (int, float)) and not np.isnan(score) else "NaN"
             print(f"  '{metric_name}': {score_str},")
        print("}")
    else:
         # DataFrame ë“±ìœ¼ë¡œ ë°˜í™˜ë  ê²½ìš° ì²˜ë¦¬
         print(evaluation_results)

    print("\n**ì§€í‘œ ì„¤ëª…:**")
    print("  - faithfulness: ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ë‚´ìš©ì— ì–¼ë§ˆë‚˜ ê¸°ë°˜í•˜ëŠ”ì§€ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ,å¹»è¦º ì ìŒ)")
    print("  - answer_relevancy: ìƒì„±ëœ ë‹µë³€ì´ ì›ë³¸ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì„±ì´ ë†’ì€ì§€ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
else:
    print("í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

print("\n--- ë‹¨ê³„ 12: RAGAS í‰ê°€ ì™„ë£Œ ---")

"""# 14: ì •ì„±ì  í‰ê°€ (LLM-as-Judge)"""

# -*- coding: utf-8 -*-
# === ë‹¨ê³„ 14: ì •ì„±ì  í‰ê°€ (LLM-as-Judge) ===
# (ChatOpenAI ì‚¬ìš©ì— ë§ê²Œ ìˆ˜ì •ë¨)

# ì´ ë¸”ë¡ì€ ë‹¨ê³„ 11ì—ì„œ RAG ì‹œìŠ¤í…œ ì‹¤í–‰(qa_chain.invoke ë˜ëŠ” __call__) í›„ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

print("\n--- ë‹¨ê³„ 14: ì •ì„±ì  í‰ê°€ (LLM-as-Judge) ì‹œì‘ ---") # <<< ë‹¨ê³„ ë²ˆí˜¸ ìˆ˜ì •

# --- 14.1: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ê°ì²´ í™•ì¸ ---
print("\n--- 14.1: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ê°ì²´ í™•ì¸ ---") # <<< ë‹¨ê³„ ë²ˆí˜¸ ìˆ˜ì •
import json
import traceback
import re # JSON ì¶”ì¶œ ìœ„í•´ ì„í¬íŠ¸ ì¶”ê°€
# LangChain ê´€ë ¨ (ì´ë¯¸ ì„í¬íŠ¸ ë˜ì—ˆì„ ê°€ëŠ¥ì„± ë†’ìŒ, í™•ì¸ ì°¨ì›)
try: from langchain.prompts import PromptTemplate
except ImportError: print("!! [ì˜¤ë¥˜] langchain.prompts ì„í¬íŠ¸ ì‹¤íŒ¨")
# LLM í´ë˜ìŠ¤ í™•ì¸ (ChatOpenAI)
try: from langchain_openai import ChatOpenAI
except ImportError: print("!! [ì˜¤ë¥˜] langchain_openai.ChatOpenAI ì„í¬íŠ¸ ì‹¤íŒ¨")

# ì´ì „ ë‹¨ê³„ ê²°ê³¼ í™•ì¸
rag_execution_successful = ('query' in locals() and query and
                           'result' in locals() and isinstance(result, dict) and
                           'result' in result and 'source_documents' in result)
# í‰ê°€ì LLM í™•ì¸ (ë‹¨ê³„ 9ì—ì„œ ì •ì˜ëœ llm ì‚¬ìš©)
llm_available = 'llm' in locals() and llm is not None and isinstance(llm, ChatOpenAI) # íƒ€ì… í™•ì¸ ì¶”ê°€

if not rag_execution_successful:
    print("!! [ì˜¤ë¥˜] ì´ì „ ë‹¨ê³„(ë‹¨ê³„ 11)ì˜ RAG ì‹¤í–‰ ê²°ê³¼('query', 'result') ì—†ìŒ. í‰ê°€ ë¶ˆê°€.")
if not llm_available:
    print("!! [ì˜¤ë¥˜] í‰ê°€ìš© LLM ê°ì²´('llm')ê°€ ì—†ê±°ë‚˜ ChatOpenAI íƒ€ì…ì´ ì•„ë‹˜. í‰ê°€ ë¶ˆê°€.")

# --- 14.2: í‰ê°€ ì‹¤í–‰ ---
print("\n--- 14.2: í‰ê°€ ì‹¤í–‰ ---") # <<< ë‹¨ê³„ ë²ˆí˜¸ ìˆ˜ì •
if rag_execution_successful and llm_available:
    # í‰ê°€ì— ì‚¬ìš©í•  LLM (ê¸°ì¡´ llm ì‚¬ìš©)
    evaluator_llm = llm
    # ë§Œì•½ ë‹¤ë¥¸ ê°•ë ¥í•œ ëª¨ë¸ì„ ì“°ê³  ì‹¶ë‹¤ë©´ ì—¬ê¸°ì„œ ì¬ì •ì˜
    # evaluator_llm = ChatOpenAI(model="gpt-4o", temperature=0)

    # ====> ìˆ˜ì •ëœ ë¶€ë¶„: .model ëŒ€ì‹  .model_name ì‚¬ìš© <====
    print(f"[ì •ë³´] í‰ê°€ì LLM: {evaluator_llm.model_name}")
    # ===============================================

    # í‰ê°€ ë°ì´í„° ì¶”ì¶œ
    rag_answer = result.get("result", "")
    source_docs = result.get("source_documents", []) # source_documentsê°€ ì—†ë‹¤ë©´ ì´ì „ ë‹¨ê³„ ë¬¸ì œ
    if not source_docs:
         print("!! [ê²½ê³ ] result ê°ì²´ì— 'source_documents'ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì—†ìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ ì—†ëŠ” í‰ê°€ ì§„í–‰.")
         context_str = "" # ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì§„í–‰
    else:
         context_str = "\n\n".join([doc.page_content for doc in source_docs])

    # í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ê¸°ì¡´ ìœ ì§€)
    evaluation_template = """ë‹¹ì‹ ì€ AI ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ 'ì§ˆë¬¸', 'ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ', ê·¸ë¦¬ê³  AIê°€ ìƒì„±í•œ 'ë‹µë³€'ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”. í‰ê°€ëŠ” 'ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ'ì— ìˆëŠ” ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.

    [í‰ê°€ ê¸°ì¤€]
    1.  **Faithfulness (ì¶©ì‹¤ì„±):** ë‹µë³€ì´ 'ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ' ë‚´ìš©ì—ë§Œ ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆê¹Œ? ì™¸ë¶€ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì»¨í…ìŠ¤íŠ¸ì™€ ëª¨ìˆœë˜ëŠ” ë‚´ìš©ì€ ì—†ìŠµë‹ˆê¹Œ? (Yes/No)
    2.  **Relevance (ê´€ë ¨ì„±):** ë‹µë³€ì´ ì‚¬ìš©ìì˜ 'ì§ˆë¬¸'ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆìŠµë‹ˆê¹Œ? (Yes/No)
    3.  **Clarity (ëª…í™•ì„±):** ë‹µë³€ì´ ëª…í™•í•˜ê³  ê°„ê²°í•˜ë©° ì´í•´í•˜ê¸° ì‰½ìŠµë‹ˆê¹Œ? (Yes/No)
    4.  **Completeness (ì™„ì „ì„± - ì»¨í…ìŠ¤íŠ¸ ê¸°ì¤€):** ë‹µë³€ì´ 'ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ' ë‚´ì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” ì •ë³´ ë²”ìœ„ ë‚´ì—ì„œ 'ì§ˆë¬¸'ì— ëŒ€í•´ ì¶©ë¶„íˆ ì™„ì „í•˜ê²Œ ë‹µë³€í•˜ê³  ìˆìŠµë‹ˆê¹Œ? (Yes/No)

    [ì…ë ¥ ì •ë³´]
    ì§ˆë¬¸: {question}

    ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ:
    {context}

    AI ìƒì„± ë‹µë³€: {answer}

    [í‰ê°€ ê²°ê³¼ ì¶œë ¥]
    ê° í‰ê°€ ê¸°ì¤€ì— ëŒ€í•´ "Yes" ë˜ëŠ” "No"ë¡œ í‰ê°€í•˜ê³ , ê° í‰ê°€ì— ëŒ€í•œ ê°„ëµí•œ ê·¼ê±°ë¥¼ í¬í•¨í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì£¼ì„¸ìš”.
    ì˜ˆì‹œ: {{
        "faithfulness": {{\"score\": \"Yes\", \"justification\": \"ë‹µë³€ ë‚´ìš©ì´ ëª¨ë‘ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.\"}},
        "relevance": {{\"score\": \"Yes\", \"justification\": \"ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •í™•íˆ íŒŒì•…í•˜ê³  ë‹µë³€í–ˆìŠµë‹ˆë‹¤.\"}},
        "clarity": {{\"score\": \"Yes\", \"justification\": \"ê°„ê²°í•˜ê³  ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\"}},
        "completeness": {{\"score\": \"No\", \"justification\": \"ì§ˆë¬¸ì˜ ì¼ë¶€ ì¸¡ë©´(ì˜ˆ: ì˜ˆì™¸ ì‚¬í•­)ì— ëŒ€í•œ ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì— ìˆì—ˆìœ¼ë‚˜ ë‹µë³€ì— í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\"}}
    }}

    JSON ì¶œë ¥:
    """
    try:
        EVALUATION_PROMPT = PromptTemplate.from_template(evaluation_template)
    except NameError:
         print("!! [ì˜¤ë¥˜] PromptTemplate í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¨ê³„ 1 ì„í¬íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
         EVALUATION_PROMPT = None

    # í‰ê°€ ì²´ì¸ êµ¬ì„± ë° ì‹¤í–‰
    if EVALUATION_PROMPT:
        try:
            print("\nLLM ê¸°ë°˜ ì •ì„±ì  í‰ê°€ ì§„í–‰ ì¤‘...")
            eval_chain = EVALUATION_PROMPT | evaluator_llm # LangChain Expression Language ì‚¬ìš©

            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í‰ê°€ì LLMì˜ í† í° ì œí•œ ê³ ë ¤)
            # GPT-4oëŠ” ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥ (ì˜ˆ: 128k), ë¹„ìš© ë° íš¨ìœ¨ì„± ìœ„í•´ ì œí•œ ê°€ëŠ¥
            max_context_length = 100000 # ì˜ˆì‹œ: í•„ìš”ì‹œ ì¡°ì • (GPT-4oì— ë§ì¶° ëŠ˜ë¦¼)
            if len(context_str) > max_context_length:
                print(f"  [ê²½ê³ ] ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ ê¸¸ì–´ {max_context_length}ìë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤.")
                context_for_eval = context_str[:max_context_length]
            else:
                context_for_eval = context_str
                if not context_for_eval: # source_documents ì—†ì„ ê²½ìš°
                    print("  [ì •ë³´] ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ì—†ì´ í‰ê°€ ì§„í–‰.")

            # ì…ë ¥ê°’ ì¤€ë¹„
            chain_input = {
                "question": query if query else "ì§ˆë¬¸ ì •ë³´ ì—†ìŒ", # query ë³€ìˆ˜ í™•ì¸
                "context": context_for_eval,
                "answer": rag_answer if rag_answer else "ë‹µë³€ ì •ë³´ ì—†ìŒ" # rag_answer ë³€ìˆ˜ í™•ì¸
            }

            eval_result_str = eval_chain.invoke(chain_input).content

            print("\n[LLM í‰ê°€ ê²°ê³¼ (Raw)]:\n", eval_result_str)

            # JSON íŒŒì‹± ì‹œë„ ë° ê²°ê³¼ ì¶œë ¥
            print("\n[íŒŒì‹±ëœ í‰ê°€ ê²°ê³¼]:")
            try:
                # LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ```json ... ``` ê³ ë ¤)
                match = re.search(r'```json\s*(\{.*?\})\s*```', eval_result_str, re.DOTALL | re.IGNORECASE)
                if match:
                    json_str = match.group(1)
                else:
                    # ì½”ë“œ ë¸”ë¡ ì—†ìœ¼ë©´, ì‘ë‹µ ì‹œì‘ì´ '{' ì¸ì§€ í™•ì¸
                    json_str = eval_result_str.strip()
                    # ê°€ë” LLMì´ JSON ì•ì— ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, '{' ë¥¼ ì°¾ì•„ ì‹œì‘ì  ì¡°ì • ì‹œë„
                    start_brace = json_str.find('{')
                    if start_brace != -1:
                        json_str = json_str[start_brace:]
                        # ë§ˆì§€ë§‰ '}' ì´í›„ ë‚´ìš© ì œê±° ì‹œë„ (ë‹¨ìˆœ êµ¬í˜„)
                        end_brace = json_str.rfind('}')
                        if end_brace != -1:
                             json_str = json_str[:end_brace+1]
                        else:
                             raise json.JSONDecodeError("ë‹«ëŠ” ì¤‘ê´„í˜¸ '}' ì—†ìŒ", json_str, 0)
                    else:
                         raise json.JSONDecodeError("ì‘ë‹µì´ JSON í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ", json_str, 0)

                eval_data = json.loads(json_str)
                for criterion, details in eval_data.items():
                    # detailsê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ì§€ í™•ì¸
                    if isinstance(details, dict):
                         print(f"- {str(criterion).capitalize()}:") # í‚¤ê°€ ë¬¸ìì—´ ì•„ë‹ ìˆ˜ë„ ìˆìœ¼ë‹ˆ str() ì¶”ê°€
                         print(f"  - ì ìˆ˜: {details.get('score', 'N/A')}")
                         print(f"  - ê·¼ê±°: {details.get('justification', 'N/A')}")
                    else:
                         # ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì¼ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶œë ¥
                         print(f"- {str(criterion).capitalize()}: {details}")

            except json.JSONDecodeError as json_e:
                print(f"  !! í‰ê°€ ê²°ê³¼ê°€ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹ˆê±°ë‚˜ íŒŒì‹± ì˜¤ë¥˜ ë°œìƒ: {json_e}")
                print("     Raw ê²°ê³¼ë¥¼ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”.")
            except Exception as parse_e:
                print(f"  !! í‰ê°€ ê²°ê³¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {parse_e}")
                traceback.print_exc()

        except Exception as e:
            print(f"!! LLM í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("   - API í‚¤, í• ë‹¹ëŸ‰, ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë“±ì„ í™•ì¸í•˜ì„¸ìš”.")
            traceback.print_exc()
    else:
        print("!! [ì˜¤ë¥˜] í‰ê°€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿(EVALUATION_PROMPT) ìƒì„± ì‹¤íŒ¨.")

else:
    # í‰ê°€ ì‹¤í–‰ ë¶ˆê°€ ì‚¬ìœ  ì¶œë ¥
    print("\n!! ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë˜ëŠ” LLM ê°ì²´ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•„ ì •ì„±ì  í‰ê°€ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


print("\n--- ë‹¨ê³„ 14: ì •ì„±ì  í‰ê°€ ì™„ë£Œ ---") # <<< ë‹¨ê³„ ë²ˆí˜¸ ìˆ˜ì •